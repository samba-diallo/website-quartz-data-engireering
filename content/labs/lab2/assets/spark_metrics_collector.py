#!/usr/bin/env python3
"""
Script de collecte automatique des m√©triques Spark UI
Auteurs: DIALLO Samba, DIOP Mouhamed
Cours: Data Engineering 1 - CEU
"""

import requests
import json
import csv
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class SparkMetricsCollector:
    """Collecteur de m√©triques depuis Spark UI via API REST"""
    
    def __init__(self, spark_ui_url="http://localhost:4040", app_id=None):
        """
        Args:
            spark_ui_url: URL du Spark UI (d√©faut: http://localhost:4040)
            app_id: ID de l'application Spark (auto-d√©tect√© si None)
        """
        self.spark_ui_url = spark_ui_url.rstrip('/')
        self.app_id = app_id
        self.metrics_file = Path("data/lab2_metrics_log.csv")
        
    def check_spark_ui_available(self):
        """V√©rifie si Spark UI est accessible"""
        try:
            response = requests.get(f"{self.spark_ui_url}/api/v1/applications", timeout=2)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def get_application_id(self):
        """R√©cup√®re l'ID de l'application Spark en cours"""
        try:
            response = requests.get(f"{self.spark_ui_url}/api/v1/applications")
            apps = response.json()
            if apps:
                # Prendre la premi√®re application (la plus r√©cente)
                self.app_id = apps[0]['id']
                return self.app_id
            return None
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration de l'app ID: {e}")
            return None
    
    def get_job_metrics(self, job_id):
        """R√©cup√®re les m√©triques d'un job sp√©cifique"""
        try:
            url = f"{self.spark_ui_url}/api/v1/applications/{self.app_id}/jobs/{job_id}"
            response = requests.get(url)
            return response.json()
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration des m√©triques du job {job_id}: {e}")
            return None
    
    def get_stage_metrics(self, stage_id):
        """R√©cup√®re les m√©triques d'un stage sp√©cifique"""
        try:
            url = f"{self.spark_ui_url}/api/v1/applications/{self.app_id}/stages/{stage_id}"
            response = requests.get(url)
            stages = response.json()
            if stages:
                return stages[0]  # Prendre la premi√®re tentative
            return None
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration des m√©triques du stage {stage_id}: {e}")
            return None
    
    def get_all_jobs(self):
        """R√©cup√®re tous les jobs de l'application"""
        try:
            url = f"{self.spark_ui_url}/api/v1/applications/{self.app_id}/jobs"
            response = requests.get(url)
            return response.json()
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration de tous les jobs: {e}")
            return []
    
    def get_all_stages(self):
        """R√©cup√®re tous les stages de l'application"""
        try:
            url = f"{self.spark_ui_url}/api/v1/applications/{self.app_id}/stages"
            response = requests.get(url)
            return response.json()
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration de tous les stages: {e}")
            return []
    
    def extract_metrics_from_stage(self, stage):
        """Extrait les m√©triques importantes d'un stage"""
        if not stage:
            return {}
        
        metrics = {
            'stage_id': stage.get('stageId', ''),
            'stage_name': stage.get('name', ''),
            'num_tasks': stage.get('numTasks', 0),
            'input_bytes': stage.get('inputBytes', 0),
            'input_records': stage.get('inputRecords', 0),
            'output_bytes': stage.get('outputBytes', 0),
            'output_records': stage.get('outputRecords', 0),
            'shuffle_read_bytes': stage.get('shuffleReadBytes', 0),
            'shuffle_read_records': stage.get('shuffleReadRecords', 0),
            'shuffle_write_bytes': stage.get('shuffleWriteBytes', 0),
            'shuffle_write_records': stage.get('shuffleWriteRecords', 0),
            'duration': stage.get('executorRunTime', 0),
            'status': stage.get('status', '')
        }
        return metrics
    
    def collect_current_metrics(self, run_id, task_name, note=""):
        """Collecte les m√©triques de la session Spark actuelle"""
        if not self.check_spark_ui_available():
            print(f"‚ö†Ô∏è  Spark UI non accessible √† {self.spark_ui_url}")
            print("   Assurez-vous que Spark est en cours d'ex√©cution")
            return None
        
        if not self.app_id:
            self.get_application_id()
        
        if not self.app_id:
            print("‚ùå Impossible de r√©cup√©rer l'ID de l'application")
            return None
        
        print(f"‚úÖ Application Spark d√©tect√©e: {self.app_id}")
        
        # R√©cup√©rer tous les stages
        stages = self.get_all_stages()
        
        if not stages:
            print("‚ö†Ô∏è  Aucun stage trouv√©")
            return None
        
        # Calculer les m√©triques agr√©g√©es
        total_input_bytes = 0
        total_shuffle_read = 0
        total_shuffle_write = 0
        files_read = 0
        
        for stage in stages:
            if stage.get('status') == 'COMPLETE':
                total_input_bytes += stage.get('inputBytes', 0)
                total_shuffle_read += stage.get('shuffleReadBytes', 0)
                total_shuffle_write += stage.get('shuffleWriteBytes', 0)
                files_read += stage.get('numTasks', 0)
        
        metrics_row = {
            'run_id': run_id,
            'task': task_name,
            'note': note,
            'files_read': files_read,
            'input_size_bytes': total_input_bytes,
            'shuffle_read_bytes': total_shuffle_read,
            'shuffle_write_bytes': total_shuffle_write,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"\nüìä M√©triques collect√©es pour {task_name}:")
        print(f"   - Fichiers lus: {files_read}")
        print(f"   - Taille input: {total_input_bytes:,} bytes ({total_input_bytes/1024/1024:.2f} MB)")
        print(f"   - Shuffle read: {total_shuffle_read:,} bytes ({total_shuffle_read/1024/1024:.2f} MB)")
        print(f"   - Shuffle write: {total_shuffle_write:,} bytes ({total_shuffle_write/1024/1024:.2f} MB)")
        
        return metrics_row
    
    def save_metrics(self, metrics_row):
        """Sauvegarde les m√©triques dans le CSV"""
        # Cr√©er le fichier s'il n'existe pas
        if not self.metrics_file.exists():
            self.metrics_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.metrics_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=metrics_row.keys())
                writer.writeheader()
        
        # Ajouter la ligne
        with open(self.metrics_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=metrics_row.keys())
            writer.writerow(metrics_row)
        
        print(f"‚úÖ M√©triques sauvegard√©es dans {self.metrics_file}")
    
    def display_metrics_summary(self):
        """Affiche un r√©sum√© des m√©triques collect√©es"""
        if not self.metrics_file.exists():
            print("‚ùå Aucune m√©trique trouv√©e")
            return
        
        df = pd.read_csv(self.metrics_file)
        print("\n" + "="*80)
        print("üìà R√âSUM√â DES M√âTRIQUES COLLECT√âES")
        print("="*80)
        print(df.to_string(index=False))
        print("="*80 + "\n")
    
    def get_spark_ui_urls(self):
        """Retourne les URLs importantes du Spark UI pour les captures"""
        if not self.app_id:
            self.get_application_id()
        
        urls = {
            'jobs': f"{self.spark_ui_url}/jobs/",
            'stages': f"{self.spark_ui_url}/stages/",
            'storage': f"{self.spark_ui_url}/storage/",
            'environment': f"{self.spark_ui_url}/environment/",
            'executors': f"{self.spark_ui_url}/executors/",
            'sql': f"{self.spark_ui_url}/SQL/"
        }
        
        print("\nüîó URLs Spark UI pour captures d'√©cran:")
        for name, url in urls.items():
            print(f"   {name.capitalize()}: {url}")
        
        return urls


def main():
    """Fonction principale - Exemple d'utilisation"""
    print("="*80)
    print("üöÄ SPARK METRICS COLLECTOR - Lab 2")
    print("="*80 + "\n")
    
    # Initialiser le collecteur
    collector = SparkMetricsCollector()
    
    # V√©rifier la disponibilit√©
    if not collector.check_spark_ui_available():
        print("‚ùå Spark UI n'est pas accessible")
        print("   Veuillez d√©marrer votre notebook Spark d'abord")
        print(f"   V√©rifiez que Spark UI est accessible √†: {collector.spark_ui_url}")
        return
    
    print("‚úÖ Spark UI est accessible\n")
    
    # Afficher les URLs pour captures
    collector.get_spark_ui_urls()
    
    print("\n" + "="*80)
    print("üí° INSTRUCTIONS D'UTILISATION")
    print("="*80)
    print("""
1. Dans votre notebook Spark, importez ce module:
   
   from spark_metrics_collector import SparkMetricsCollector
   collector = SparkMetricsCollector()

2. Avant chaque t√¢che importante, collectez les m√©triques:
   
   # Exemple: apr√®s l'ingestion des donn√©es
   metrics = collector.collect_current_metrics(
       run_id='r1',
       task_name='ingest_plan',
       note='Ingestion des donn√©es sources'
   )
   collector.save_metrics(metrics)

3. T√¢ches √† tracker pour le Lab 2:
   - ingest_plan : Ingestion initiale
   - fact_join : Cr√©ation de fact_sales
   - dim_creation : Cr√©ation des dimensions
   - broadcast_test : Test avec broadcast
   
4. Pour afficher le r√©sum√©:
   
   collector.display_metrics_summary()

5. Ouvrez les URLs Spark UI affich√©es ci-dessus dans votre navigateur
   et prenez des captures d'√©cran pour le dossier proof/
    """)
    
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
