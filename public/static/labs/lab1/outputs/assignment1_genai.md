# Utilisation de l'Intelligence Artificielle - Lab 1

**Auteurs :** DIALLO Samba, DIOP Mouhamed  
**Lab :** Lab 1 - Analyse de Fréquence de Mots  
**Date :** Octobre 2025

## Outil IA Utilisé

**Claude Sonnet 4.5** (via GitHub Copilot)

## Nature de l'Assistance

L'IA a été utilisée pour les tâches suivantes :

### 1. Configuration de l'Environnement
- Résolution des problèmes de configuration PySpark (`JAVA_HOME`, `SPARK_HOME`)
- Installation des dépendances (OpenJDK, PySpark, Jupyter)
- Configuration de l'environnement Conda

### 2. Compréhension des Concepts
- Différences entre les API RDD et DataFrame
- Quand utiliser RDD vs DataFrame
- Fonctionnement du moteur Catalyst de Spark
- Optimisations de performance (shuffle partitions, coalesce)

### 3. Débogage du Code
- Correction d'erreurs de syntaxe PySpark
- Résolution de problèmes de parsing CSV
- Gestion des valeurs nulles et des chaînes vides

### 4. Optimisation du Code
- Utilisation de fonctions natives au lieu d'UDFs
- Réduction du nombre de partitions pour l'exécution locale
- Filtrage précoce des données

### 5. Documentation
- Structure du README
- Explications des approches RDD et DataFrame
- Meilleures pratiques de documentation

## Ce Qui A Été Fait Manuellement

- **Implémentation du code** : Les transformations RDD et DataFrame ont été écrites en comprenant les concepts Spark
- **Analyse des résultats** : Comparaison des performances et interprétation des résultats
- **Tests et validation** : Exécution du code et vérification des sorties
- **Décisions d'architecture** : Choix des approches et des structures de données

## Apprentissages

L'utilisation de l'IA a permis :
- **Gain de temps** : Résolution rapide des problèmes de configuration (~40% de temps économisé)
- **Compréhension approfondie** : Explications claires des concepts Spark
- **Bonnes pratiques** : Apprentissage des patterns d'optimisation
- **Débogage efficace** : Identification rapide des erreurs

## Transparence

L'IA a été utilisée comme un **outil d'apprentissage**, similaire à :
- Consulter la documentation officielle de Spark
- Chercher des solutions sur Stack Overflow
- Demander de l'aide à un assistant enseignant

Le travail final représente notre compréhension et application des principes de Data Engineering avec Apache Spark.

## Temps Estimé

- **Temps total du projet** : ~8 heures
- **Temps avec IA** : ~5 heures
- **Économie de temps** : ~37%

L'IA a principalement accéléré les phases de configuration et de débogage, permettant de se concentrer sur la compréhension des concepts et l'implémentation.
