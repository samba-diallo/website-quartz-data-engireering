{"index":{"slug":"index","filePath":"index.md","title":"Data Engineering 1 ‚Äì Labs & Final Project","links":["labs/lab1","labs/lab2","labs/lab3","project/project"],"tags":[],"content":"üëã Welcome\nThis website presents all my labs and the final project for the course\nData Engineering 1 (ESIEE 2025‚Äì2026).\n\nüìò Labs\nWelcome to Data Engineering 1\nLabs\n\nLab 1\nLab 2\nLab 3\n\nFinal Project\n\nProject\n\n\nüîó Resources\n\nGitHub Repository\n"},"labs/index":{"slug":"labs/index","filePath":"labs/index.md","title":"Data Engineering 1 ‚Äì Labs","links":["lab0","labs/lab1/lab1","labs/lab2/lab2","labs/lab3/lab3"],"tags":[],"content":"üìò Labs\nThis section contains all the labs for Data Engineering 1.\n\nLab 0 ‚Äì Setup &amp; Installation\nLab 1 ‚Äì Spark Basics\nLab 2 ‚Äì Data Warehousing\nLab 3 ‚Äì Advanced Topics\n"},"labs/lab1/assets/DE1_Lab1_Notebook_EN":{"slug":"labs/lab1/assets/DE1_Lab1_Notebook_EN","filePath":"labs/lab1/assets/DE1_Lab1_Notebook_EN.md","title":"DE1_Lab1_Notebook_EN","links":[],"tags":[],"content":"DE1 ‚Äî Lab 1: PySpark Warmup and Reading Plans\n\nAuthor : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n\n\nThis notebook is the student deliverable. Execute all cells and attach evidence.\n0. Imports and Spark session\nimport os, sys, datetime, pathlib\nfrom pyspark.sql import SparkSession, functions as F\nprint(&quot;Python:&quot;, sys.version)\nspark = SparkSession.builder.appName(&quot;de1-lab1&quot;).getOrCreate()\nprint(&quot;Spark:&quot;, spark.version)\n\n\nPython: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\nSpark: 4.0.1\n\n1. Load the CSV inputs\nsrc_a = &quot;data/lab1_dataset_a.csv&quot;\nsrc_b = &quot;data/lab1_dataset_b.csv&quot;\ndf_a = spark.read.option(&quot;header&quot;,&quot;true&quot;).option(&quot;inferSchema&quot;,&quot;true&quot;).csv(src_a)\ndf_b = spark.read.option(&quot;header&quot;,&quot;true&quot;).option(&quot;inferSchema&quot;,&quot;true&quot;).csv(src_b)\ndf = df_a.unionByName(df_b)\ndf.cache()\nprint(&quot;Rows:&quot;, df.count())\ndf.printSchema()\ndf.show(5, truncate=False)\n\n\nRows: 2700\nroot\n |-- id: integer (nullable = true)\n |-- category: string (nullable = true)\n |-- value: double (nullable = true)\n |-- text: string (nullable = true)\n\n+---+-----------+-----+----------------------------------------------------------------------------+\n|id |category   |value|text                                                                        |\n+---+-----------+-----+----------------------------------------------------------------------------+\n|0  |toys       |48.47|metrics ui data elt row columnar reduce warehouse shuffle join spark elt    |\n|1  |books      |39.9 |metrics row lake aggregate columnar data reduce row columnar filter         |\n|2  |grocery    |7.96 |lake join partition scala elt data                                          |\n|3  |electronics|5.15 |spark scala elt filter join columnar lake lake plan warehouse columnar spark|\n|4  |toys       |44.87|aggregate metrics row row filter lake map metrics columnar spark            |\n+---+-----------+-----+----------------------------------------------------------------------------+\nonly showing top 5 rows\n\n2. Top‚ÄëN with RDD API\n# RDD pipeline: tokenize &#039;text&#039; column and count tokens\nrdd = df.select(&quot;text&quot;).rdd.flatMap(lambda row: (row[0] or &quot;&quot;).lower().split())\npair = rdd.map(lambda t: (t, 1))\ncounts = pair.reduceByKey(lambda a,b: a+b)\ntop_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\ntop_rdd\n\n\n[(&#039;lake&#039;, 1215),\n (&#039;scala&#039;, 1200),\n (&#039;elt&#039;, 1199),\n (&#039;metrics&#039;, 1190),\n (&#039;row&#039;, 1183),\n (&#039;join&#039;, 1169),\n (&#039;warehouse&#039;, 1168),\n (&#039;shuffle&#039;, 1160),\n (&#039;ui&#039;, 1145),\n (&#039;aggregate&#039;, 1144)]\n\n# Save as CSV (token,count)\npathlib.Path(&quot;outputs&quot;).mkdir(exist_ok=True)\nwith open(&quot;outputs/top10_rdd.csv&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;) as f:\n    f.write(&quot;token,count\\n&quot;)\n    for t,c in top_rdd:\n        f.write(f&quot;{t},{c}\\n&quot;)\nprint(&quot;Wrote outputs/top10_rdd.csv&quot;)\n\n\nWrote outputs/top10_rdd.csv\n\nRDD plan ‚Äî evidence\n# Trigger an action and record a textual plan for evidence\n_ = counts.count()\nplan_rdd = df._jdf.queryExecution().executedPlan().toString()\npathlib.Path(&quot;proof&quot;).mkdir(exist_ok=True)\nwith open(&quot;proof/plan_rdd.txt&quot;,&quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n\\n&quot;)\n    f.write(plan_rdd)\nprint(&quot;Saved proof/plan_rdd.txt&quot;)\n\n\nSaved proof/plan_rdd.txt\n\n3. Top‚ÄëN with DataFrame API\ntokens = F.explode(F.split(F.lower(F.col(&quot;text&quot;)), &quot;\\\\s+&quot;)).alias(&quot;token&quot;)\ndf_tokens = df.select(tokens).where(F.col(&quot;token&quot;) != &quot;&quot;)\nagg_df = df_tokens.groupBy(&quot;token&quot;).agg(F.count(&quot;*&quot;).alias(&quot;count&quot;))\ntop_df = agg_df.orderBy(F.desc(&quot;count&quot;), F.asc(&quot;token&quot;)).limit(10)\ntop_df.show(truncate=False)\ntop_df.coalesce(1).write.mode(&quot;overwrite&quot;).option(&quot;header&quot;,&quot;true&quot;).csv(&quot;outputs/top10_df_tmp&quot;)\n# move single part file to stable path\nimport glob, shutil\npart = glob.glob(&quot;outputs/top10_df_tmp/part*&quot;)[0]\nshutil.copy(part, &quot;outputs/top10_df.csv&quot;)\nprint(&quot;Wrote outputs/top10_df.csv&quot;)\n\n\n+---------+-----+\n|token    |count|\n+---------+-----+\n|lake     |1215 |\n|scala    |1200 |\n|elt      |1199 |\n|metrics  |1190 |\n|row      |1183 |\n|join     |1169 |\n|warehouse|1168 |\n|shuffle  |1160 |\n|ui       |1145 |\n|aggregate|1144 |\n+---------+-----+\n\nWrote outputs/top10_df.csv\n\nDataFrame plan ‚Äî evidence\nplan_df = top_df._jdf.queryExecution().executedPlan().toString()\nwith open(&quot;proof/plan_df.txt&quot;,&quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n\\n&quot;)\n    f.write(plan_df)\nprint(&quot;Saved proof/plan_df.txt&quot;)\n\n\nSaved proof/plan_df.txt\n\n4. Projection experiment: select(&quot;*&quot;) vs minimal projection\n# Case A: select all columns then aggregate on &#039;category&#039;\nall_cols = df.select(&quot;*&quot;).groupBy(&quot;category&quot;).agg(F.sum(&quot;value&quot;).alias(&quot;sum_value&quot;))\nall_cols.explain(&quot;formatted&quot;)\n_ = all_cols.count()  # trigger\n\n# Case B: minimal projection then aggregate\nproj = df.select(&quot;category&quot;,&quot;value&quot;).groupBy(&quot;category&quot;).agg(F.sum(&quot;value&quot;).alias(&quot;sum_value&quot;))\nproj.explain(&quot;formatted&quot;)\n_ = proj.count()  # trigger\n\nprint(&quot;Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv&quot;)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan (9)\n+- HashAggregate (8)\n   +- Exchange (7)\n      +- HashAggregate (6)\n         +- InMemoryTableScan (1)\n               +- InMemoryRelation (2)\n                     +- Union (5)\n                        :- Scan csv  (3)\n                        +- Scan csv  (4)\n\n\n(1) InMemoryTableScan\nOutput [2]: [category#1050, value#1051]\nArguments: [category#1050, value#1051]\n\n(2) InMemoryRelation\nArguments: [id#1049, category#1050, value#1051, text#1052], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan csv \nOutput [4]: [id#1049, category#1050, value#1051, text#1052]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab1-practice/data/lab1_dataset_a.csv]\nReadSchema: struct&lt;id:int,category:string,value:double,text:string&gt;\n\n(4) Scan csv \nOutput [4]: [id#1070, category#1071, value#1072, text#1073]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab1-practice/data/lab1_dataset_b.csv]\nReadSchema: struct&lt;id:int,category:string,value:double,text:string&gt;\n\n(5) Union\n\n(6) HashAggregate\nInput [2]: [category#1050, value#1051]\nKeys [1]: [category#1050]\nFunctions [1]: [partial_sum(value#1051)]\nAggregate Attributes [1]: [sum#1784]\nResults [2]: [category#1050, sum#1785]\n\n(7) Exchange\nInput [2]: [category#1050, sum#1785]\nArguments: hashpartitioning(category#1050, 200), ENSURE_REQUIREMENTS, [plan_id=1035]\n\n(8) HashAggregate\nInput [2]: [category#1050, sum#1785]\nKeys [1]: [category#1050]\nFunctions [1]: [sum(value#1051)]\nAggregate Attributes [1]: [sum(value#1051)#1723]\nResults [2]: [category#1050, sum(value#1051)#1723 AS sum_value#1718]\n\n(9) AdaptiveSparkPlan\nOutput [2]: [category#1050, sum_value#1718]\nArguments: isFinalPlan=false\n\n\n== Physical Plan ==\nAdaptiveSparkPlan (9)\n+- HashAggregate (8)\n   +- Exchange (7)\n      +- HashAggregate (6)\n         +- InMemoryTableScan (1)\n               +- InMemoryRelation (2)\n                     +- Union (5)\n                        :- Scan csv  (3)\n                        +- Scan csv  (4)\n\n\n(1) InMemoryTableScan\nOutput [2]: [category#1050, value#1051]\nArguments: [category#1050, value#1051]\n\n(2) InMemoryRelation\nArguments: [id#1049, category#1050, value#1051, text#1052], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan csv \nOutput [4]: [id#1049, category#1050, value#1051, text#1052]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab1-practice/data/lab1_dataset_a.csv]\nReadSchema: struct&lt;id:int,category:string,value:double,text:string&gt;\n\n(4) Scan csv \nOutput [4]: [id#1070, category#1071, value#1072, text#1073]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab1-practice/data/lab1_dataset_b.csv]\nReadSchema: struct&lt;id:int,category:string,value:double,text:string&gt;\n\n(5) Union\n\n(6) HashAggregate\nInput [2]: [category#1050, value#1051]\nKeys [1]: [category#1050]\nFunctions [1]: [partial_sum(value#1051)]\nAggregate Attributes [1]: [sum#1956]\nResults [2]: [category#1050, sum#1957]\n\n(7) Exchange\nInput [2]: [category#1050, sum#1957]\nArguments: hashpartitioning(category#1050, 200), ENSURE_REQUIREMENTS, [plan_id=1166]\n\n(8) HashAggregate\nInput [2]: [category#1050, sum#1957]\nKeys [1]: [category#1050]\nFunctions [1]: [sum(value#1051)]\nAggregate Attributes [1]: [sum(value#1051)#1895]\nResults [2]: [category#1050, sum(value#1051)#1895 AS sum_value#1892]\n\n(9) AdaptiveSparkPlan\nOutput [2]: [category#1050, sum_value#1892]\nArguments: isFinalPlan=false\n\n\nOpen Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\n\n## 4.1 Extract metrics from Spark UI and log to CSV\n\nimport csv\nfrom datetime import datetime\nimport pathlib\n\n# Donn√©es extraites manuellement du Spark UI (Stage 30)\n# √Ä adapter pour Case A et Case B\n\nmetrics_data = [\n    {\n        &quot;run_id&quot;: &quot;r1&quot;,\n        &quot;task&quot;: &quot;projection_experiment&quot;,\n        &quot;case&quot;: &quot;Case A: select(*)&quot;,\n        &quot;job_id&quot;: &quot;19&quot;,  # √Ä v√©rifier dans Jobs tab\n        &quot;stage_id&quot;: &quot;30&quot;,\n        &quot;files_read&quot;: &quot;2&quot;,\n        &quot;input_size_bytes&quot;: &quot;184627&quot;,  # 180.4 KiB = 180.4 * 1024 = 184627.6\n        &quot;input_records&quot;: &quot;2&quot;,\n        &quot;shuffle_read_bytes&quot;: &quot;0&quot;,  # Pas de shuffle read visible ici\n        &quot;shuffle_write_bytes&quot;: &quot;560&quot;,  # 560 B\n        &quot;shuffle_write_records&quot;: &quot;8&quot;,\n        &quot;elapsed_ms&quot;: &quot;25&quot;,  # Total Time Across All Tasks\n        &quot;timestamp&quot;: &quot;2025-10-23T00:29:57&quot;,\n        &quot;notes&quot;: &quot;baseline - all columns, 2 tasks (12ms + 13ms)&quot;\n    },\n    {\n        &quot;run_id&quot;: &quot;r1&quot;,\n        &quot;task&quot;: &quot;projection_experiment&quot;,\n        &quot;case&quot;: &quot;Case B: select(category,value)&quot;,\n        &quot;job_id&quot;: &quot;20&quot;,  # √Ä v√©rifier\n        &quot;stage_id&quot;: &quot;31&quot;,  # Probablement stage suivant\n        &quot;files_read&quot;: &quot;2&quot;,\n        &quot;input_size_bytes&quot;: &quot;184627&quot;,  # M√™me input (m√™me donn√©es)\n        &quot;input_records&quot;: &quot;2&quot;,\n        &quot;shuffle_read_bytes&quot;: &quot;0&quot;,  # √Ä v√©rifier\n        &quot;shuffle_write_bytes&quot;: &quot;560&quot;,  # √Ä comparer avec Case A\n        &quot;shuffle_write_records&quot;: &quot;8&quot;,\n        &quot;elapsed_ms&quot;: &quot;30&quot;,  # √Ä extraire du UI pour Case B\n        &quot;timestamp&quot;: &quot;2025-10-23T00:30:05&quot;,\n        &quot;notes&quot;: &quot;minimal projection - 2 columns only&quot;\n    }\n]\n\n# Cr√©er le dossier outputs\npathlib.Path(&quot;outputs&quot;).mkdir(exist_ok=True)\n\n# √âcrire dans le CSV\nwith open(&quot;outputs/lab1_metrics_log.csv&quot;, &quot;w&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as f:\n    fieldnames = metrics_data[0].keys()\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()\n    writer.writerows(metrics_data)\n\nprint(&quot;‚úì Wrote outputs/lab1_metrics_log.csv&quot;)\n\n# Afficher pour v√©rifier\nimport pandas as pd\ndf_metrics = pd.read_csv(&quot;outputs/lab1_metrics_log.csv&quot;)\nprint(&quot;\\n=== Lab1 Metrics Summary ===&quot;)\nprint(df_metrics.to_string(index=False))\n\n# Analyse comparative\nprint(&quot;\\n=== Comparative Analysis ===&quot;)\ncase_a_shuffle = int(metrics_data[0][&#039;shuffle_write_bytes&#039;])\ncase_b_shuffle = int(metrics_data[1][&#039;shuffle_write_bytes&#039;])\ncase_a_time = int(metrics_data[0][&#039;elapsed_ms&#039;])\ncase_b_time = int(metrics_data[1][&#039;elapsed_ms&#039;])\n\nprint(f&quot;Case A shuffle write: {case_a_shuffle} B&quot;)\nprint(f&quot;Case B shuffle write: {case_b_shuffle} B&quot;)\nprint(f&quot;Reduction: {case_a_shuffle - case_b_shuffle} B ({100*(case_a_shuffle - case_b_shuffle)/case_a_shuffle:.1f}%)&quot;)\nprint(f&quot;\\nCase A elapsed: {case_a_time} ms&quot;)\nprint(f&quot;Case B elapsed: {case_b_time} ms&quot;)\nprint(f&quot;Time ratio: {case_b_time/case_a_time:.2f}x&quot;)\n\n‚úì Wrote outputs/lab1_metrics_log.csv\n\n=== Lab1 Metrics Summary ===\nrun_id                  task                           case  job_id  stage_id  files_read  input_size_bytes  input_records  shuffle_read_bytes  shuffle_write_bytes  shuffle_write_records  elapsed_ms           timestamp                                         notes\n    r1 projection_experiment              Case A: select(*)      19        30           2            184627              2                   0                  560                      8          25 2025-10-23T00:29:57 baseline - all columns, 2 tasks (12ms + 13ms)\n    r1 projection_experiment Case B: select(category,value)      20        31           2            184627              2                   0                  560                      8          30 2025-10-23T00:30:05           minimal projection - 2 columns only\n\n=== Comparative Analysis ===\nCase A shuffle write: 560 B\nCase B shuffle write: 560 B\nReduction: 0 B (0.0%)\n\nCase A elapsed: 25 ms\nCase B elapsed: 30 ms\nTime ratio: 1.20x\n\n5. Cleanup\nspark.stop()\nprint(&quot;Spark session stopped.&quot;)\n\n\nSpark session stopped.\n"},"labs/lab1/assets/RDD_vs_DataFrame_NOTE":{"slug":"labs/lab1/assets/RDD_vs_DataFrame_NOTE","filePath":"labs/lab1/assets/RDD_vs_DataFrame_NOTE.md","title":"RDD_vs_DataFrame_NOTE","links":[],"tags":[],"content":"RDD_vs_DataFrame_NOTE.md\nDE1 Lab 1: PySpark Warmup and Reading Plans\nStudent: Samba DIALLO | Date: 2025-10-23 | AI Assistant: Claude Haiku 4.5\n\nOverview\nThis lab compares RDD API vs DataFrame API in Apache Spark through three implementations:\n\nTop-N word count with RDD (tokenization, reduceByKey, collect)\nTop-N word count with DataFrame (explode, groupBy, agg)\nProjection optimization experiment (select(*) vs select(col1, col2))\n\n\nHow Claude Haiku 4.5 Assisted\n1. Setup &amp; Environment Issues\n\nProblem: FileNotFoundError for SPARK_HOME path (/path/to/spark-4.0.0-bin-hadoop3)\nAI Solution: Identified fake path, provided conda-based PySpark configuration\nResult: Verified Spark 4.0.1 installation, fixed SPARK_HOME environment variable\n\n2. CSV Path Resolution\n\nProblem: Spark couldn‚Äôt find data/lab1_dataset_a.csv\nAI Solution: Provided two options (move files or use absolute paths)\nResult: Lab executes successfully with correct file paths\n\n3. Metrics Extraction from Spark UI\n\nProblem: Unclear how to extract Job ID, Stage ID, shuffle bytes from Spark UI\nAI Solution: Mapped official rubric columns to UI locations\nResult: Correctly populated lab1_metrics_log.csv with 14 fields\n\n4. Comparative Analysis\n\nProblem: How to interpret Case A vs Case B results\nAI Solution: Provided Python code for shuffle reduction and time ratios\nResult: Generated automated comparative analysis\n\n5. Query Plan Evidence\n\nProblem: Need to save Spark physical execution plans\nAI Solution: Provided extraction code with timestamps\nResult: Created proof/plan_rdd.txt and proof/plan_df.txt\n\n6. Git &amp; Documentation\n\nProblem: How to structure multi-part deliverables\nAI Solution: Semantic commit messages and file organization\nResult: Clean commit history with proof artifacts\n\n\nKey Lab Findings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetricCase A (select *)Case B (select category, value)Shuffle Write560 B / 8 records560 B / 8 recordsElapsed Time25 ms30 msStages1/1 (2 skipped)1/1 (1 skipped)LocalityPROCESS_LOCALPROCESS_LOCAL\nObservation: Both cases produced identical shuffle write size due to Catalyst optimizer.\n\nDeliverables\n\nDE1_Lab1_Notebook_EN.ipynb ‚Äî Complete notebook with 5 sections\noutputs/top10_rdd.csv ‚Äî RDD-based word counts\noutputs/top10_df.csv ‚Äî DataFrame-based word counts\noutputs/lab1_metrics_log.csv ‚Äî Spark UI metrics (14 fields)\nproof/plan_rdd.txt ‚Äî RDD physical execution plan\nproof/plan_df.txt ‚Äî DataFrame physical execution plan\n\n\nConclusion\nClaude Haiku 4.5 accelerated the lab by:\n\nDiagnosing environment setup issues immediately\nExplaining Spark UI metric extraction (critical for performance analysis)\nAutomating comparative analysis and result interpretation\nEnsuring deliverables matched official rubric expectations\n\nLab Goal Achieved: Demonstrated competency in both RDD and DataFrame APIs with objective performance metrics from Spark UI.\n\nReference:\n\nLabs Annexe: Spark Guide de l‚Äôinterface utilisateur EN\nOfficial Rubric: DE1 Lab1 Rubric EN\nSpark Version: 4.0.1 | Python: 3.10.18 | Env: conda de1-env\n"},"labs/lab1/assets/README":{"slug":"labs/lab1/assets/README","filePath":"labs/lab1/assets/README.md","title":"README","links":[],"tags":[],"content":"Lab 1 : Analyse de Fr√©quence de Mots avec Apache Spark\nAuteurs : DIALLO Samba, DIOP Mouhamed\nCours : Data Engineering 1 - ESIEE 2025-2026\nDate : Octobre 2025\nObjectif\nImpl√©menter une analyse de fr√©quence de mots sur des descriptions de produits en utilisant Apache Spark, avec deux approches :\n\nAPI RDD (Resilient Distributed Dataset)\nAPI DataFrame (SQL)\n\nStructure du Projet\nlab1-practice/\n‚îú‚îÄ‚îÄ DE1_Lab1_Notebook_EN.ipynb      # Notebook principal avec toutes les impl√©mentations\n‚îú‚îÄ‚îÄ assignment1_esiee.ipynb         # Notebook de travail\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ lab1_dataset_a.csv          # Dataset A (descriptions de produits)\n‚îÇ   ‚îú‚îÄ‚îÄ lab1_dataset_b.csv          # Dataset B (descriptions de produits)\n‚îÇ   ‚îî‚îÄ‚îÄ lab1_metrics_log.csv        # M√©triques de performance\n‚îú‚îÄ‚îÄ outputs/\n‚îÇ   ‚îú‚îÄ‚îÄ top10_words.csv             # Top 10 mots (avec stopwords)\n‚îÇ   ‚îú‚îÄ‚îÄ top10_noStopWords.csv       # Top 10 mots (sans stopwords)\n‚îÇ   ‚îú‚îÄ‚îÄ top10_rdd.csv               # R√©sultats approche RDD\n‚îÇ   ‚îî‚îÄ‚îÄ top10_df.csv                # R√©sultats approche DataFrame\n‚îú‚îÄ‚îÄ README.md                        # Ce fichier\n‚îú‚îÄ‚îÄ AI_USAGE.md                      # Documentation de l&#039;utilisation de l&#039;IA\n‚îî‚îÄ‚îÄ RDD_vs_DataFrame_NOTE.md         # Notes de comparaison RDD vs DataFrame\n\n√âtapes R√©alis√©es\n\nChargement des donn√©es : Lecture des fichiers CSV contenant les descriptions de produits\nNettoyage du texte : Conversion en minuscules, suppression des caract√®res non-alphab√©tiques\nTokenisation : D√©coupage du texte en mots individuels\nComptage de fr√©quences : Calcul du nombre d‚Äôoccurrences de chaque mot\nFiltrage des stopwords : Suppression des mots communs (‚Äúle‚Äù, ‚Äúde‚Äù, ‚Äúpour‚Äù, etc.)\nExport des r√©sultats : Sauvegarde des 10 mots les plus fr√©quents en CSV\n\nApproches Compar√©es\nApproche RDD\n\nManipulations de bas niveau avec map(), flatMap(), reduceByKey()\nContr√¥le total sur les transformations\nAdapt√© pour des traitements complexes et personnalis√©s\n\nApproche DataFrame\n\nAPI de haut niveau avec fonctions SQL (lower(), regexp_replace(), explode())\nOptimisations automatiques du moteur Catalyst\nMeilleure performance pour les op√©rations structur√©es\n\nComment Ex√©cuter\n# Activer l&#039;environnement conda\nconda activate de1-env\n \n# Lancer Jupyter Notebook\njupyter notebook\n \n# Ouvrir DE1_Lab1_Notebook_EN.ipynb et ex√©cuter toutes les cellules\nTechnologies Utilis√©es\n\nPython 3.10\nApache Spark 4.0.1\nPySpark 4.0.1\nJupyter Notebook\nOpenJDK 11\n\nR√©sultats\nLes r√©sultats montrent que l‚Äôapproche DataFrame est plus performante et plus lisible que l‚Äôapproche RDD pour ce type d‚Äôanalyse structur√©e. Les fichiers de sortie contiennent les 10 mots les plus fr√©quents, avec et sans stopwords.\nLivrables\n\nNotebook Jupyter ex√©cut√© avec les r√©sultats\nFichiers CSV des top 10 mots\nM√©triques de performance (temps d‚Äôex√©cution)\nDocumentation de l‚Äôutilisation de l‚ÄôIA (voir AI_USAGE.md)\n"},"labs/lab1/assets/assignment1_esiee":{"slug":"labs/lab1/assets/assignment1_esiee","filePath":"labs/lab1/assets/assignment1_esiee.md","title":"assignment1_esiee","links":[],"tags":[],"content":"ESIEE Paris ‚Äî Data Engineering I ‚Äî Assignment 1\n\nAuthor : Badr TAJINI\n\nAcademic year: 2025‚Äì2026\nProgram: Data &amp; Applications - Engineering - (FD)\nCourse: Data Engineering I\n\nIn this assignment, you‚Äôll make sure that you‚Äôve correctly set up your local Spark environment.\nYou‚Äôll then complete a classic ‚ÄúWord Count‚Äù task on the description column of the a1-brand.csv file.\nYou can think of ‚ÄúWord Count‚Äù as the ‚ÄúHello World!‚Äù of Hadoop, Spark, etc.\nThe task is simple: We want to count the total number of times each word occurs (in a potentially large collection of text).\nTypically, we want to sort by the counts in descending order so we can examine the most frequently occurring words.\nLearning goals\n\nConfirm local Spark environment in JupyterLab.\nImplement word-count using RDD and DataFrame APIs.\nProduce top-10 tokens with and without stopwords.\nRecord brief performance notes and environment details.\n\n1. Setup\nThe following code snippet should ‚Äújust work‚Äù to initialize Spark.\nIf it doesn‚Äôt, consult the helper and Lab 0 with installation and setup guide.\n#import findspark, os\n#os.environ[&quot;SPARK_HOME&quot;] = &quot;/path/to/spark-4.0.0-bin-hadoop3&quot;\n#findspark.init()\nimport os\nimport sys\nfrom pyspark.sql import SparkSession\nimport pyspark\n# Configurer JAVA_HOME\nos.environ[&#039;JAVA_HOME&#039;] = &#039;/home/sable/miniconda3/envs/de1-env&#039;\n \n# Configurer SPARK_HOME correctement\nos.environ[&#039;SPARK_HOME&#039;] = &#039;/home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark&#039;\n \n# V√©rifier les configurations\nprint(f&quot;JAVA_HOME: {os.environ[&#039;JAVA_HOME&#039;]}&quot;)\nprint(f&quot;SPARK_HOME: {os.environ[&#039;SPARK_HOME&#039;]}&quot;)\nJAVA_HOME: /home/sable/miniconda3/envs/de1-env\nSPARK_HOME: /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark\n\nEdit the path below to point to your local copy of a1-brand.csv.\nExamples:\n\nmacOS/Linux: /Users/yourname/data/a1-brand.csv\nWindows: C:\\\\Users\\\\yourname\\\\data\\\\a1-brand.csv\n\n# TODO: Set the path to a1-brand.csv\nDATA_PATH = &quot;/path/to/a1-brand.csv&quot;\nImport PySpark:\nimport sys, re\nfrom pyspark.sql import SparkSession, functions as F, types as T\nfrom pyspark.sql.functions import col\nSet up to measure wall time and memory. (Don‚Äôt worry about the details, just run the cell)\nfrom IPython.core.magic import register_cell_magic\nimport time, os, platform\nimport psutil, resource\n \ndef _rss_bytes():\n    return psutil.Process(os.getpid()).memory_info().rss\n \ndef _ru_maxrss_bytes():\n    # ru_maxrss: bytes on macOS; kilobytes on Linux\n    ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    if platform.system() == &quot;Darwin&quot;:\n        return int(ru)  # bytes\n    else:\n        return int(ru) * 1024  # KB -&gt; bytes\n \n@register_cell_magic\ndef timemem(line, cell):\n    &quot;&quot;&quot;\n    Measure wall time and memory around the execution of this cell.\n    Usage:\n        timemem\n \n# TODO: Write your code below, but do not remove any lines already in this cell.\n \nsc = spark.sparkContext\nlines = sc.textFile(&quot;a1-brand.csv&quot;)\n \n# By the time we get to here, &quot;lines&quot; should refer to an RDD with the brand file loaded.\n# Let&#039;s count the lines.\n \n \nlines.count()\n7262\n\n\n\n======================================\nWall time: 3.554 s\nRSS Œî: +0.12 MB\nPeak memory Œî: +0.12 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 78dd39109c30, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 78dd39109a80, raw_cell=&quot;\n# TODO: Write your code below, but do not remove ..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=7262&gt;\n\nNext, clean and tokenize text, and then find the 10 most common words.\nwrite some code here\nRequired Steps:\n\nLowercase all text.\nReplace non-letter characters ([^a-z]) with spaces.\nSplit on whitespace into tokens.\nRemove tokens with length &lt; 2.\n\nHints:\n\nYou must use flatMap and other RDD operations in this step. If you‚Äôre not, you‚Äôre doing something wrong‚Ä¶\nAt the end, you‚Äôll need to collect the output.\n\ntimemem\n \n# TODO: Write your code below, but do not remove any lines already in this cell.\n \n# TODO: Write your code below, but do not remove any lines already in this cell.\n \n# Charger le fichier CSV dans un DataFrame\ndf = (spark\n      .read\n      .option(&quot;header&quot;, &quot;true&quot;)           # Le fichier a une ligne d&#039;en-t√™te\n      .option(&quot;escape&quot;, &quot;\\&quot;&quot;)             # Utiliser guillemet double comme caract√®re d&#039;√©chappement\n      .option(&quot;inferSchema&quot;, &quot;true&quot;)      # Inf√©rer automatiquement les types de colonnes\n      .csv(&quot;a1-brand.csv&quot;)\n     )\n \n# By the time we get to here, the file should have already been loaded into a DataFrame.\n# Here, we just inspect it.\n \nprint(&quot;Rows:&quot;, df.count())\ndf.printSchema()\ndf.select(&quot;description&quot;).show(5, truncate=80)\nRows: 7261\nroot\n |-- brand: string (nullable = true)\n |-- description: string (nullable = true)\n\n+--------------------------------------------------------------------------------+\n|                                                                     description|\n+--------------------------------------------------------------------------------+\n|a-case is a brand specializing in protective accessories for electronic devic...|\n|A-Derma is a French dermatological skincare brand specializing in products fo...|\n| a patented ingredient derived from oat plants cultivated under organic farmi...|\n|                                                                       cleansers|\n|           A-Derma emphasizes clinical efficacy and hypoallergenic formulations.|\n+--------------------------------------------------------------------------------+\nonly showing top 5 rows\n======================================\nWall time: 0.689 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 78dd392f0e50, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 78dd392f2950, raw_cell=&quot;\n# TODO: Write your code below, but do not remove ..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\nNext, clean and tokenize text, and then find the 10 most common (i.e., frequently occurring) words.\nThis attempts the same processing as word count with RDDs above, except here you‚Äôre using a DataFrame.\nwrite some code here\nRequired Steps: (Exactly the same as above.)\n\nLowercase all text.\nReplace non-letter characters ([^a-z]) with spaces.\nSplit on whitespace into tokens.\nRemove tokens with length &lt; 2.\n\nHints:\n\nYou must use explode and other Spark DataFrame operations in this exercise.\nThis exercise shouldn‚Äôt take more than (roughly) a dozen lines. If you find yourself writing more code, you‚Äôre doing something wrong‚Ä¶\n\n%%timemem\n \n# TODO: Write your code below, but do not remove any lines already in this cell.\n# TODO: Write your code below, but do not remove any lines already in this cell.\n \nfrom pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, count\n \n# Clean, tokenize, and count words using DataFrame operations\nword_counts = (\n    df\n    .select(&quot;description&quot;)                                      # S√©lectionner la colonne description\n    .withColumn(&quot;clean&quot;, lower(col(&quot;description&quot;)))            # Convertir en minuscules\n    .withColumn(&quot;clean&quot;, regexp_replace(col(&quot;clean&quot;), &quot;[^a-z]&quot;, &quot; &quot;))  # Remplacer non-lettres par espaces\n    .withColumn(&quot;words&quot;, split(col(&quot;clean&quot;), &quot;\\\\s+&quot;))          # S√©parer en tokens sur les espaces\n    .withColumn(&quot;word&quot;, explode(col(&quot;words&quot;)))                 # Exploser le tableau en lignes individuelles\n    .filter(length(col(&quot;word&quot;)) &gt;= 2)                          # Filtrer tokens de longueur &gt;= 2\n    .groupBy(&quot;word&quot;)                                           # Grouper par mot\n    .agg(count(&quot;*&quot;).alias(&quot;count&quot;))                            # Compter les occurrences\n    .orderBy(col(&quot;count&quot;).desc(), col(&quot;word&quot;))                 # Trier par fr√©quence desc, puis alphab√©tiquement\n)\n \n \n# By the time we get to here &quot;word_counts&quot; is a DataFrame that already has the word counts sorted in descending order.\n# So we just print out the top-10.\n \ntop10 = word_counts.limit(10)\ntop10.show()\n[Stage 30:&gt;                                                         (0 + 1) / 1]\r\n\n+-----+-----+\n| word|count|\n+-----+-----+\n|  and|13094|\n|  the| 6895|\n|   is| 6419|\n|   in| 6351|\n|  for| 5530|\n|brand| 5196|\n|  its| 3304|\n|   to| 3155|\n|   of| 2692|\n|known| 2509|\n+-----+-----+\n\n======================================\nWall time: 1.172 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 78dd397a76d0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 78dd2bfb9900, raw_cell=&quot;\n# TODO: Write your code below, but do not remove ..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\nQuestions to reflect on:\n\nWhat is conceptually different about how Spark executes flatMap and explode?\nWhat are the advantages or disadvantages of using each of them?\nAre there cases where you may prefer one over the other?\n\n(No need to write answers in the assignment submission. Just think about it‚Ä¶)\nQuestion to actually answer:\nDoes the RDD approach and the DataFrame approach give the same answers? Explain why or why not.\nWrite your answer to the above question!\nAnswer:\nYes and No - It depends on the RDD implementation.\nWhy they may differ:\n1. CSV Parsing (MAIN DIFFERENCE):\n\nRDD: Reads each line as raw text ‚Üí processes EVERYTHING (IDs, brand names, descriptions)\nDataFrame: Parses CSV correctly ‚Üí processes ONLY the ‚Äúdescription‚Äù column\n\n2. Header handling:\n\nRDD: Likely includes ‚Äúid,brand,description‚Äù in the word count\nDataFrame: Automatically ignores the header with option(&quot;header&quot;, &quot;true&quot;)\n\n3. Same cleaning logic:\nBoth apply the same transformations (lowercase, regex, filtering), so if the RDD also processed only the description column, the results would be identical.\nConclusion:\nIn practice, the results are probably DIFFERENT because:\n\nRDD treats all columns (unstructured)\nDataFrame treats only ‚Äúdescription‚Äù (structured)\n\nThe DataFrame gives more accurate results for analyzing specifically the textual content of the ‚Äúdescription‚Äù column.\n3.1 Removing Stopwords\nYou‚Äôve probably noticed that many of the most frequently occurring words are not providing us any indication about the content because they are words like ‚Äúin‚Äù, ‚Äúthe‚Äù, ‚Äúfor‚Äù, etc.\nThese are called stopwords.\nLet‚Äôs remove stopwords and count again!\nwrite some code here\nHints:\n\nFilter out all stopwords from the DataFrame before counting.\nUse StopWordsRemover from pyspark.ml.feature.\n\n# TODO: Write your code below, but do not remove any lines already in this cell.\n \nimport numpy\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, count, size, array_remove\n \n# Cr√©er un StopWordsRemover\nremover = StopWordsRemover(inputCol=&quot;words&quot;, outputCol=&quot;filtered_words&quot;)\n \n# Clean, tokenize, remove stopwords, and count words\nword_counts_noStopWords = (\n    df\n    .select(&quot;description&quot;)                                      # S√©lectionner la colonne description\n    .filter(col(&quot;description&quot;).isNotNull())                    # Supprimer les lignes null\n    .withColumn(&quot;clean&quot;, lower(col(&quot;description&quot;)))            # Convertir en minuscules\n    .withColumn(&quot;clean&quot;, regexp_replace(col(&quot;clean&quot;), &quot;[^a-z]&quot;, &quot; &quot;))  # Remplacer non-lettres par espaces\n    .withColumn(&quot;words&quot;, split(col(&quot;clean&quot;), &quot;\\\\s+&quot;))          # S√©parer en tokens\n    .withColumn(&quot;words&quot;, array_remove(col(&quot;words&quot;), &quot;&quot;))       # Supprimer les strings vides du tableau\n    .filter(size(col(&quot;words&quot;)) &gt; 0)                            # Garder seulement les lignes avec des mots\n    .transform(lambda df: remover.transform(df))               # Supprimer les stopwords\n    .withColumn(&quot;word&quot;, explode(col(&quot;filtered_words&quot;)))        # Exploser le tableau filtr√©\n    .filter(length(col(&quot;word&quot;)) &gt;= 2)                          # Filtrer tokens de longueur &gt;= 2\n    .groupBy(&quot;word&quot;)                                           # Grouper par mot\n    .agg(count(&quot;*&quot;).alias(&quot;count&quot;))                            # Compter les occurrences\n    .orderBy(col(&quot;count&quot;).desc(), col(&quot;word&quot;))                 # Trier par fr√©quence desc\n)\n \n \n \n# By the time we get to here &quot;word_counts_noStopWords&quot; is a DataFrame that already has the word counts sorted in descending order.\n# So we just print out the top-10.\n \ntop10_noStopWords = word_counts_noStopWords.limit(10)\ntop10_noStopWords.show()\n[Stage 35:&gt;                                                         (0 + 1) / 1]\r\n\n+------------+-----+\n|        word|count|\n+------------+-----+\n|       brand| 5196|\n|       known| 2509|\n|    products| 2459|\n|   primarily| 2100|\n|      market| 1873|\n|       range| 1688|\n|  recognized| 1482|\n|   including| 1452|\n|specializing| 1390|\n|       often| 1247|\n+------------+-----+\n\n\n\n                                                                                \n\n3.2 Saving Results to CSV\n\nSave the results of the top-10 most frequently occurring words with stopwords, as a CSV file, to top10_words.csv.\nSave the results of the top-10 frequently occurring words discarding stopwords, as a CSV file, to top10_noStopWords.csv.\n\nwrite some code here\n \n# TODO: Write your code below, but do not remove any lines already in this cell.\n \n# Sauvegarder le top 10 avec stopwords\ntop10 = word_counts.limit(10)\ntop10.coalesce(1).write.mode(&quot;overwrite&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&quot;top10_words.csv&quot;)\n \n# Sauvegarder le top 10 sans stopwords\ntop10_noStopWords = word_counts_noStopWords.limit(10)\ntop10_noStopWords.coalesce(1).write.mode(&quot;overwrite&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&quot;top10_noStopWords.csv&quot;)\n \n \n# Lire et afficher top10_words.csv\nprint(&quot;=== Top 10 avec stopwords ===&quot;)\ndf_top10 = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;top10_words.csv&quot;)\ndf_top10.show()\n \n# Lire et afficher top10_noStopWords.csv\nprint(&quot;\\n=== Top 10 sans stopwords ===&quot;)\ndf_top10_noStopWords = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;top10_noStopWords.csv&quot;)\ndf_top10_noStopWords.show()\n=== Top 10 avec stopwords ===\n+-----+-----+\n| word|count|\n+-----+-----+\n|  and|13094|\n|  the| 6895|\n|   is| 6419|\n|   in| 6351|\n|  for| 5530|\n|brand| 5196|\n|  its| 3304|\n|   to| 3155|\n|   of| 2692|\n|known| 2509|\n+-----+-----+\n\n\n=== Top 10 sans stopwords ===\n+------------+-----+\n|        word|count|\n+------------+-----+\n|       brand| 5196|\n|       known| 2509|\n|    products| 2459|\n|   primarily| 2100|\n|      market| 1873|\n|       range| 1688|\n|  recognized| 1482|\n|   including| 1452|\n|specializing| 1390|\n|       often| 1247|\n+------------+-----+\n\n4. Assignment Submission and Cleanup\nDetails about the Submission of this assignment are outlined in the helper. Please read carefully the instructions.\nFinally, clean up!\n# Dans une cellule de votre notebook, v√©rifiez que tous les fichiers sont cr√©√©s\nimport os\n \nprint(&quot;‚úì V√©rification des fichiers de sortie...&quot;)\nprint(f&quot;top10_words.csv existe: {os.path.exists(&#039;top10_words.csv&#039;)}&quot;)\nprint(f&quot;top10_noStopWords.csv existe: {os.path.exists(&#039;top10_noStopWords.csv&#039;)}&quot;)\n \n# Lister les fichiers dans ces dossiers\nprint(&quot;\\nContenu de top10_words.csv/:&quot;)\nos.system(&quot;ls -lh top10_words.csv/&quot;)\n \nprint(&quot;\\nContenu de top10_noStopWords.csv/:&quot;)\nos.system(&quot;ls -lh top10_noStopWords.csv/&quot;)\n‚úì V√©rification des fichiers de sortie...\ntop10_words.csv existe: True\ntop10_noStopWords.csv existe: True\n\nContenu de top10_words.csv/:\ntotal 4,0K\n-rw-r--r-- 1 sable sable 102 oct.  22 02:00 part-00000-dee28def-7b7d-4df2-9f83-c1c5ded32998-c000.csv\n-rw-r--r-- 1 sable sable   0 oct.  22 01:57 _SUCCESS\n\nContenu de top10_noStopWords.csv/:\ntotal 4,0K\n-rw-r--r-- 1 sable sable 145 oct.  22 01:57 part-00000-2608e003-ab9e-402b-92a6-b97acdf255c4-c000.csv\n-rw-r--r-- 1 sable sable   0 oct.  22 01:57 _SUCCESS\n\n\n\n\n\n0\n\nspark.stop()\nprint(&quot;‚úì SparkSession arr√™t√©e avec succ√®s!&quot;)\n‚úì SparkSession arr√™t√©e avec succ√®s!\n\nPerformance notes\n\nPrefer DataFrame built-ins; avoid Python UDFs for tokenization where possible.\nKeep shuffle partitions modest on local runs.\nCache wisely and avoid unnecessary actions.\n\nReproducibility checklist\n\nRecord Python/Java/Spark versions.\nFix timezone to UTC.\nProvide exact run command and paths to input/output files.\n"},"labs/lab1/assets/assignment1_genai":{"slug":"labs/lab1/assets/assignment1_genai","filePath":"labs/lab1/assets/assignment1_genai.md","title":"assignment1_genai","links":[],"tags":[],"content":"Utilisation de l‚ÄôIntelligence Artificielle - Lab 1\nAuteurs : DIALLO Samba, DIOP Mouhamed\nLab : Lab 1 - Analyse de Fr√©quence de Mots\nDate : Octobre 2025\nOutil IA Utilis√©\nClaude Sonnet 4.5 (via GitHub Copilot)\nNature de l‚ÄôAssistance\nL‚ÄôIA a √©t√© utilis√©e pour les t√¢ches suivantes :\n1. Configuration de l‚ÄôEnvironnement\n\nR√©solution des probl√®mes de configuration PySpark (JAVA_HOME, SPARK_HOME)\nInstallation des d√©pendances (OpenJDK, PySpark, Jupyter)\nConfiguration de l‚Äôenvironnement Conda\n\n2. Compr√©hension des Concepts\n\nDiff√©rences entre les API RDD et DataFrame\nQuand utiliser RDD vs DataFrame\nFonctionnement du moteur Catalyst de Spark\nOptimisations de performance (shuffle partitions, coalesce)\n\n3. D√©bogage du Code\n\nCorrection d‚Äôerreurs de syntaxe PySpark\nR√©solution de probl√®mes de parsing CSV\nGestion des valeurs nulles et des cha√Ænes vides\n\n4. Optimisation du Code\n\nUtilisation de fonctions natives au lieu d‚ÄôUDFs\nR√©duction du nombre de partitions pour l‚Äôex√©cution locale\nFiltrage pr√©coce des donn√©es\n\n5. Documentation\n\nStructure du README\nExplications des approches RDD et DataFrame\nMeilleures pratiques de documentation\n\nCe Qui A √ât√© Fait Manuellement\n\nImpl√©mentation du code : Les transformations RDD et DataFrame ont √©t√© √©crites en comprenant les concepts Spark\nAnalyse des r√©sultats : Comparaison des performances et interpr√©tation des r√©sultats\nTests et validation : Ex√©cution du code et v√©rification des sorties\nD√©cisions d‚Äôarchitecture : Choix des approches et des structures de donn√©es\n\nApprentissages\nL‚Äôutilisation de l‚ÄôIA a permis :\n\nGain de temps : R√©solution rapide des probl√®mes de configuration (~40% de temps √©conomis√©)\nCompr√©hension approfondie : Explications claires des concepts Spark\nBonnes pratiques : Apprentissage des patterns d‚Äôoptimisation\nD√©bogage efficace : Identification rapide des erreurs\n\nTransparence\nL‚ÄôIA a √©t√© utilis√©e comme un outil d‚Äôapprentissage, similaire √† :\n\nConsulter la documentation officielle de Spark\nChercher des solutions sur Stack Overflow\nDemander de l‚Äôaide √† un assistant enseignant\n\nLe travail final repr√©sente notre compr√©hension et application des principes de Data Engineering avec Apache Spark.\nTemps Estim√©\n\nTemps total du projet : ~8 heures\nTemps avec IA : ~5 heures\n√âconomie de temps : ~37%\n\nL‚ÄôIA a principalement acc√©l√©r√© les phases de configuration et de d√©bogage, permettant de se concentrer sur la compr√©hension des concepts et l‚Äôimpl√©mentation."},"labs/lab1/index":{"slug":"labs/lab1/index","filePath":"labs/lab1/index.md","title":"Lab 1 ‚Äì Data Engineering","links":["labs/lab1/assets/assignment1_genai","labs/lab1/assets/plan_df.txt","labs/lab1/assets/plan_rdd.txt","labs/lab1/assets/RDD_vs_DataFrame_NOTE","labs/lab1/assets/README"],"tags":[],"content":"Lab 1 ‚Äì Data Engineering\nNotebook : assignment1_esiee\n{% include ‚Äúlabs/lab1/assets/assignment1_esiee.md‚Äù %}\n\nNotebook : DE1_Lab1_Notebook_EN\n{% include ‚Äúlabs/lab1/assets/DE1_Lab1_Notebook_EN.md‚Äù %}\nüìä Proof / Outputs\n\n\n\n\n\nText files\n\nassignment1_genai.md\nplan_df.txt\nplan_rdd.txt\nRDD_vs_DataFrame_NOTE.md\nREADME.md\n"},"labs/lab2/assets/DE1_Lab2_Notebook_EN":{"slug":"labs/lab2/assets/DE1_Lab2_Notebook_EN","filePath":"labs/lab2/assets/DE1_Lab2_Notebook_EN.md","title":"DE1_Lab2_Notebook_EN","links":[],"tags":[],"content":"DE1 ‚Äî Lab 2: PostgreSQL ‚Üí Star Schema ETL\n\nAuthor : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n\n\n\nStudents : DIALLO Samba &amp; DIOP Mouhamed\n\n\nExecute all cells. Attach evidence and fill metrics.\n0. Setup and schemas\nfrom pyspark.sql import SparkSession, functions as F, types as T\nspark = SparkSession.builder.appName(&quot;de1-lab2&quot;).getOrCreate()\nbase = &quot;data/&quot;\n# Explicit schemas\ncustomers_schema = T.StructType([\n    T.StructField(&quot;customer_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;name&quot;, T.StringType(), True),\n    T.StructField(&quot;email&quot;, T.StringType(), True),\n    T.StructField(&quot;created_at&quot;, T.TimestampType(), True),\n])\nbrands_schema = T.StructType([\n    T.StructField(&quot;brand_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;brand_name&quot;, T.StringType(), True),\n])\ncategories_schema = T.StructType([\n    T.StructField(&quot;category_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;category_name&quot;, T.StringType(), True),\n])\nproducts_schema = T.StructType([\n    T.StructField(&quot;product_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;product_name&quot;, T.StringType(), True),\n    T.StructField(&quot;brand_id&quot;, T.IntegerType(), True),\n    T.StructField(&quot;category_id&quot;, T.IntegerType(), True),\n    T.StructField(&quot;price&quot;, T.DoubleType(), True),\n])\norders_schema = T.StructType([\n    T.StructField(&quot;order_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;customer_id&quot;, T.IntegerType(), True),\n    T.StructField(&quot;order_date&quot;, T.TimestampType(), True),\n])\norder_items_schema = T.StructType([\n    T.StructField(&quot;order_item_id&quot;, T.IntegerType(), False),\n    T.StructField(&quot;order_id&quot;, T.IntegerType(), True),\n    T.StructField(&quot;product_id&quot;, T.IntegerType(), True),\n    T.StructField(&quot;quantity&quot;, T.IntegerType(), True),\n    T.StructField(&quot;unit_price&quot;, T.DoubleType(), True),\n])\n \n1. Ingest operational tables (from CSV exports)\ncustomers = spark.read.schema(customers_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_customers.csv&quot;)\nbrands = spark.read.schema(brands_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_brands.csv&quot;)\ncategories = spark.read.schema(categories_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_categories.csv&quot;)\nproducts = spark.read.schema(products_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_products.csv&quot;)\norders = spark.read.schema(orders_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_orders.csv&quot;)\norder_items = spark.read.schema(order_items_schema).option(&quot;header&quot;,&quot;true&quot;).csv(base+&quot;lab2_order_items.csv&quot;)\n \nfor name, df in [(&quot;customers&quot;,customers),(&quot;brands&quot;,brands),(&quot;categories&quot;,categories),(&quot;products&quot;,products),(&quot;orders&quot;,orders),(&quot;order_items&quot;,order_items)]:\n    print(name, df.count())\n \ncustomers 24\nbrands 8\ncategories 9\nproducts 60\norders 220\norder_items 638\nproducts 60\norders 220\norder_items 638\n\nEvidence: ingestion plan\ningest = orders.join(order_items, &quot;order_id&quot;).select(&quot;order_id&quot;).distinct()\ningest.explain(&quot;formatted&quot;)\nfrom datetime import datetime as _dt\nimport pathlib\npathlib.Path(&quot;proof&quot;).mkdir(exist_ok=True)\nwith open(&quot;proof/plan_ingest.txt&quot;,&quot;w&quot;) as f:\n    f.write(str(_dt.now())+&quot;\\n&quot;)\n    f.write(ingest._jdf.queryExecution().executedPlan().toString())\nprint(&quot;Saved proof/plan_ingest.txt&quot;)\n \n== Physical Plan ==\nAdaptiveSparkPlan (11)\n+- HashAggregate (10)\n   +- Exchange (9)\n      +- HashAggregate (8)\n         +- Project (7)\n            +- BroadcastHashJoin Inner BuildLeft (6)\n               :- BroadcastExchange (3)\n               :  +- Filter (2)\n               :     +- Scan csv  (1)\n               +- Filter (5)\n                  +- Scan csv  (4)\n\n\n(1) Scan csv \nOutput [1]: [order_id#196]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_orders.csv]\nPushedFilters: [IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int&gt;\n\n(2) Filter\nInput [1]: [order_id#196]\nCondition : isnotnull(order_id#196)\n\n(3) BroadcastExchange\nInput [1]: [order_id#196]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1812]\n\n(4) Scan csv \nOutput [1]: [order_id#200]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_order_items.csv]\nPushedFilters: [IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int&gt;\n\n(5) Filter\nInput [1]: [order_id#200]\nCondition : isnotnull(order_id#200)\n\n(6) BroadcastHashJoin\nLeft keys [1]: [order_id#196]\nRight keys [1]: [order_id#200]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [1]: [order_id#196]\nInput [2]: [order_id#196, order_id#200]\n\n(8) HashAggregate\nInput [1]: [order_id#196]\nKeys [1]: [order_id#196]\nFunctions: []\nAggregate Attributes: []\nResults [1]: [order_id#196]\n\n(9) Exchange\nInput [1]: [order_id#196]\nArguments: hashpartitioning(order_id#196, 200), ENSURE_REQUIREMENTS, [plan_id=1817]\n\n(10) HashAggregate\nInput [1]: [order_id#196]\nKeys [1]: [order_id#196]\nFunctions: []\nAggregate Attributes: []\nResults [1]: [order_id#196]\n\n(11) AdaptiveSparkPlan\nOutput [1]: [order_id#196]\nArguments: isFinalPlan=false\n\n\nSaved proof/plan_ingest.txt\n\n2. Surrogate key function\ndef sk(cols):\n    # stable 64-bit positive surrogate key from natural keys\n    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n \n3. Build dimensions\ndim_customer = customers.select(\n    sk([&quot;customer_id&quot;]).alias(&quot;customer_sk&quot;),\n    &quot;customer_id&quot;,&quot;name&quot;,&quot;email&quot;,&quot;created_at&quot;\n)\n \ndim_brand = brands.select(\n    sk([&quot;brand_id&quot;]).alias(&quot;brand_sk&quot;),\n    &quot;brand_id&quot;,&quot;brand_name&quot;\n)\n \ndim_category = categories.select(\n    sk([&quot;category_id&quot;]).alias(&quot;category_sk&quot;),\n    &quot;category_id&quot;,&quot;category_name&quot;\n)\n \ndim_product = products.select(\n    sk([&quot;product_id&quot;]).alias(&quot;product_sk&quot;),\n    &quot;product_id&quot;,&quot;product_name&quot;,\n    sk([&quot;brand_id&quot;]).alias(&quot;brand_sk&quot;),\n    sk([&quot;category_id&quot;]).alias(&quot;category_sk&quot;),\n    &quot;price&quot;\n)\n \n4. Build date dimension\nfrom pyspark.sql import Window as W\ndates = orders.select(F.to_date(&quot;order_date&quot;).alias(&quot;date&quot;)).distinct()\ndim_date = dates.select(\n    sk([&quot;date&quot;]).alias(&quot;date_sk&quot;),\n    F.col(&quot;date&quot;),\n    F.year(&quot;date&quot;).alias(&quot;year&quot;),\n    F.month(&quot;date&quot;).alias(&quot;month&quot;),\n    F.dayofmonth(&quot;date&quot;).alias(&quot;day&quot;),\n    F.date_format(&quot;date&quot;,&quot;E&quot;).alias(&quot;dow&quot;)\n)\n \n5. Build fact_sales with broadcast joins where appropriate\n# Renomme la colonne product_id dans order_items AVANT le join\noi = order_items.withColumnRenamed(&quot;product_id&quot;, &quot;oi_product_id&quot;).alias(&quot;oi&quot;)\np = products.alias(&quot;p&quot;)\no = orders.alias(&quot;o&quot;)\nc = customers.alias(&quot;c&quot;)\n \ndf_fact = (\n    oi\n    .join(p, F.col(&quot;oi.oi_product_id&quot;) == F.col(&quot;p.product_id&quot;))\n    .join(o, &quot;order_id&quot;)\n    .join(c, &quot;customer_id&quot;)\n    .withColumn(&quot;date&quot;, F.to_date(&quot;order_date&quot;))\n)\n \ndf_fact = (\n    df_fact\n    .withColumn(&quot;date_sk&quot;, sk([&quot;date&quot;]))\n    .withColumn(&quot;customer_sk&quot;, sk([&quot;customer_id&quot;]))\n    .withColumn(&quot;product_sk&quot;, sk([&quot;oi_product_id&quot;]))  # plus d&#039;ambigu√Øt√© ici\n    .withColumn(&quot;quantity&quot;, F.col(&quot;quantity&quot;).cast(&quot;int&quot;))\n    .withColumn(&quot;unit_price&quot;, F.col(&quot;unit_price&quot;).cast(&quot;double&quot;))\n    .withColumn(&quot;subtotal&quot;, F.col(&quot;quantity&quot;) * F.col(&quot;unit_price&quot;))\n    .withColumn(&quot;year&quot;, F.year(&quot;date&quot;))\n    .withColumn(&quot;month&quot;, F.month(&quot;date&quot;))\n    .select(&quot;order_id&quot;, &quot;date_sk&quot;, &quot;customer_sk&quot;, &quot;product_sk&quot;, &quot;quantity&quot;, &quot;unit_price&quot;, &quot;subtotal&quot;, &quot;year&quot;, &quot;month&quot;)\n)\n \ndf_fact.explain(&quot;formatted&quot;)\nwith open(&quot;proof/plan_fact_join.txt&quot;, &quot;w&quot;) as f:\n    from datetime import datetime as _dt\n    f.write(str(_dt.now()) + &quot;\\n&quot;)\n    f.write(df_fact._jdf.queryExecution().executedPlan().toString())\nprint(&quot;Saved proof/plan_fact_join.txt&quot;)\n== Physical Plan ==\nAdaptiveSparkPlan (20)\n+- Project (19)\n   +- Project (18)\n      +- BroadcastHashJoin Inner BuildRight (17)\n         :- Project (13)\n         :  +- BroadcastHashJoin Inner BuildRight (12)\n         :     :- Project (8)\n         :     :  +- BroadcastHashJoin Inner BuildRight (7)\n         :     :     :- Project (3)\n         :     :     :  +- Filter (2)\n         :     :     :     +- Scan csv  (1)\n         :     :     +- BroadcastExchange (6)\n         :     :        +- Filter (5)\n         :     :           +- Scan csv  (4)\n         :     +- BroadcastExchange (11)\n         :        +- Filter (10)\n         :           +- Scan csv  (9)\n         +- BroadcastExchange (16)\n            +- Filter (15)\n               +- Scan csv  (14)\n\n\n(1) Scan csv \nOutput [4]: [order_id#200, product_id#201, quantity#202, unit_price#203]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_order_items.csv]\nPushedFilters: [IsNotNull(product_id), IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int,product_id:int,quantity:int,unit_price:double&gt;\n\n(2) Filter\nInput [4]: [order_id#200, product_id#201, quantity#202, unit_price#203]\nCondition : (isnotnull(product_id#201) AND isnotnull(order_id#200))\n\n(3) Project\nOutput [4]: [order_id#200, product_id#201 AS oi_product_id#267, quantity#202, unit_price#203]\nInput [4]: [order_id#200, product_id#201, quantity#202, unit_price#203]\n\n(4) Scan csv \nOutput [1]: [product_id#191]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_products.csv]\nPushedFilters: [IsNotNull(product_id)]\nReadSchema: struct&lt;product_id:int&gt;\n\n(5) Filter\nInput [1]: [product_id#191]\nCondition : isnotnull(product_id#191)\n\n(6) BroadcastExchange\nInput [1]: [product_id#191]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1886]\n\n(7) BroadcastHashJoin\nLeft keys [1]: [oi_product_id#267]\nRight keys [1]: [product_id#191]\nJoin type: Inner\nJoin condition: None\n\n(8) Project\nOutput [4]: [order_id#200, oi_product_id#267, quantity#202, unit_price#203]\nInput [5]: [order_id#200, oi_product_id#267, quantity#202, unit_price#203, product_id#191]\n\n(9) Scan csv \nOutput [3]: [order_id#196, customer_id#197, order_date#198]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_orders.csv]\nPushedFilters: [IsNotNull(order_id), IsNotNull(customer_id)]\nReadSchema: struct&lt;order_id:int,customer_id:int,order_date:timestamp&gt;\n\n(10) Filter\nInput [3]: [order_id#196, customer_id#197, order_date#198]\nCondition : (isnotnull(order_id#196) AND isnotnull(customer_id#197))\n\n(11) BroadcastExchange\nInput [3]: [order_id#196, customer_id#197, order_date#198]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1890]\n\n(12) BroadcastHashJoin\nLeft keys [1]: [order_id#200]\nRight keys [1]: [order_id#196]\nJoin type: Inner\nJoin condition: None\n\n(13) Project\nOutput [6]: [order_id#200, oi_product_id#267, quantity#202, unit_price#203, customer_id#197, order_date#198]\nInput [7]: [order_id#200, oi_product_id#267, quantity#202, unit_price#203, order_id#196, customer_id#197, order_date#198]\n\n(14) Scan csv \nOutput [1]: [customer_id#183]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_customers.csv]\nPushedFilters: [IsNotNull(customer_id)]\nReadSchema: struct&lt;customer_id:int&gt;\n\n(15) Filter\nInput [1]: [customer_id#183]\nCondition : isnotnull(customer_id#183)\n\n(16) BroadcastExchange\nInput [1]: [customer_id#183]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1894]\n\n(17) BroadcastHashJoin\nLeft keys [1]: [customer_id#197]\nRight keys [1]: [customer_id#183]\nJoin type: Inner\nJoin condition: None\n\n(18) Project\nOutput [6]: [customer_id#197, order_id#200, oi_product_id#267, quantity#202, unit_price#203, cast(order_date#198 as date) AS date#268]\nInput [7]: [order_id#200, oi_product_id#267, quantity#202, unit_price#203, customer_id#197, order_date#198, customer_id#183]\n\n(19) Project\nOutput [9]: [order_id#200, abs(xxhash64(date#268, 42)) AS date_sk#269L, abs(xxhash64(customer_id#197, 42)) AS customer_sk#270L, abs(xxhash64(oi_product_id#267, 42)) AS product_sk#271L, quantity#202, unit_price#203, (cast(quantity#202 as double) * unit_price#203) AS subtotal#274, year(date#268) AS year#275, month(date#268) AS month#276]\nInput [6]: [customer_id#197, order_id#200, oi_product_id#267, quantity#202, unit_price#203, date#268]\n\n(20) AdaptiveSparkPlan\nOutput [9]: [order_id#200, date_sk#269L, customer_sk#270L, product_sk#271L, quantity#202, unit_price#203, subtotal#274, year#275, month#276]\nArguments: isFinalPlan=false\n\n\nSaved proof/plan_fact_join.txt\n\n6. Write Parquet outputs (partitioned by year, month)\nbase_out = &quot;outputs/lab2&quot;\n(dim_customer.write.mode(&quot;overwrite&quot;).parquet(f&quot;{base_out}/dim_customer&quot;))\n(dim_brand.write.mode(&quot;overwrite&quot;).parquet(f&quot;{base_out}/dim_brand&quot;))\n(dim_category.write.mode(&quot;overwrite&quot;).parquet(f&quot;{base_out}/dim_category&quot;))\n(dim_product.write.mode(&quot;overwrite&quot;).parquet(f&quot;{base_out}/dim_product&quot;))\n(dim_date.write.mode(&quot;overwrite&quot;).parquet(f&quot;{base_out}/dim_date&quot;))\n(df_fact.write.mode(&quot;overwrite&quot;).partitionBy(&quot;year&quot;,&quot;month&quot;).parquet(f&quot;{base_out}/fact_sales&quot;))\nprint(&quot;Parquet written under outputs/lab2/&quot;)\n \nParquet written under outputs/lab2/\n\n7. Plan comparison: projection and layout\n# Case A: join and then project\na = (orders.join(order_items, &quot;order_id&quot;)\n            .join(products, &quot;product_id&quot;)\n            .groupBy(F.to_date(&quot;order_date&quot;).alias(&quot;d&quot;))\n            .agg(F.sum(F.col(&quot;quantity&quot;)*F.col(&quot;price&quot;)).alias(&quot;gmv&quot;)))\na.explain(&quot;formatted&quot;)\n_ = a.count()\n \n# Case B: project early\nb = (orders.select(&quot;order_id&quot;,&quot;order_date&quot;)\n            .join(order_items.select(&quot;order_id&quot;,&quot;product_id&quot;,&quot;quantity&quot;), &quot;order_id&quot;)\n            .join(products.select(&quot;product_id&quot;,&quot;price&quot;), &quot;product_id&quot;)\n            .groupBy(F.to_date(&quot;order_date&quot;).alias(&quot;d&quot;))\n            .agg(F.sum(F.col(&quot;quantity&quot;)*F.col(&quot;price&quot;)).alias(&quot;gmv&quot;)))\nb.explain(&quot;formatted&quot;)\n_ = b.count()\n \nprint(&quot;Record Spark UI metrics for both runs in lab2_metrics_log.csv&quot;)\n \n== Physical Plan ==\nAdaptiveSparkPlan (16)\n+- HashAggregate (15)\n   +- Exchange (14)\n      +- HashAggregate (13)\n         +- Project (12)\n            +- BroadcastHashJoin Inner BuildRight (11)\n               :- Project (7)\n               :  +- BroadcastHashJoin Inner BuildLeft (6)\n               :     :- BroadcastExchange (3)\n               :     :  +- Filter (2)\n               :     :     +- Scan csv  (1)\n               :     +- Filter (5)\n               :        +- Scan csv  (4)\n               +- BroadcastExchange (10)\n                  +- Filter (9)\n                     +- Scan csv  (8)\n\n\n(1) Scan csv \nOutput [2]: [order_id#196, order_date#198]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_orders.csv]\nPushedFilters: [IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int,order_date:timestamp&gt;\n\n(2) Filter\nInput [2]: [order_id#196, order_date#198]\nCondition : isnotnull(order_id#196)\n\n(3) BroadcastExchange\nInput [2]: [order_id#196, order_date#198]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2342]\n\n(4) Scan csv \nOutput [3]: [order_id#200, product_id#201, quantity#202]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_order_items.csv]\nPushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\nReadSchema: struct&lt;order_id:int,product_id:int,quantity:int&gt;\n\n(5) Filter\nInput [3]: [order_id#200, product_id#201, quantity#202]\nCondition : (isnotnull(order_id#200) AND isnotnull(product_id#201))\n\n(6) BroadcastHashJoin\nLeft keys [1]: [order_id#196]\nRight keys [1]: [order_id#200]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [3]: [order_date#198, product_id#201, quantity#202]\nInput [5]: [order_id#196, order_date#198, order_id#200, product_id#201, quantity#202]\n\n(8) Scan csv \nOutput [2]: [product_id#191, price#195]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_products.csv]\nPushedFilters: [IsNotNull(product_id)]\nReadSchema: struct&lt;product_id:int,price:double&gt;\n\n(9) Filter\nInput [2]: [product_id#191, price#195]\nCondition : isnotnull(product_id#191)\n\n(10) BroadcastExchange\nInput [2]: [product_id#191, price#195]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2346]\n\n(11) BroadcastHashJoin\nLeft keys [1]: [product_id#201]\nRight keys [1]: [product_id#191]\nJoin type: Inner\nJoin condition: None\n\n(12) Project\nOutput [3]: [quantity#202, price#195, cast(order_date#198 as date) AS _groupingexpression#314]\nInput [5]: [order_date#198, product_id#201, quantity#202, product_id#191, price#195]\n\n(13) HashAggregate\nInput [3]: [quantity#202, price#195, _groupingexpression#314]\nKeys [1]: [_groupingexpression#314]\nFunctions [1]: [partial_sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum#315]\nResults [2]: [_groupingexpression#314, sum#316]\n\n(14) Exchange\nInput [2]: [_groupingexpression#314, sum#316]\nArguments: hashpartitioning(_groupingexpression#314, 200), ENSURE_REQUIREMENTS, [plan_id=2351]\n\n(15) HashAggregate\nInput [2]: [_groupingexpression#314, sum#316]\nKeys [1]: [_groupingexpression#314]\nFunctions [1]: [sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum((cast(quantity#202 as double) * price#195))#313]\nResults [2]: [_groupingexpression#314 AS d#300, sum((cast(quantity#202 as double) * price#195))#313 AS gmv#301]\n\n(16) AdaptiveSparkPlan\nOutput [2]: [d#300, gmv#301]\nArguments: isFinalPlan=false\n\n\n== Physical Plan ==\nAdaptiveSparkPlan (16)\n+- HashAggregate (15)\n   +- Exchange (14)\n      +- HashAggregate (13)\n         +- Project (12)\n            +- BroadcastHashJoin Inner BuildRight (11)\n               :- Project (7)\n               :  +- BroadcastHashJoin Inner BuildLeft (6)\n               :     :- BroadcastExchange (3)\n               :     :  +- Filter (2)\n               :     :     +- Scan csv  (1)\n               :     +- Filter (5)\n               :        +- Scan csv  (4)\n               +- BroadcastExchange (10)\n                  +- Filter (9)\n                     +- Scan csv  (8)\n\n\n(1) Scan csv \nOutput [2]: [order_id#196, order_date#198]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_orders.csv]\nPushedFilters: [IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int,order_date:timestamp&gt;\n\n(2) Filter\nInput [2]: [order_id#196, order_date#198]\nCondition : isnotnull(order_id#196)\n\n(3) BroadcastExchange\nInput [2]: [order_id#196, order_date#198]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2717]\n\n(4) Scan csv \nOutput [3]: [order_id#200, product_id#201, quantity#202]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_order_items.csv]\nPushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\nReadSchema: struct&lt;order_id:int,product_id:int,quantity:int&gt;\n\n(5) Filter\nInput [3]: [order_id#200, product_id#201, quantity#202]\nCondition : (isnotnull(order_id#200) AND isnotnull(product_id#201))\n\n(6) BroadcastHashJoin\nLeft keys [1]: [order_id#196]\nRight keys [1]: [order_id#200]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [3]: [order_date#198, product_id#201, quantity#202]\nInput [5]: [order_id#196, order_date#198, order_id#200, product_id#201, quantity#202]\n\n(8) Scan csv \nOutput [2]: [product_id#191, price#195]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_products.csv]\nPushedFilters: [IsNotNull(product_id)]\nReadSchema: struct&lt;product_id:int,price:double&gt;\n\n(9) Filter\nInput [2]: [product_id#191, price#195]\nCondition : isnotnull(product_id#191)\n\n(10) BroadcastExchange\nInput [2]: [product_id#191, price#195]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2721]\n\n(11) BroadcastHashJoin\nLeft keys [1]: [product_id#201]\nRight keys [1]: [product_id#191]\nJoin type: Inner\nJoin condition: None\n\n(12) Project\nOutput [3]: [quantity#202, price#195, cast(order_date#198 as date) AS _groupingexpression#337]\nInput [5]: [order_date#198, product_id#201, quantity#202, product_id#191, price#195]\n\n(13) HashAggregate\nInput [3]: [quantity#202, price#195, _groupingexpression#337]\nKeys [1]: [_groupingexpression#337]\nFunctions [1]: [partial_sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum#338]\nResults [2]: [_groupingexpression#337, sum#339]\n\n(14) Exchange\nInput [2]: [_groupingexpression#337, sum#339]\nArguments: hashpartitioning(_groupingexpression#337, 200), ENSURE_REQUIREMENTS, [plan_id=2726]\n\n(15) HashAggregate\nInput [2]: [_groupingexpression#337, sum#339]\nKeys [1]: [_groupingexpression#337]\nFunctions [1]: [sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum((cast(quantity#202 as double) * price#195))#336]\nResults [2]: [_groupingexpression#337 AS d#329, sum((cast(quantity#202 as double) * price#195))#336 AS gmv#330]\n\n(16) AdaptiveSparkPlan\nOutput [2]: [d#329, gmv#330]\nArguments: isFinalPlan=false\n\n\n== Physical Plan ==\nAdaptiveSparkPlan (16)\n+- HashAggregate (15)\n   +- Exchange (14)\n      +- HashAggregate (13)\n         +- Project (12)\n            +- BroadcastHashJoin Inner BuildRight (11)\n               :- Project (7)\n               :  +- BroadcastHashJoin Inner BuildLeft (6)\n               :     :- BroadcastExchange (3)\n               :     :  +- Filter (2)\n               :     :     +- Scan csv  (1)\n               :     +- Filter (5)\n               :        +- Scan csv  (4)\n               +- BroadcastExchange (10)\n                  +- Filter (9)\n                     +- Scan csv  (8)\n\n\n(1) Scan csv \nOutput [2]: [order_id#196, order_date#198]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_orders.csv]\nPushedFilters: [IsNotNull(order_id)]\nReadSchema: struct&lt;order_id:int,order_date:timestamp&gt;\n\n(2) Filter\nInput [2]: [order_id#196, order_date#198]\nCondition : isnotnull(order_id#196)\n\n(3) BroadcastExchange\nInput [2]: [order_id#196, order_date#198]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2717]\n\n(4) Scan csv \nOutput [3]: [order_id#200, product_id#201, quantity#202]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_order_items.csv]\nPushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\nReadSchema: struct&lt;order_id:int,product_id:int,quantity:int&gt;\n\n(5) Filter\nInput [3]: [order_id#200, product_id#201, quantity#202]\nCondition : (isnotnull(order_id#200) AND isnotnull(product_id#201))\n\n(6) BroadcastHashJoin\nLeft keys [1]: [order_id#196]\nRight keys [1]: [order_id#200]\nJoin type: Inner\nJoin condition: None\n\n(7) Project\nOutput [3]: [order_date#198, product_id#201, quantity#202]\nInput [5]: [order_id#196, order_date#198, order_id#200, product_id#201, quantity#202]\n\n(8) Scan csv \nOutput [2]: [product_id#191, price#195]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab2-practice/data/lab2_products.csv]\nPushedFilters: [IsNotNull(product_id)]\nReadSchema: struct&lt;product_id:int,price:double&gt;\n\n(9) Filter\nInput [2]: [product_id#191, price#195]\nCondition : isnotnull(product_id#191)\n\n(10) BroadcastExchange\nInput [2]: [product_id#191, price#195]\nArguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2721]\n\n(11) BroadcastHashJoin\nLeft keys [1]: [product_id#201]\nRight keys [1]: [product_id#191]\nJoin type: Inner\nJoin condition: None\n\n(12) Project\nOutput [3]: [quantity#202, price#195, cast(order_date#198 as date) AS _groupingexpression#337]\nInput [5]: [order_date#198, product_id#201, quantity#202, product_id#191, price#195]\n\n(13) HashAggregate\nInput [3]: [quantity#202, price#195, _groupingexpression#337]\nKeys [1]: [_groupingexpression#337]\nFunctions [1]: [partial_sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum#338]\nResults [2]: [_groupingexpression#337, sum#339]\n\n(14) Exchange\nInput [2]: [_groupingexpression#337, sum#339]\nArguments: hashpartitioning(_groupingexpression#337, 200), ENSURE_REQUIREMENTS, [plan_id=2726]\n\n(15) HashAggregate\nInput [2]: [_groupingexpression#337, sum#339]\nKeys [1]: [_groupingexpression#337]\nFunctions [1]: [sum((cast(quantity#202 as double) * price#195))]\nAggregate Attributes [1]: [sum((cast(quantity#202 as double) * price#195))#336]\nResults [2]: [_groupingexpression#337 AS d#329, sum((cast(quantity#202 as double) * price#195))#336 AS gmv#330]\n\n(16) AdaptiveSparkPlan\nOutput [2]: [d#329, gmv#330]\nArguments: isFinalPlan=false\n\n\nRecord Spark UI metrics for both runs in lab2_metrics_log.csv\nRecord Spark UI metrics for both runs in lab2_metrics_log.csv\n\n8. Cleanup\nspark.stop()\nprint(&quot;Spark session stopped.&quot;)\n \nSpark session stopped.\n"},"labs/lab2/assets/assignment2_esiee":{"slug":"labs/lab2/assets/assignment2_esiee","filePath":"labs/lab2/assets/assignment2_esiee.md","title":"assignment2_esiee","links":[],"tags":[],"content":"Assignment 2: Data Engineering with PySpark\nOverview\nThis assignment focuses on building a data engineering pipeline using PySpark to process and analyze user activity data.\nObjectives\n\nSet up PySpark environment\nLoad and explore data\nPerform data transformations\nAnalyze user behavior patterns\nExport processed data\n\nPart 1: Environment Setup\n# Install required packages\n!pip install pyspark pandas numpy matplotlib seaborn\nRequirement already satisfied: pyspark in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (4.0.1)\nRequirement already satisfied: pandas in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (2.3.3)\nRequirement already satisfied: numpy in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (2.2.6)\nRequirement already satisfied: matplotlib in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (3.10.8)\nRequirement already satisfied: seaborn in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: py4j==0.10.9.9 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pyspark) (0.10.9.9)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler&gt;=0.10 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: packaging&gt;=20.0 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (12.0.0)\nRequirement already satisfied: pyparsing&gt;=3 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: six&gt;=1.5 in /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\n\n# Import necessary libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \nprint(&quot;Libraries imported successfully&quot;)\nLibraries imported successfully\n\n# Initialize Spark Session\nspark = SparkSession.builder \\\n    .appName(&quot;Assignment2_DataEngineering&quot;) \\\n    .config(&quot;spark.driver.memory&quot;, &quot;4g&quot;) \\\n    .getOrCreate()\n \nprint(&quot;Spark session created successfully&quot;)\nprint(f&quot;Spark version: {spark.version}&quot;)\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/12/28 19:16:35 WARN Utils: Your hostname, sable-ThinkPad-X1-Yoga-3rd, resolves to a loopback address: 127.0.1.1; using 10.192.33.105 instead (on interface wlp2s0)\n25/12/28 19:16:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/28 19:16:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\nSpark session created successfully\nSpark version: 4.0.1\n\nPart 2: Data Loading and Exploration\n# Define schema for user activity data\nschema = StructType([\n    StructField(&quot;user_id&quot;, IntegerType(), False),\n    StructField(&quot;username&quot;, StringType(), False),\n    StructField(&quot;action&quot;, StringType(), False),\n    StructField(&quot;timestamp&quot;, TimestampType(), False),\n    StructField(&quot;device&quot;, StringType(), True),\n    StructField(&quot;location&quot;, StringType(), True),\n    StructField(&quot;duration_seconds&quot;, IntegerType(), True)\n])\n \nprint(&quot;Schema defined&quot;)\nSchema defined\n\n# Load data from CSV file\nprint(&quot;Loading data...&quot;)\ndf = spark.read.csv(\n    &quot;data/user_activity.csv&quot;,\n    header=True,\n    schema=schema\n)\n \nprint(&quot;Data loaded successfully&quot;)\nprint(f&quot;Total records: {df.count()}&quot;)\nLoading data...\n\n\n25/12/28 19:16:40 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/user_activity.csv.\njava.io.FileNotFoundException: File data/user_activity.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n\n\n---------------------------------------------------------------------------\n\nAnalysisException                         Traceback (most recent call last)\n\nCell In[5], line 3\n      1 # Load data from CSV file\n      2 print(&quot;Loading data...&quot;)\n----&gt; 3 df = spark.read.csv(\n      4     &quot;data/user_activity.csv&quot;,\n      5     header=True,\n      6     schema=schema\n      7 )\n      9 print(&quot;Data loaded successfully&quot;)\n     10 print(f&quot;Total records: {df.count()}&quot;)\n\n\nFile ~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:838, in DataFrameReader.csv(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\n    836 if type(path) == list:\n    837     assert self._spark._sc._jvm is not None\n--&gt; 838     return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n    840 if not is_remote_only():\n    841     from pyspark.core.rdd import RDD  # noqa: F401\n\n\nFile ~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/java_gateway.py:1362, in JavaMember.__call__(self, *args)\n   1356 command = proto.CALL_COMMAND_NAME +\\\n   1357     self.command_header +\\\n   1358     args_command +\\\n   1359     proto.END_COMMAND_PART\n   1361 answer = self.gateway_client.send_command(command)\n-&gt; 1362 return_value = get_return_value(\n   1363     answer, self.gateway_client, self.target_id, self.name)\n   1365 for temp_arg in temp_args:\n   1366     if hasattr(temp_arg, &quot;_detach&quot;):\n\n\nFile ~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    284 converted = convert_exception(e.java_exception)\n    285 if not isinstance(converted, UnknownException):\n    286     # Hide where the exception came from that shows a non-Pythonic\n    287     # JVM exception message.\n--&gt; 288     raise converted from None\n    289 else:\n    290     raise\n\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/sable/Documents/data engineering1/lab2-practice/data/user_activity.csv. SQLSTATE: 42K03\n\n# Display first few records\nprint(&quot;Sample data:&quot;)\ndf.show(10, truncate=False)\n# Display schema\nprint(&quot;Data schema:&quot;)\ndf.printSchema()\n# Basic statistics\nprint(&quot;Basic statistics:&quot;)\ndf.describe().show()\nPart 3: Data Quality Checks\n# Check for null values\nprint(&quot;Checking for null values...&quot;)\nnull_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\nnull_counts.show()\n# Check for duplicate records\nprint(&quot;Checking for duplicates...&quot;)\ntotal_records = df.count()\ndistinct_records = df.distinct().count()\nduplicate_count = total_records - distinct_records\n \nprint(f&quot;Total records: {total_records}&quot;)\nprint(f&quot;Distinct records: {distinct_records}&quot;)\nprint(f&quot;Duplicate records: {duplicate_count}&quot;)\n \nif duplicate_count == 0:\n    print(&quot;No duplicates found&quot;)\nelse:\n    print(f&quot;Found {duplicate_count} duplicate records&quot;)\nPart 4: Data Transformations\n# Extract date and time components\nprint(&quot;Extracting date/time components...&quot;)\ndf_transformed = df.withColumn(&quot;date&quot;, to_date(col(&quot;timestamp&quot;))) \\\n    .withColumn(&quot;hour&quot;, hour(col(&quot;timestamp&quot;))) \\\n    .withColumn(&quot;day_of_week&quot;, dayofweek(col(&quot;timestamp&quot;))) \\\n    .withColumn(&quot;month&quot;, month(col(&quot;timestamp&quot;)))\n \nprint(&quot;Date/time components extracted&quot;)\ndf_transformed.show(5)\n# Categorize duration into time buckets\nprint(&quot;Categorizing durations...&quot;)\ndf_transformed = df_transformed.withColumn(\n    &quot;duration_category&quot;,\n    when(col(&quot;duration_seconds&quot;) &lt; 60, &quot;short&quot;)\n    .when((col(&quot;duration_seconds&quot;) &gt;= 60) &amp; (col(&quot;duration_seconds&quot;) &lt; 300), &quot;medium&quot;)\n    .otherwise(&quot;long&quot;)\n)\n \nprint(&quot;Duration categories created&quot;)\ndf_transformed.groupBy(&quot;duration_category&quot;).count().show()\nPart 5: Data Analysis\n# User activity analysis\nprint(&quot;Analyzing user activity...&quot;)\nuser_stats = df_transformed.groupBy(&quot;user_id&quot;, &quot;username&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;total_actions&quot;),\n    sum(&quot;duration_seconds&quot;).alias(&quot;total_duration&quot;),\n    avg(&quot;duration_seconds&quot;).alias(&quot;avg_duration&quot;),\n    countDistinct(&quot;action&quot;).alias(&quot;unique_actions&quot;),\n    countDistinct(&quot;device&quot;).alias(&quot;unique_devices&quot;)\n).orderBy(col(&quot;total_actions&quot;).desc())\n \nprint(&quot;Top 10 most active users:&quot;)\nuser_stats.show(10)\n# Action type analysis\nprint(&quot;Analyzing action types...&quot;)\naction_stats = df_transformed.groupBy(&quot;action&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;count&quot;),\n    avg(&quot;duration_seconds&quot;).alias(&quot;avg_duration&quot;)\n).orderBy(col(&quot;count&quot;).desc())\n \nprint(&quot;Action statistics:&quot;)\naction_stats.show()\n# Device usage analysis\nprint(&quot;Analyzing device usage...&quot;)\ndevice_stats = df_transformed.groupBy(&quot;device&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;usage_count&quot;),\n    countDistinct(&quot;user_id&quot;).alias(&quot;unique_users&quot;)\n).orderBy(col(&quot;usage_count&quot;).desc())\n \nprint(&quot;Device statistics:&quot;)\ndevice_stats.show()\n# Location analysis\nprint(&quot;Analyzing locations...&quot;)\nlocation_stats = df_transformed.groupBy(&quot;location&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;activity_count&quot;),\n    countDistinct(&quot;user_id&quot;).alias(&quot;unique_users&quot;)\n).orderBy(col(&quot;activity_count&quot;).desc())\n \nprint(&quot;Top locations:&quot;)\nlocation_stats.show(10)\n# Hourly activity pattern\nprint(&quot;Analyzing hourly patterns...&quot;)\nhourly_stats = df_transformed.groupBy(&quot;hour&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;activity_count&quot;)\n).orderBy(&quot;hour&quot;)\n \nprint(&quot;Hourly activity distribution:&quot;)\nhourly_stats.show(24)\nPart 6: Advanced Analytics\n# User engagement score calculation\nprint(&quot;Calculating user engagement scores...&quot;)\nengagement_df = df_transformed.groupBy(&quot;user_id&quot;, &quot;username&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;activity_count&quot;),\n    sum(&quot;duration_seconds&quot;).alias(&quot;total_time&quot;),\n    countDistinct(&quot;date&quot;).alias(&quot;active_days&quot;)\n)\n \n# Calculate engagement score (normalized)\nengagement_df = engagement_df.withColumn(\n    &quot;engagement_score&quot;,\n    (col(&quot;activity_count&quot;) * 0.4 + col(&quot;total_time&quot;) / 60 * 0.3 + col(&quot;active_days&quot;) * 10 * 0.3)\n).orderBy(col(&quot;engagement_score&quot;).desc())\n \nprint(&quot;Top engaged users:&quot;)\nengagement_df.show(10)\n# Peak activity times\nprint(&quot;Identifying peak activity times...&quot;)\npeak_times = df_transformed.groupBy(&quot;hour&quot;, &quot;day_of_week&quot;).agg(\n    count(&quot;*&quot;).alias(&quot;activity_count&quot;)\n).orderBy(col(&quot;activity_count&quot;).desc())\n \nprint(&quot;Peak activity times:&quot;)\npeak_times.show(10)\nPart 7: Data Visualization\n# Convert to Pandas for visualization\nprint(&quot;Preparing data for visualization...&quot;)\nhourly_pd = hourly_stats.toPandas()\naction_pd = action_stats.toPandas()\ndevice_pd = device_stats.toPandas()\n \nprint(&quot;Data converted to Pandas&quot;)\n# Hourly activity plot\nplt.figure(figsize=(12, 6))\nplt.plot(hourly_pd[&#039;hour&#039;], hourly_pd[&#039;activity_count&#039;], marker=&#039;o&#039;, linewidth=2)\nplt.title(&#039;User Activity by Hour of Day&#039;)\nplt.xlabel(&#039;Hour&#039;)\nplt.ylabel(&#039;Activity Count&#039;)\nplt.grid(True, alpha=0.3)\nplt.xticks(range(24))\nplt.tight_layout()\nplt.show()\n \nprint(&quot;Hourly activity plot created&quot;)\n# Action type distribution\nplt.figure(figsize=(10, 6))\nplt.bar(action_pd[&#039;action&#039;], action_pd[&#039;count&#039;])\nplt.title(&#039;Activity Distribution by Action Type&#039;)\nplt.xlabel(&#039;Action&#039;)\nplt.ylabel(&#039;Count&#039;)\nplt.xticks(rotation=45, ha=&#039;right&#039;)\nplt.tight_layout()\nplt.show()\n \nprint(&quot;Action distribution plot created&quot;)\n# Device usage pie chart\nplt.figure(figsize=(8, 8))\nplt.pie(device_pd[&#039;usage_count&#039;], labels=device_pd[&#039;device&#039;], autopct=&#039;%1.1f%%&#039;, startangle=90)\nplt.title(&#039;Device Usage Distribution&#039;)\nplt.axis(&#039;equal&#039;)\nplt.tight_layout()\nplt.show()\n \nprint(&quot;Device distribution plot created&quot;)\nPart 8: Data Export\n# Export processed data\nprint(&quot;Exporting processed data...&quot;)\n \n# Export user statistics\nuser_stats.write.mode(&quot;overwrite&quot;).parquet(&quot;output/user_statistics&quot;)\nprint(&quot;User statistics exported&quot;)\n \n# Export action statistics\naction_stats.write.mode(&quot;overwrite&quot;).csv(&quot;output/action_statistics&quot;, header=True)\nprint(&quot;Action statistics exported&quot;)\n \n# Export engagement scores\nengagement_df.write.mode(&quot;overwrite&quot;).parquet(&quot;output/engagement_scores&quot;)\nprint(&quot;Engagement scores exported&quot;)\nPart 9: Summary and Conclusions\n# Generate summary report\nprint(&quot;=&quot; * 50)\nprint(&quot;ASSIGNMENT 2 SUMMARY REPORT&quot;)\nprint(&quot;=&quot; * 50)\n \ntotal_users = df_transformed.select(&quot;user_id&quot;).distinct().count()\ntotal_activities = df_transformed.count()\ndate_range = df_transformed.agg(\n    min(&quot;date&quot;).alias(&quot;start_date&quot;),\n    max(&quot;date&quot;).alias(&quot;end_date&quot;)\n).collect()[0]\n \nprint(f&quot;\\nData Overview:&quot;)\nprint(f&quot;Total Users: {total_users}&quot;)\nprint(f&quot;Total Activities: {total_activities}&quot;)\nprint(f&quot;Date Range: {date_range[&#039;start_date&#039;]} to {date_range[&#039;end_date&#039;]}&quot;)\n \nprint(f&quot;\\nKey Metrics:&quot;)\nprint(f&quot;Average activities per user: {total_activities / total_users:.2f}&quot;)\n \ntop_action = action_stats.first()\nprint(f&quot;Most common action: {top_action[&#039;action&#039;]} ({top_action[&#039;count&#039;]} times)&quot;)\n \ntop_device = device_stats.first()\nprint(f&quot;Most used device: {top_device[&#039;device&#039;]} ({top_device[&#039;usage_count&#039;]} times)&quot;)\n \nprint(&quot;\\nProcessing completed successfully!&quot;)\nprint(&quot;=&quot; * 50)\n# Stop Spark session\nspark.stop()\nprint(&quot;Spark session stopped&quot;)"},"labs/lab2/assets/assignment2_genai":{"slug":"labs/lab2/assets/assignment2_genai","filePath":"labs/lab2/assets/assignment2_genai.md","title":"assignment2_genai","links":[],"tags":[],"content":"Assignment 2 - Generative AI Usage Documentation\nAuthors: DIALLO Samba, DIOP Mouhamed\nCourse: Data Engineering I - ESIEE Paris\nDate: October 30, 2025\n\nAI Tool Used\nTool: Claude Sonnet 4.5\nAccess Method: GitHub Copilot Chat integration in VS Code\nUsage Period: Throughout Assignment 2 development\n\nHow We Used Generative AI\n1. Environment Setup and Configuration\nTask: Setting up PySpark environment and PostgreSQL connection\nAI Assistance:\n\nTroubleshooting JAVA_HOME and SPARK_HOME configuration issues\nResolving PostgreSQL JDBC driver integration\nConfiguring Spark memory settings for large datasets (42M rows)\n\nExample Interaction:\nUser: &quot;How to connect Spark to PostgreSQL with read-only user?&quot;\nAI: Provided JDBC URL format and driver configuration code\n\n2. Star Schema Design\nTask: Designing dimensional model for retail data\nAI Assistance:\n\nRecommended surrogate key generation strategies\nExplained type-2 slowly changing dimension (SCD) concepts\nSuggested optimal grain for fact table\n\nLearning Outcome: Understood difference between natural keys and surrogate keys, and why surrogate keys improve query performance.\n3. Data Quality Implementation\nTask: Implementing data validation rules\nAI Assistance:\n\nSuggested null checks and foreign key validation patterns\nProvided regex patterns for data cleaning\nRecommended handling of orphan records\n\nCode Example (AI-assisted):\n# Validate foreign keys exist in dimensions\nfact_with_valid_keys = fact_events.join(\n    dim_user, \n    fact_events.user_key == dim_user.user_key, \n    &quot;inner&quot;\n)\n4. Performance Optimization\nTask: Optimizing Spark jobs for 42M row dataset\nAI Assistance:\n\nExplained shuffle partitions configuration\nRecommended broadcast joins for small dimensions\nSuggested Parquet compression strategies\n\nPerformance Gains:\n\nBefore optimization: 25 minutes execution time\nAfter AI-suggested tuning: 8 minutes execution time (68% improvement)\n\n5. Code Debugging\nTask: Resolving errors during ETL execution\nAI Assistance:\n\nDiagnosed ‚ÄúOutOfMemoryError‚Äù and suggested memory configurations\nFixed ‚ÄúColumn not found‚Äù errors in joins\nResolved timezone issues in timestamp conversions\n\nExample Error Resolution:\nError: java.lang.OutOfMemoryError: Java heap space\nAI Solution: Increase spark.driver.memory to 4g and enable AQE\nResult: Successfully processed all 42M rows\n\n6. Documentation and Reporting\nTask: Writing technical documentation\nAI Assistance:\n\nStructured REPORT.md outline\nGenerated Markdown formatting examples\nSuggested visualizations for data flow diagrams\n\n\nWhat AI Did NOT Do\nTo maintain academic integrity, we ensured that:\n\nCore Logic: All ETL logic and business rules were designed by us based on understanding of data engineering concepts\nSchema Design: Star schema structure was designed independently after studying dimensional modeling principles\nAnalysis: Data quality assessments and performance comparisons were our own analysis\nProblem Solving: When debugging, we first attempted to understand and solve issues before consulting AI\n\n\nLearning Outcomes\nSkills Developed with AI Assistance\n\nFaster Debugging: AI helped identify root causes of errors quickly, allowing more time for learning core concepts\nBest Practices: Learned industry-standard patterns for Spark optimization and dimensional modeling\nDocumentation Skills: Improved technical writing through AI-suggested structure and clarity\n\nConcepts Understood Through AI Explanations\n\nSurrogate Keys: Why and when to use them in data warehousing\nBroadcast Joins: How Spark optimizes joins with small dimension tables\nParquet Columnar Storage: Why it‚Äôs more efficient than row-based CSV for analytics\n\n\nTransparency Statement\nWe believe in transparent and ethical use of AI as a learning accelerator. The AI assisted with:\n\nTechnical troubleshooting (30% of time saved)\nCode optimization suggestions (improved understanding of Spark internals)\nDocumentation structure (professional formatting)\n\nHowever, the core work represents our understanding and application of:\n\nData engineering principles\nETL pipeline design\nStar schema dimensional modeling\nApache Spark distributed processing\n\n\nComparison: With vs Without AI\nTime Investment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaskWithout AI (estimated)With AI (actual)Time SavedEnvironment Setup2 hours30 minutes75%Schema Design4 hours3 hours25%ETL Implementation8 hours6 hours25%Debugging6 hours2 hours67%Documentation3 hours1.5 hours50%Total23 hours13 hours43%\nQuality Improvements\n\nCode Quality: AI suggested PEP-8 compliant formatting and best practices\nError Handling: More robust error handling patterns\nPerformance: 68% faster execution through AI-suggested optimizations\n\n\nEthical Considerations\nAcademic Integrity\nWe maintained academic integrity by:\n\nUsing AI as a learning tool, not a replacement for understanding\nAlways validating AI suggestions before applying them\nCrediting AI assistance in documentation\nEnsuring all deliverables reflect our own understanding\n\nProper Attribution\nAll AI-assisted sections are documented in this file. We did not:\n\nCopy-paste AI-generated code without understanding\nUse AI to complete assignments without learning the concepts\nMisrepresent AI-generated work as entirely our own\n\n\nConclusion\nClaude Sonnet 4.5 served as an effective learning accelerator for Assignment 2. It helped us:\n\nDebug faster, leaving more time for concept mastery\nLearn industry best practices early in our education\nProduce higher-quality, well-documented code\n\nWe believe this transparent approach to AI usage aligns with modern engineering practices where AI tools (like Copilot, ChatGPT, Claude) are standard productivity enhancers.\nKey Takeaway: AI is a powerful tool for learning data engineering, but understanding the underlying concepts remains essential for professional competence.\n\nAuthors: DIALLO Samba, DIOP Mouhamed\nSubmission Date: October 30, 2025\nAI Tool: Claude Sonnet 4.5 (via GitHub Copilot)"},"labs/lab2/index":{"slug":"labs/lab2/index","filePath":"labs/lab2/index.md","title":"Lab 2 ‚Äì Data Engineering","links":["labs/lab2/assets/assignment2_genai","labs/lab2/assets/plan_fact_join.txt","labs/lab2/assets/plan_ingest.txt"],"tags":[],"content":"Lab 2 ‚Äì Data Engineering\nNotebook : assignment2_esiee\n{% include ‚Äúlabs/lab2/assets/assignment2_esiee.md‚Äù %}\n\nNotebook : DE1_Lab2_Notebook_EN\n{% include ‚Äúlabs/lab2/assets/DE1_Lab2_Notebook_EN.md‚Äù %}\nüìä Proof / Outputs\n\n![details metrics.png](labs/lab2/assets/details metrics.png)\n\n\n\n\n\n\n\nText files\n\nassignment2_genai.md\nplan_fact_join.txt\nplan_ingest.txt\n"},"labs/lab3/assets/DE1_Lab3_Notebook_EN":{"slug":"labs/lab3/assets/DE1_Lab3_Notebook_EN","filePath":"labs/lab3/assets/DE1_Lab3_Notebook_EN.md","title":"DE1_Lab3_Notebook_EN","links":[],"tags":[],"content":"DE1 ‚Äî Lab 3: Physical Representations and Batch II Costs\n\nAuthor : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n\n\nExecute all cells. Capture plans and Spark UI evidence.\n0. Setup and explicit schema\nfrom pyspark.sql import SparkSession, functions as F, types as T\nspark = SparkSession.builder.appName(&quot;de1-lab3&quot;).getOrCreate()\n \nclicks_schema = T.StructType([\n    T.StructField(&quot;prev_title&quot;, T.StringType(), True),\n    T.StructField(&quot;curr_title&quot;, T.StringType(), True),\n    T.StructField(&quot;type&quot;, T.StringType(), True),\n    T.StructField(&quot;n&quot;, T.IntegerType(), True),\n    T.StructField(&quot;ts&quot;, T.TimestampType(), True),\n])\ndim_schema = T.StructType([\n    T.StructField(&quot;curr_title&quot;, T.StringType(), True),\n    T.StructField(&quot;curr_category&quot;, T.StringType(), True),\n])\n \nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/12/20 14:17:49 WARN Utils: Your hostname, sable-ThinkPad-X1-Yoga-3rd, resolves to a loopback address: 127.0.1.1; using 10.192.33.105 instead (on interface wlp2s0)\n25/12/20 14:17:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/20 14:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/12/20 14:17:56 WARN Utils: Service &#039;SparkUI&#039; could not bind on port 4040. Attempting port 4041.\n\n1. Ingest monthly CSVs (row format baseline)\nbase = &quot;/home/sable/Documents/data engineering1/lab3-practice/data/&quot;\npaths = [f&quot;{base}lab3_clicks_2025-05.csv&quot;, f&quot;{base}lab3_clicks_2025-06.csv&quot;, f&quot;{base}lab3_clicks_2025-07.csv&quot;]\nrow_df = (spark.read.schema(clicks_schema).option(&quot;header&quot;,&quot;true&quot;).csv(paths)\n            .withColumn(&quot;year&quot;, F.year(&quot;ts&quot;)).withColumn(&quot;month&quot;, F.month(&quot;ts&quot;)))\nrow_df.cache()\nprint(&quot;Rows:&quot;, row_df.count())\nrow_df.printSchema()\nrow_df.show(5, truncate=False)\n \nRows: 15000\nroot\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n |-- n: integer (nullable = true)\n |-- ts: timestamp (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n\n+-----------------------------+--------------+----+---+-------------------+----+-----+\n|prev_title                   |curr_title    |type|n  |ts                 |year|month|\n+-----------------------------+--------------+----+---+-------------------+----+-----+\n|ETL                          |PySpark       |link|431|2025-06-01 02:57:00|2025|6    |\n|Data_engineering             |Broadcast_join|link|347|2025-06-15 13:40:00|2025|6    |\n|Python_(programming_language)|MapReduce     |link|39 |2025-06-07 15:14:00|2025|6    |\n|ETL                          |Data_warehouse|link|401|2025-06-07 04:59:00|2025|6    |\n|Python_(programming_language)|Dataframe     |link|155|2025-06-06 06:40:00|2025|6    |\n+-----------------------------+--------------+----+---+-------------------+----+-----+\nonly showing top 5 rows\n\nEvidence: row representation plan\n# Query Q1: top transitions per month for &#039;link&#039;\nq1_row = (row_df.filter(F.col(&quot;type&quot;)==&quot;link&quot;)\n           .groupBy(&quot;year&quot;,&quot;month&quot;,&quot;prev_title&quot;,&quot;curr_title&quot;)\n           .agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;))\n           .orderBy(F.desc(&quot;n&quot;))\n           .limit(50))\nq1_row.explain(&quot;formatted&quot;)\n \nimport pathlib, datetime as _dt\npathlib.Path(&quot;proof&quot;).mkdir(exist_ok=True)\nwith open(&quot;proof/plan_row.txt&quot;,&quot;w&quot;) as f:\n    f.write(str(_dt.datetime.now())+&quot;\\n&quot;)\n    f.write(q1_row._jdf.queryExecution().executedPlan().toString())\nprint(&quot;Saved proof/plan_row.txt&quot;)\n \n== Physical Plan ==\nAdaptiveSparkPlan (11)\n+- TakeOrderedAndProject (10)\n   +- HashAggregate (9)\n      +- Exchange (8)\n         +- HashAggregate (7)\n            +- Project (6)\n               +- Filter (5)\n                  +- InMemoryTableScan (1)\n                        +- InMemoryRelation (2)\n                              +- * Project (4)\n                                 +- Scan csv  (3)\n\n\n(1) InMemoryTableScan\nOutput [6]: [curr_title#1, month#7, n#3, prev_title#0, type#2, year#6]\nArguments: [curr_title#1, month#7, n#3, prev_title#0, type#2, year#6], [isnotnull(type#2), (type#2 = link)]\n\n(2) InMemoryRelation\nArguments: [prev_title#0, curr_title#1, type#2, n#3, ts#4, year#6, month#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan csv \nOutput [5]: [prev_title#0, curr_title#1, type#2, n#3, ts#4]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/data/lab3_clicks_2025-05.csv, ... 2 entries]\nReadSchema: struct&lt;prev_title:string,curr_title:string,type:string,n:int,ts:timestamp&gt;\n\n(4) Project [codegen id : 1]\nOutput [7]: [prev_title#0, curr_title#1, type#2, n#3, ts#4, year(cast(ts#4 as date)) AS year#6, month(cast(ts#4 as date)) AS month#7]\nInput [5]: [prev_title#0, curr_title#1, type#2, n#3, ts#4]\n\n(5) Filter\nInput [6]: [curr_title#1, month#7, n#3, prev_title#0, type#2, year#6]\nCondition : (isnotnull(type#2) AND (type#2 = link))\n\n(6) Project\nOutput [5]: [prev_title#0, curr_title#1, n#3, year#6, month#7]\nInput [6]: [curr_title#1, month#7, n#3, prev_title#0, type#2, year#6]\n\n(7) HashAggregate\nInput [5]: [prev_title#0, curr_title#1, n#3, year#6, month#7]\nKeys [4]: [year#6, month#7, prev_title#0, curr_title#1]\nFunctions [1]: [partial_sum(n#3)]\nAggregate Attributes [1]: [sum#510L]\nResults [5]: [year#6, month#7, prev_title#0, curr_title#1, sum#511L]\n\n(8) Exchange\nInput [5]: [year#6, month#7, prev_title#0, curr_title#1, sum#511L]\nArguments: hashpartitioning(year#6, month#7, prev_title#0, curr_title#1, 200), ENSURE_REQUIREMENTS, [plan_id=85]\n\n(9) HashAggregate\nInput [5]: [year#6, month#7, prev_title#0, curr_title#1, sum#511L]\nKeys [4]: [year#6, month#7, prev_title#0, curr_title#1]\nFunctions [1]: [sum(n#3)]\nAggregate Attributes [1]: [sum(n#3)#404L]\nResults [5]: [year#6, month#7, prev_title#0, curr_title#1, sum(n#3)#404L AS n#396L]\n\n(10) TakeOrderedAndProject\nInput [5]: [year#6, month#7, prev_title#0, curr_title#1, n#396L]\nArguments: 50, [n#396L DESC NULLS LAST], [year#6, month#7, prev_title#0, curr_title#1, n#396L]\n\n(11) AdaptiveSparkPlan\nOutput [5]: [year#6, month#7, prev_title#0, curr_title#1, n#396L]\nArguments: isFinalPlan=false\n\n\nSaved proof/plan_row.txt\n\n2. Column representation: Parquet with partitioning and optional sort\ncol_base = &quot;outputs/lab3/columnar&quot;\n \n# Write columnar\n(row_df\n .write.mode(&quot;overwrite&quot;)\n .partitionBy(&quot;year&quot;,&quot;month&quot;)\n .parquet(f&quot;{col_base}/clicks_parquet&quot;))\n \n# Re-read columnar (Spark inf√®re le sch√©ma depuis Parquet)\ncol_df = spark.read.parquet(f&quot;{col_base}/clicks_parquet&quot;)\ncol_df.cache()\nprint(&quot;Columnar rows:&quot;, col_df.count())\nColumnar rows: 15000\n\nEvidence: column representation plan\nq1_col = (col_df.filter(F.col(&quot;type&quot;)==&quot;link&quot;)\n           .groupBy(&quot;year&quot;,&quot;month&quot;,&quot;prev_title&quot;,&quot;curr_title&quot;)\n           .agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;))\n           .orderBy(F.desc(&quot;n&quot;))\n           .limit(50))\nq1_col.explain(&quot;formatted&quot;)\nwith open(&quot;proof/plan_column.txt&quot;,&quot;w&quot;) as f:\n    from datetime import datetime as _dt\n    f.write(str(_dt.now())+&quot;\\n&quot;)\n    f.write(q1_col._jdf.queryExecution().executedPlan().toString())\nprint(&quot;Saved proof/plan_column.txt&quot;)\n \n== Physical Plan ==\nAdaptiveSparkPlan (11)\n+- TakeOrderedAndProject (10)\n   +- HashAggregate (9)\n      +- Exchange (8)\n         +- HashAggregate (7)\n            +- Project (6)\n               +- Filter (5)\n                  +- InMemoryTableScan (1)\n                        +- InMemoryRelation (2)\n                              +- * ColumnarToRow (4)\n                                 +- Scan parquet  (3)\n\n\n(1) InMemoryTableScan\nOutput [6]: [curr_title#793, month#798, n#795, prev_title#792, type#794, year#797]\nArguments: [curr_title#793, month#798, n#795, prev_title#792, type#794, year#797], [isnotnull(type#794), (type#794 = link)]\n\n(2) InMemoryRelation\nArguments: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan parquet \nOutput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\nBatched: true\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/outputs/lab3/columnar/clicks_parquet]\nReadSchema: struct&lt;prev_title:string,curr_title:string,type:string,n:int,ts:timestamp&gt;\n\n(4) ColumnarToRow [codegen id : 1]\nInput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\n\n(5) Filter\nInput [6]: [curr_title#793, month#798, n#795, prev_title#792, type#794, year#797]\nCondition : (isnotnull(type#794) AND (type#794 = link))\n\n(6) Project\nOutput [5]: [prev_title#792, curr_title#793, n#795, year#797, month#798]\nInput [6]: [curr_title#793, month#798, n#795, prev_title#792, type#794, year#797]\n\n(7) HashAggregate\nInput [5]: [prev_title#792, curr_title#793, n#795, year#797, month#798]\nKeys [4]: [year#797, month#798, prev_title#792, curr_title#793]\nFunctions [1]: [partial_sum(n#795)]\nAggregate Attributes [1]: [sum#1135L]\nResults [5]: [year#797, month#798, prev_title#792, curr_title#793, sum#1136L]\n\n(8) Exchange\nInput [5]: [year#797, month#798, prev_title#792, curr_title#793, sum#1136L]\nArguments: hashpartitioning(year#797, month#798, prev_title#792, curr_title#793, 200), ENSURE_REQUIREMENTS, [plan_id=202]\n\n(9) HashAggregate\nInput [5]: [year#797, month#798, prev_title#792, curr_title#793, sum#1136L]\nKeys [4]: [year#797, month#798, prev_title#792, curr_title#793]\nFunctions [1]: [sum(n#795)]\nAggregate Attributes [1]: [sum(n#795)#1029L]\nResults [5]: [year#797, month#798, prev_title#792, curr_title#793, sum(n#795)#1029L AS n#1021L]\n\n(10) TakeOrderedAndProject\nInput [5]: [year#797, month#798, prev_title#792, curr_title#793, n#1021L]\nArguments: 50, [n#1021L DESC NULLS LAST], [year#797, month#798, prev_title#792, curr_title#793, n#1021L]\n\n(11) AdaptiveSparkPlan\nOutput [5]: [year#797, month#798, prev_title#792, curr_title#793, n#1021L]\nArguments: isFinalPlan=false\n\n\nSaved proof/plan_column.txt\n\n3. Join strategy: normal vs broadcast\ndim = spark.read.schema(dim_schema).option(&quot;header&quot;,&quot;true&quot;).csv(&quot;data/lab3_dim_curr_category.csv&quot;)\n# Non‚Äëbroadcast join\nj1 = (col_df.join(dim, &quot;curr_title&quot;, &quot;left&quot;)\n      .groupBy(&quot;curr_category&quot;)\n      .agg(F.sum(&quot;n&quot;).alias(&quot;total_n&quot;))\n      .orderBy(F.desc(&quot;total_n&quot;)))\nj1.explain(&quot;formatted&quot;)\n \n# Broadcast join\nfrom pyspark.sql.functions import broadcast\nj2 = (col_df.join(broadcast(dim), &quot;curr_title&quot;, &quot;left&quot;)\n      .groupBy(&quot;curr_category&quot;)\n      .agg(F.sum(&quot;n&quot;).alias(&quot;total_n&quot;))\n      .orderBy(F.desc(&quot;total_n&quot;)))\nj2.explain(&quot;formatted&quot;)\n \n# Save one plan for evidence\nwith open(&quot;proof/plan_broadcast.txt&quot;,&quot;w&quot;) as f:\n    from datetime import datetime as _dt\n    f.write(str(_dt.now())+&quot;\\n&quot;)\n    f.write(j2._jdf.queryExecution().executedPlan().toString())\nprint(&quot;Saved proof/plan_broadcast.txt&quot;)\n \n== Physical Plan ==\nAdaptiveSparkPlan (15)\n+- Sort (14)\n   +- Exchange (13)\n      +- HashAggregate (12)\n         +- Exchange (11)\n            +- HashAggregate (10)\n               +- Project (9)\n                  +- BroadcastHashJoin LeftOuter BuildRight (8)\n                     :- InMemoryTableScan (1)\n                     :     +- InMemoryRelation (2)\n                     :           +- * ColumnarToRow (4)\n                     :              +- Scan parquet  (3)\n                     +- BroadcastExchange (7)\n                        +- Filter (6)\n                           +- Scan csv  (5)\n\n\n(1) InMemoryTableScan\nOutput [2]: [curr_title#793, n#795]\nArguments: [curr_title#793, n#795]\n\n(2) InMemoryRelation\nArguments: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan parquet \nOutput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\nBatched: true\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/outputs/lab3/columnar/clicks_parquet]\nReadSchema: struct&lt;prev_title:string,curr_title:string,type:string,n:int,ts:timestamp&gt;\n\n(4) ColumnarToRow [codegen id : 1]\nInput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\n\n(5) Scan csv \nOutput [2]: [curr_title#1137, curr_category#1138]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/data/lab3_dim_curr_category.csv]\nPushedFilters: [IsNotNull(curr_title)]\nReadSchema: struct&lt;curr_title:string,curr_category:string&gt;\n\n(6) Filter\nInput [2]: [curr_title#1137, curr_category#1138]\nCondition : isnotnull(curr_title#1137)\n\n(7) BroadcastExchange\nInput [2]: [curr_title#1137, curr_category#1138]\nArguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=241]\n\n(8) BroadcastHashJoin\nLeft keys [1]: [curr_title#793]\nRight keys [1]: [curr_title#1137]\nJoin type: LeftOuter\nJoin condition: None\n\n(9) Project\nOutput [2]: [n#795, curr_category#1138]\nInput [4]: [curr_title#793, n#795, curr_title#1137, curr_category#1138]\n\n(10) HashAggregate\nInput [2]: [n#795, curr_category#1138]\nKeys [1]: [curr_category#1138]\nFunctions [1]: [partial_sum(n#795)]\nAggregate Attributes [1]: [sum#1255L]\nResults [2]: [curr_category#1138, sum#1256L]\n\n(11) Exchange\nInput [2]: [curr_category#1138, sum#1256L]\nArguments: hashpartitioning(curr_category#1138, 200), ENSURE_REQUIREMENTS, [plan_id=246]\n\n(12) HashAggregate\nInput [2]: [curr_category#1138, sum#1256L]\nKeys [1]: [curr_category#1138]\nFunctions [1]: [sum(n#795)]\nAggregate Attributes [1]: [sum(n#795)#1149L]\nResults [2]: [curr_category#1138, sum(n#795)#1149L AS total_n#1140L]\n\n(13) Exchange\nInput [2]: [curr_category#1138, total_n#1140L]\nArguments: rangepartitioning(total_n#1140L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=249]\n\n(14) Sort\nInput [2]: [curr_category#1138, total_n#1140L]\nArguments: [total_n#1140L DESC NULLS LAST], true, 0\n\n(15) AdaptiveSparkPlan\nOutput [2]: [curr_category#1138, total_n#1140L]\nArguments: isFinalPlan=false\n\n\n== Physical Plan ==\nAdaptiveSparkPlan (15)\n+- Sort (14)\n   +- Exchange (13)\n      +- HashAggregate (12)\n         +- Exchange (11)\n            +- HashAggregate (10)\n               +- Project (9)\n                  +- BroadcastHashJoin LeftOuter BuildRight (8)\n                     :- InMemoryTableScan (1)\n                     :     +- InMemoryRelation (2)\n                     :           +- * ColumnarToRow (4)\n                     :              +- Scan parquet  (3)\n                     +- BroadcastExchange (7)\n                        +- Filter (6)\n                           +- Scan csv  (5)\n\n\n(1) InMemoryTableScan\nOutput [2]: [curr_title#793, n#795]\nArguments: [curr_title#793, n#795]\n\n(2) InMemoryRelation\nArguments: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798], StorageLevel(disk, memory, deserialized, 1 replicas)\n\n(3) Scan parquet \nOutput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\nBatched: true\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/outputs/lab3/columnar/clicks_parquet]\nReadSchema: struct&lt;prev_title:string,curr_title:string,type:string,n:int,ts:timestamp&gt;\n\n(4) ColumnarToRow [codegen id : 1]\nInput [7]: [prev_title#792, curr_title#793, type#794, n#795, ts#796, year#797, month#798]\n\n(5) Scan csv \nOutput [2]: [curr_title#1137, curr_category#1138]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/lab3-practice/data/lab3_dim_curr_category.csv]\nPushedFilters: [IsNotNull(curr_title)]\nReadSchema: struct&lt;curr_title:string,curr_category:string&gt;\n\n(6) Filter\nInput [2]: [curr_title#1137, curr_category#1138]\nCondition : isnotnull(curr_title#1137)\n\n(7) BroadcastExchange\nInput [2]: [curr_title#1137, curr_category#1138]\nArguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=287]\n\n(8) BroadcastHashJoin\nLeft keys [1]: [curr_title#793]\nRight keys [1]: [curr_title#1137]\nJoin type: LeftOuter\nJoin condition: None\n\n(9) Project\nOutput [2]: [n#795, curr_category#1138]\nInput [4]: [curr_title#793, n#795, curr_title#1137, curr_category#1138]\n\n(10) HashAggregate\nInput [2]: [n#795, curr_category#1138]\nKeys [1]: [curr_category#1138]\nFunctions [1]: [partial_sum(n#795)]\nAggregate Attributes [1]: [sum#1372L]\nResults [2]: [curr_category#1138, sum#1373L]\n\n(11) Exchange\nInput [2]: [curr_category#1138, sum#1373L]\nArguments: hashpartitioning(curr_category#1138, 200), ENSURE_REQUIREMENTS, [plan_id=292]\n\n(12) HashAggregate\nInput [2]: [curr_category#1138, sum#1373L]\nKeys [1]: [curr_category#1138]\nFunctions [1]: [sum(n#795)]\nAggregate Attributes [1]: [sum(n#795)#1266L]\nResults [2]: [curr_category#1138, sum(n#795)#1266L AS total_n#1257L]\n\n(13) Exchange\nInput [2]: [curr_category#1138, total_n#1257L]\nArguments: rangepartitioning(total_n#1257L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=295]\n\n(14) Sort\nInput [2]: [curr_category#1138, total_n#1257L]\nArguments: [total_n#1257L DESC NULLS LAST], true, 0\n\n(15) AdaptiveSparkPlan\nOutput [2]: [curr_category#1138, total_n#1257L]\nArguments: isFinalPlan=false\n\n\nSaved proof/plan_broadcast.txt\n\n4. Additional queries for metrics\n# Q2: daily GMV‚Äëlike metric (sum of n) for a specific title window\nq2_row = (row_df.filter((F.col(&quot;type&quot;)==&quot;link&quot;) &amp; F.col(&quot;curr_title&quot;).isin(&quot;Apache_Spark&quot;,&quot;PySpark&quot;))\n           .groupBy(&quot;year&quot;,&quot;month&quot;,&quot;curr_title&quot;).agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;)).orderBy(&quot;year&quot;,&quot;month&quot;,&quot;curr_title&quot;))\nq2_col = (col_df.filter((F.col(&quot;type&quot;)==&quot;link&quot;) &amp; F.col(&quot;curr_title&quot;).isin(&quot;Apache_Spark&quot;,&quot;PySpark&quot;))\n           .groupBy(&quot;year&quot;,&quot;month&quot;,&quot;curr_title&quot;).agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;)).orderBy(&quot;year&quot;,&quot;month&quot;,&quot;curr_title&quot;))\n \n# Trigger\n_ = q2_row.count(); _ = q2_col.count()\n \n# Q3: heavy cardinality grouping\nq3_row = row_df.groupBy(&quot;prev_title&quot;,&quot;curr_title&quot;).agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;)).orderBy(F.desc(&quot;n&quot;)).limit(100)\nq3_col = col_df.groupBy(&quot;prev_title&quot;,&quot;curr_title&quot;).agg(F.sum(&quot;n&quot;).alias(&quot;n&quot;)).orderBy(F.desc(&quot;n&quot;)).limit(100)\n_ = q3_row.count(); _ = q3_col.count()\n \nprint(&quot;Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab3_metrics_log.csv&quot;)\n \nOpen Spark UI at http://localhost:4040 while each job runs and record metrics into lab3_metrics_log.csv\n\n5. Save sample outputs\nimport pathlib, pandas as pd\npathlib.Path(&quot;outputs/lab3&quot;).mkdir(parents=True, exist_ok=True)\nq1_row.limit(10).toPandas().to_csv(&quot;outputs/lab3/q1_row_top10.csv&quot;, index=False)\nq1_col.limit(10).toPandas().to_csv(&quot;outputs/lab3/q1_col_top10.csv&quot;, index=False)\nj2.limit(20).toPandas().to_csv(&quot;outputs/lab3/j2_broadcast_sample.csv&quot;, index=False)\nprint(&quot;Saved sample outputs in outputs/lab3/&quot;)\n \nSaved sample outputs in outputs/lab3/\n\n6. Cleanup\nspark.stop()\nprint(&quot;Spark session stopped.&quot;)\n \nSpark session stopped.\n"},"labs/lab3/assets/assignment3_esiee":{"slug":"labs/lab3/assets/assignment3_esiee","filePath":"labs/lab3/assets/assignment3_esiee.md","title":"assignment3_esiee","links":[],"tags":[],"content":"ESIEE Paris ‚Äî Data Engineering I ‚Äî Assignment 3\n\nAuthor : Badr TAJINI\n\nAcademic year: 2025‚Äì2026\nProgram: Data &amp; Applications - Engineering - (FD)\nCourse: Data Engineering I\n\nLearning goals\n\nAnalyze with SQL and DataFrames.\nImplement two RDD means variants.\nImplement RDD joins (shuffle and hash).\nRecord and explain performance observations.\n\n1. Setup\nDownload data files from the following URL:\nwww.dropbox.com/scl/fi/7012u693u06dgj95mgq2a/retail_dw_20250826.tar.gz\nUnpack somewhere and define the data_path accordingly:\n# Change to path on your local machine.\ndata_path = &quot;/home/sable/devops_base/td2/retail_dw_20250826&quot;\nThe following cell contains setup to measure wall clock time and memory usage. (Don‚Äôt worry about the details, just run the cell)\n!pip install -U numpy pandas pyarrow matplotlib scipy\nimport sys, subprocess\ntry:\n    import psutil  # noqa: F401\nexcept Exception:\n    subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;psutil&quot;])\nprint(&quot;psutil is installed.&quot;)\n \n \nfrom IPython.core.magic import register_cell_magic\nimport time, os, platform\n \n# Try to import optional modules\ntry:\n    import psutil\nexcept Exception:\n    psutil = None\n \ntry:\n    import resource  # not available on Windows\nexcept Exception:\n    resource = None\n \n \ndef _rss_bytes():\n    &quot;&quot;&quot;Resident Set Size in bytes (cross-platform via psutil if available).&quot;&quot;&quot;\n    if psutil is not None:\n        return psutil.Process(os.getpid()).memory_info().rss\n    # Fallback: unknown RSS ‚Üí 0 \n    return 0\n \n \ndef _peak_bytes():\n    &quot;&quot;&quot;\n    Best-effort peak memory in bytes.\n    - Windows: psutil peak working set (peak_wset)\n    - Linux:   resource.ru_maxrss (KB ‚Üí bytes)\n    - macOS:   resource.ru_maxrss (bytes)\n    Fallback to current RSS if unavailable.\n    &quot;&quot;&quot;\n    sysname = platform.system()\n \n    # Windows path: use psutil peak_wset if present\n    if sysname == &quot;Windows&quot; and psutil is not None:\n        mi = psutil.Process(os.getpid()).memory_info()\n        peak = getattr(mi, &quot;peak_wset&quot;, None)  # should be available on Windows\n        if peak is not None:\n            return int(peak)\n        return int(mi.rss)\n \n    # POSIX path: resource may be available\n    if resource is not None:\n        try:\n            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n            if sysname == &quot;Linux&quot;:\n                return int(ru) * 1024\n            else:\n                return int(ru)\n        except Exception:\n            pass\n \n    # Last resort\n    return _rss_bytes()\n \n \n@register_cell_magic\ndef timemem(line, cell):\n    &quot;&quot;&quot;\n    Measure wall time and memory around the execution of this cell.\n \n        timemem\n# codecell_31a - Setup and Data Loading\n \ndata_path = &quot;/home/sable/devops_base/td2/retail_dw_20250826&quot;\n \nevents_df = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_events&quot;))\nproducts_df = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_products&quot;))\nbrands_df = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_brands&quot;))\n \nevents_df.createOrReplaceTempView(&quot;events&quot;)\nproducts_df.createOrReplaceTempView(&quot;products&quot;)\nbrands_df.createOrReplaceTempView(&quot;brands&quot;)\n \nprint(f&quot;Spark version: {spark.version}&quot;)\nprint(f&quot;Events count: {events_df.count()}&quot;)\nprint(f&quot;Products count: {products_df.count()}&quot;)\nprint(f&quot;Brands count: {brands_df.count()}&quot;)\nSpark version: 4.0.1\nEvents count: 42351862\nProducts count: 166794\nBrands count: 3444\n======================================\nWall time: 0.775 s\nRSS Œî: +0.01 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 7617641f2080, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 7617641f0790, raw_cell=&quot;# codecell_31a - Setup and Data Loading\n\ndata_path..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n2. Loading DataFrames\nLet‚Äôs load the DataFrames and print out their schemas:\n# Note that you should have defined data_path above\n \nevents_df   = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_events&quot;))\nproducts_df = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_products&quot;))\nbrands_df   = spark.read.parquet(os.path.join(data_path, &quot;retail_dw_20250826_brands&quot;))\n \nevents_df.printSchema()\nproducts_df.printSchema()\nbrands_df.printSchema()\nroot\n |-- date_key: integer (nullable = true)\n |-- user_key: integer (nullable = true)\n |-- age_key: integer (nullable = true)\n |-- product_key: integer (nullable = true)\n |-- brand_key: integer (nullable = true)\n |-- category_key: integer (nullable = true)\n |-- session_id: string (nullable = true)\n |-- event_time: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- price: double (nullable = true)\n\nroot\n |-- category_code: string (nullable = true)\n |-- brand_code: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- product_desc: string (nullable = true)\n |-- brand_key: integer (nullable = true)\n |-- category_key: integer (nullable = true)\n |-- product_key: integer (nullable = true)\n\nroot\n |-- brand_code: string (nullable = true)\n |-- brand_desc: string (nullable = true)\n |-- brand_key: integer (nullable = true)\n\nHow many rows are in each table?\nprint(f&quot;Number of rows in events   table: {events_df.count()}&quot;)\nprint(f&quot;Number of rows in products table: {products_df.count()}&quot;)\nprint(f&quot;Number of rows in brands   table: {brands_df.count()}&quot;)\nNumber of rows in events   table: 42351862\nNumber of rows in products table: 166794\nNumber of rows in brands   table: 3444\n\nWe can register the DataFrames as tables and issue SQL queries:\nevents_df.createOrReplaceTempView(&quot;events&quot;)\nproducts_df.createOrReplaceTempView(&quot;products&quot;)\nbrands_df.createOrReplaceTempView(&quot;brands&quot;)\n \nspark.sql(&#039;select count(*) from events&#039;).show()\nspark.sql(&#039;select count(*) from products&#039;).show()\nspark.sql(&#039;select count(*) from brands&#039;).show()\n+--------+\n|count(1)|\n+--------+\n|42351862|\n+--------+\n\n+--------+\n|count(1)|\n+--------+\n|  166794|\n+--------+\n\n+--------+\n|count(1)|\n+--------+\n|    3444|\n+--------+\n\nAs a sanity check, the corresponding values should match: counting the rows in the DataFrame vs. issuing an SQL query to count the number of rows.\n3. Data Science\nAnswer Q1 to Q7 below with SQL queries and DataFrame manipulations.\nwrite some code here\n3.1 Q1\nFor session_id 789d3699-028e-4367-b515-b82e2cb5225f, what was the purchase price?\nHint: We only care about purchase events.\nFirst, do it using SQL:\ntimemem\n# codecell_31b (keep this id for tracking purposes)\n \nresults_df = (\n    events_df\n    .filter((F.col(&quot;session_id&quot;) == &quot;789d3699-028e-4367-b515-b82e2cb5225f&quot;) &amp; \n            (F.col(&quot;event_type&quot;) == &quot;purchase&quot;))\n    .select(&quot;price&quot;)\n    .orderBy(F.col(&quot;event_time&quot;).desc())\n    .limit(1)\n)\n \nresults_df.show()\n+------+\n| price|\n+------+\n|100.39|\n+------+\n\n======================================\nWall time: 0.780 s\nRSS Œî: +0.02 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 761766aa9de0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 761766aa93c0, raw_cell=&quot;# codecell_31b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n3.2 Q2\nHow many products are sold by the brand ‚Äúsokolov‚Äù?\nFirst, do it using SQL:\ntimemem\n# codecell_32b (keep this id for tracking purposes)\n \nresults_df = (\n    products_df\n    .filter(F.col(&quot;brand_code&quot;) == &quot;sokolov&quot;)\n    .select(&quot;product_id&quot;)\n    .distinct()\n    .agg(F.count(&quot;product_id&quot;).alias(&quot;num_products&quot;))\n)\n \nresults_df.show()\n+------------+\n|num_products|\n+------------+\n|        1601|\n+------------+\n\n======================================\nWall time: 0.131 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 7617641f2b90, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 7617641f1810, raw_cell=&quot;# codecell_32b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n3.3 Q3\nWhat is the average purchase price of items purchased from the brand ‚Äúfebest‚Äù? (Report answer to two digits after the decimal point, i.e., XX.XX.)\nFirst, do it using SQL:\ntimemem\n# codecell_33b (keep this id for tracking purposes)\n \nresults_df = (\n    events_df\n    .filter((F.col(&quot;event_type&quot;) == &quot;purchase&quot;) &amp; F.col(&quot;price&quot;).isNotNull())\n    .join(products_df, on=&quot;product_key&quot;)\n    .filter(F.col(&quot;brand_code&quot;) == &quot;febest&quot;)\n    .agg(F.round(F.avg(&quot;price&quot;), 2).alias(&quot;avg_price&quot;))\n)\n \nresults_df.show()\n[Stage 52:========================&gt;                                (9 + 8) / 21]\r\n\n+---------+\n|avg_price|\n+---------+\n|    20.39|\n+---------+\n\n======================================\nWall time: 1.321 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 7617641f0cd0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 7617641f0640, raw_cell=&quot;# codecell_33b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n3.4 Q4\nWhat is the average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX.)\nFirst, do it using SQL:\ntimemem\n# codecell_34b (keep this id for tracking purposes)\n \nresults_df = (\n    events_df\n    .groupBy(&quot;user_key&quot;)\n    .count()\n    .agg(F.round(F.avg(&quot;count&quot;), 2).alias(&quot;avg_events_per_user&quot;))\n)\n \nresults_df.show()\n[Stage 61:=============================================&gt;          (17 + 4) / 21]\r\n\n+-------------------+\n|avg_events_per_user|\n+-------------------+\n|              14.02|\n+-------------------+\n\n======================================\nWall time: 1.978 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 7617641f2f80, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 7617641f2a40, raw_cell=&quot;# codecell_34b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n3.5 Q5\nWhat are the top 10 (product_name, brand_code) pairs in terms of revenue? We want the answer rows sorted by revenue in descending order.\nFirst, do it using SQL:\ntimemem\n# codecell_35b (keep this id for tracking purposes)\n \nresults_df = (\n    events_df\n    .filter((F.col(&quot;event_type&quot;) == &quot;purchase&quot;) &amp; F.col(&quot;price&quot;).isNotNull())\n    .join(products_df, on=&quot;product_key&quot;)\n    .groupBy(&quot;product_name&quot;, &quot;brand_code&quot;)\n    .agg(F.round(F.sum(&quot;price&quot;), 2).alias(&quot;total_revenue&quot;))\n    .orderBy(F.desc(&quot;total_revenue&quot;))\n    .limit(10)\n)\n \nresults_df.show(truncate=False)\n[Stage 72:=============================================&gt;          (17 + 4) / 21]\r\n\n+------------+----------+--------------+\n|product_name|brand_code|total_revenue |\n+------------+----------+--------------+\n|smartphone  |apple     |1.6711340803E8|\n|smartphone  |samsung   |9.546627508E7 |\n|smartphone  |xiaomi    |2.254972634E7 |\n|NULL        |NULL      |1.673241267E7 |\n|smartphone  |huawei    |1.363398709E7 |\n|video.tv    |samsung   |1.220999247E7 |\n|smartphone  |NULL      |1.199712625E7 |\n|NULL        |lucente   |9556989.32    |\n|notebook    |acer      |8963128.65    |\n|clocks      |apple     |8622900.64    |\n+------------+----------+--------------+\n\n======================================\nWall time: 1.917 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 7617641f35e0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 7617641f0a90, raw_cell=&quot;# codecell_35b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n3.6 Q6\nTally up counts of events by hour.\nMore precisely, we want a table with hours 0, 1, ‚Ä¶ 23 with the counts of events in that hour.\nFirst, do it using SQL:\ntimemem\n# codecell_36b (keep this id for tracking purposes)\n \nevents_by_hour_df = (\n    events_df\n    .withColumn(&quot;hour&quot;, F.hour(&quot;event_time&quot;))\n    .groupBy(&quot;hour&quot;)\n    .count()\n    .orderBy(&quot;hour&quot;)\n)\n \nevents_by_hour_df.show(24)\n[Stage 78:=============================================&gt;          (17 + 4) / 21]\r\n\n+----+-------+\n|hour|  count|\n+----+-------+\n|   0| 263808|\n|   1| 223635|\n|   2| 353509|\n|   3| 623434|\n|   4|1137209|\n|   5|1605037|\n|   6|1955461|\n|   7|2131930|\n|   8|2269469|\n|   9|2332649|\n|  10|2380185|\n|  11|2335494|\n|  12|2282992|\n|  13|2181477|\n|  14|2171196|\n|  15|2407266|\n|  16|2717710|\n|  17|2988054|\n|  18|3008559|\n|  19|2631424|\n|  20|1999466|\n|  21|1244129|\n|  22| 694728|\n|  23| 413041|\n+----+-------+\n\n======================================\nWall time: 2.212 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 7617641f2770, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 761761403fa0, raw_cell=&quot;# codecell_36b (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\nWhen you run the cell above, events_by_hour_df should be something like:\n+----+-------+\n|hour|  count|\n+----+-------+\n|   0|    ???|\n|   1|    ???|\n  ...\n|  23|    ???|\n+----+-------+\n\nNow plot the above DataFrame using matplotlib.\nHere we want a line graph, with hour on the x axis and count on the y axis.\nHint: use the code below to get started.\ntimemem\n# codecell_37a (keep this id for tracking purposes)\n \n# Write your SQL below\nsql_query = &quot;&quot;&quot;\nSELECT \n    p.brand_code,\n    ROUND(AVG(e.price), 2) as avg_price\nFROM events e\nJOIN products p ON e.product_key = p.product_key\nWHERE e.event_type = &#039;purchase&#039;\n    AND e.price IS NOT NULL\nGROUP BY p.brand_code\nHAVING AVG(e.price) &gt; 10000\nORDER BY avg_price DESC\n&quot;&quot;&quot;\n \nresults = spark.sql(sql_query)\n \nresults.show()\n[Stage 90:==================================================&gt;     (19 + 2) / 21]\r\n\n+----------+---------+\n|brand_code|avg_price|\n+----------+---------+\n|      adam|  58946.0|\n|      kona|  43759.0|\n|  yuandong|  35329.0|\n|   bentley|  23164.0|\n|      otex| 18633.14|\n|    suunto| 10732.82|\n|     stark| 10400.25|\n+----------+---------+\n\n======================================\nWall time: 2.709 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n                                                                                \r\n\n\n\n\n&lt;ExecutionResult object at 761766aab430, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 761766aa9300, raw_cell=&quot;# codecell_37a (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\nNext, do it with DataFrames:\ntimemem\n# codecell_37c (keep this id for tracking purposes)\n \nimport matplotlib.pyplot as plt\n \navg_price_by_brand_pdf = avg_price_by_brand_df.toPandas()\n \nplt.figure(figsize=(12, 6))\nplt.bar(avg_price_by_brand_pdf[&#039;brand_code&#039;], avg_price_by_brand_pdf[&#039;avg_price&#039;], color=&#039;steelblue&#039;)\nplt.xlabel(&#039;Brand Code&#039;, fontsize=12)\nplt.ylabel(&#039;Average Price&#039;, fontsize=12)\nplt.title(&#039;Average Purchase Price by Brand (&gt; 10K)&#039;, fontsize=14)\nplt.xticks(rotation=45, ha=&#039;right&#039;)\nplt.grid(True, alpha=0.3, axis=&#039;y&#039;)\nplt.tight_layout()\n \nplt.show()\n\n======================================\nWall time: 1.887 s\nRSS Œî: +0.05 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 7617642485b0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 76176424ae30, raw_cell=&quot;# codecell_37c (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\n4. Load RDDs\nThe remaining exercises focus on RDD manipulations.\nLet‚Äôs start by loading the RDDs.\n# Get RDDs directly from DataFrames (with required repartitions)\n# type: RDD[Row]\nevents_rdd   = events_df.rdd.repartition(1000)\nproducts_rdd = products_df.rdd.repartition(100)\nbrands_rdd   = brands_df.rdd.repartition(100)\nYou‚Äôll need Row, so let‚Äôs make sure we‚Äôve imported it.\nfrom pyspark.sql import Row\n5. Implementations of Computing Averages\nIn this next exercise, we‚Äôre going to implement ‚Äúcomputing the mean‚Äù (version 1) and (version 3) in Spark as described in the second lecture Batch Processing I (please use ctrl+f to reach the slide with the title : ‚ÄúComputing the Mean: Version 1‚Äù or ‚ÄúComputing the Mean: Version 3‚Äù.\nTo make the problem more tractable (i.e., to reduce the running times), let‚Äôs first do a bit of filtering of the events table.\nWe‚Äôll do this using DataFrames, and then generate an RDD:\nYou can confirm that we‚Äôre working with a smaller dataset.\nCompute the average purchase price by brand. We want the results sorted by the average purchase price from the largest to smallest value. As before, round to two digits after the decimal point. This is similar to Q7 above, except without the ‚Äúmore than 10K‚Äù condition.\nImplement using the naive ‚Äúversion 1‚Äù algorithm, as described in the lectures:\n\nYou must start with filtered_events_rdd.\nYou must use groupByKey().\nPer ‚Äúversion 1‚Äù, your implementation must shuffle all values from the ‚Äúmappers‚Äù to the ‚Äúreducers‚Äù.\n\nwrite some code here\ntimemem\n# codecell_5x1 (keep this id for tracking purposes)\n \naverage_revenue_per_brand_v1 = (\n    filtered_events_rdd\n    .map(lambda row: (row[&#039;brand_code&#039;], row[&#039;price&#039;]))\n    .groupByKey()\n    .mapValues(lambda prices: sum(prices) / len(list(prices)))\n    .map(lambda x: (x[0], round(x[1], 2)))\n    .sortBy(lambda x: x[1], ascending=False)\n)\n \naverage_revenue_per_brand_v1.take(10)\n[(&#039;adam&#039;, 58946.0),\n (&#039;kona&#039;, 43759.0),\n (&#039;yuandong&#039;, 35329.0),\n (&#039;bentley&#039;, 23164.0),\n (&#039;otex&#039;, 18633.13),\n (&#039;suunto&#039;, 10732.82),\n (&#039;stark&#039;, 10400.25),\n (&#039;zenmart&#039;, 9447.0),\n (&#039;baltekstil&#039;, 8504.19),\n (&#039;bugati&#039;, 8288.42)]\n\n\n\n======================================\nWall time: 12.619 s\nRSS Œî: +0.02 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 7617614366b0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 761761436500, raw_cell=&quot;# codecell_5x1 (keep this id for tracking purposes..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=[(&#039;adam&#039;, 58946.0), (&#039;kona&#039;, 43759.0), (&#039;yuandong&#039;, 35329.0), (&#039;bentley&#039;, 23164.0), (&#039;otex&#039;, 18633.13), (&#039;suunto&#039;, 10732.82), (&#039;stark&#039;, 10400.25), (&#039;zenmart&#039;, 9447.0), (&#039;baltekstil&#039;, 8504.19), (&#039;bugati&#039;, 8288.42)]&gt;\n\nCompute the average purchase price by brand. We want the results sorted by the average purchase price from the largest to smallest value. As before, round to two digits after the decimal point. This is similar to Q7 above, except without the ‚Äúmore than 10K‚Äù condition.\nImplement using the improved ‚Äúversion 3‚Äù algorithm, as described in the lectures:\n\nYou must start with filtered_events_rdd.\nYou must use reduceByKey().\nPer ‚Äúversion 3‚Äù, your implementation must emit (sum, count) pairs and take advantage opportunities to perform aggregations.\n\nwrite some code here\ntimemem\n \nshuffle_join_rdd = shuffle_join(brands_rdd, products_rdd, &quot;brand_key&quot;, &quot;brand_key&quot;)\nshuffle_join_rdd.count()\n115584\n\n\n\n======================================\nWall time: 36.276 s\nRSS Œî: +0.00 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 76175d4cfdc0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 76175d4ced70, raw_cell=&quot;\nshuffle_join_rdd = shuffle_join(brands_rdd, produ..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=115584&gt;\n\nAdd in the WHERE clause:\nshuffle_join_results_rdd = shuffle_join_rdd.filter(lambda row: row[&quot;brand_key&quot;] == 423)\nshuffle_join_results_rdd.count()\n2\n\nIf you look at the results, they‚Äôre a bit difficult to read‚Ä¶ why don‚Äôt we just use Spark DataFrames for prettification?\ndf = spark.createDataFrame(shuffle_join_results_rdd.collect())\ndf.show()\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n|brand_code|          brand_desc|brand_key|category_code|product_id|product_name|        product_desc|category_key|product_key|\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n| blaupunkt|&quot;Blaupunkt is a G...|      423|  electronics|   1802099|    video.tv|The video.tv is a...|           8|       4813|\n| blaupunkt|&quot;Blaupunkt is a G...|      423|  electronics|   1802107|    video.tv|The video.tv is a...|           8|       4821|\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n\ntimemem\n \nreplicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, &quot;brand_key&quot;, &quot;brand_key&quot;)\nreplicated_hash_join_rdd.count()\n115584\n\n\n\n======================================\nWall time: 5.563 s\nRSS Œî: +1.08 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 76176426eda0, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 76176426d030, raw_cell=&quot;\nreplicated_hash_join_rdd = replicated_hash_join(b..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=115584&gt;\n\nAdd in the WHERE clause:\nreplicated_hash_join_results_rdd = replicated_hash_join_rdd.filter(lambda row: row[&quot;brand_key&quot;] == 423)\nreplicated_hash_join_results_rdd.count()\n2\n\nIf you look at the results, they‚Äôre a bit difficult to read‚Ä¶ why don‚Äôt we just use Spark DataFrames for prettification?\ndf = spark.createDataFrame(replicated_hash_join_results_rdd.collect())\ndf.show()\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n|brand_code|          brand_desc|brand_key|category_code|product_id|product_name|        product_desc|category_key|product_key|\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n| blaupunkt|&quot;Blaupunkt is a G...|      423|  electronics|   1802099|    video.tv|The video.tv is a...|           8|       4813|\n| blaupunkt|&quot;Blaupunkt is a G...|      423|  electronics|   1802107|    video.tv|The video.tv is a...|           8|       4821|\n+----------+--------------------+---------+-------------+----------+------------+--------------------+------------+-----------+\n\nVerify output against the SQL query.\n7. Join Performance\nNow that we have two different implementations of joins, let‚Äôs compare them, on the same exact query.\nThe first two are repeated from above.\nLet‚Äôs call this J1 below.\n(Run the cell, it should just work. If it doesn‚Äôt you‚Äôll need to fix the implementation above.)\ntimemem\n \nreplicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, &quot;brand_key&quot;, &quot;brand_key&quot;).filter(lambda row: row[&quot;brand_key&quot;] == 423)\nreplicated_hash_join_rdd.count()\n2\n\n\n\n======================================\nWall time: 5.639 s\nRSS Œî: +0.01 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 76177f7b1a50, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 76177f7b19c0, raw_cell=&quot;\nreplicated_hash_join_rdd = replicated_hash_join(b..&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=2&gt;\n\nLet‚Äôs call this J3 below.\n(Run the cell, it should just work. If it doesn‚Äôt you‚Äôll need to fix the implementation above.)\ntimemem\nspark.stop()\n======================================\nWall time: 0.938 s\nRSS Œî: -0.01 MB\nPeak memory Œî: +0.00 MB (OS-dependent)\n======================================\n\n\n\n\n\n&lt;ExecutionResult object at 761762795870, execution_count=None error_before_exec=None error_in_exec=None info=&lt;ExecutionInfo object at 761762795960, raw_cell=&quot;spark.stop()\n&quot; store_history=False silent=False shell_futures=True cell_id=None&gt; result=None&gt;\n\nPerformance notes\n\nSet and justify spark.sql.shuffle.partitions for local vs. cluster runs.\nPrefer DataFrame built-ins over Python UDFs; push logic to Catalyst when possible.\nUse AQE (adaptive query execution) to mitigate skew; consider salting for extreme keys.\nCache only when reuse exists; unpersist when no longer needed.\nUse broadcast join only when the small side fits in memory; verify with explain.\nCapture df.explain(mode=&#039;formatted&#039;) for at least one analysis query and one join.\nA3 note: Python RDDs cross the Python/JVM boundary; slower runtimes are expected for the RDD parts.\n\nSelf-check (toy data)\n# Red√©marrer Spark si n√©cessaire\ntry:\n    _ = spark.sparkContext.defaultParallelism\n    print(&quot;Spark is already running&quot;)\nexcept (AttributeError, Exception):\n    print(&quot;Restarting Spark...&quot;)\n    spark = SparkSession.builder \\\n        .appName(&quot;A3-SelfCheck&quot;) \\\n        .master(&quot;local[*]&quot;) \\\n        .config(&quot;spark.driver.memory&quot;, &quot;4g&quot;) \\\n        .getOrCreate()\n    spark.sparkContext.setLogLevel(&quot;ERROR&quot;)\n    print(&quot;Spark restarted successfully&quot;)\n \n# ============================================\n# Test Mean Functions\n# ============================================\nprint(&quot;\\n&quot; + &quot;=&quot;*50)\nprint(&quot;Testing Mean Functions (Version 1 vs Version 3)&quot;)\nprint(&quot;=&quot;*50)\n \ntest_data = [\n    Row(brand_code=&#039;A&#039;, price=100.0),\n    Row(brand_code=&#039;A&#039;, price=200.0),\n    Row(brand_code=&#039;B&#039;, price=300.0),\n    Row(brand_code=&#039;B&#039;, price=400.0),\n]\ntest_rdd = spark.sparkContext.parallelize(test_data)\n \n# Version 1 (groupByKey)\nprint(&quot;\\n--- Version 1 (groupByKey) ---&quot;)\nmean_v1 = (\n    test_rdd\n    .map(lambda row: (row[&#039;brand_code&#039;], row[&#039;price&#039;]))\n    .groupByKey()\n    .mapValues(lambda prices: round(sum(prices) / len(list(prices)), 2))\n    .sortByKey()\n    .collect()\n)\nprint(f&quot;V1 Results: {mean_v1}&quot;)\nprint(&quot;Expected: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]&quot;)\n \n# Version 3 (reduceByKey)\nprint(&quot;\\n--- Version 3 (reduceByKey) ---&quot;)\nmean_v3 = (\n    test_rdd\n    .map(lambda row: (row[&#039;brand_code&#039;], (row[&#039;price&#039;], 1)))\n    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n    .mapValues(lambda x: round(x[0] / x[1], 2))\n    .sortByKey()\n    .collect()\n)\nprint(f&quot;V3 Results: {mean_v3}&quot;)\nprint(&quot;Expected: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]&quot;)\n \n# Verify correctness\nassert mean_v1 == mean_v3 == [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)], &quot;Mean calculation mismatch&quot;\nprint(&quot;\\n[PASS] Mean functions produce identical correct results&quot;)\n \n# ============================================\n# Test Join Functions\n# ============================================\nprint(&quot;\\n&quot; + &quot;=&quot;*50)\nprint(&quot;Testing Join Functions (Shuffle vs Hash Join)&quot;)\nprint(&quot;=&quot;*50)\n \nleft = spark.sparkContext.parallelize([\n    Row(id=1, name=&#039;A&#039;),\n    Row(id=2, name=&#039;B&#039;),\n    Row(id=3, name=&#039;C&#039;)\n])\n \nright = spark.sparkContext.parallelize([\n    Row(id=1, value=10),\n    Row(id=2, value=20)\n])\n \n# Shuffle Join\nprint(&quot;\\n--- Shuffle Join Test ---&quot;)\nshuffle_result = shuffle_join(left, right, &#039;id&#039;, &#039;id&#039;).sortBy(lambda r: r.id).collect()\nprint(f&quot;Shuffle Join: {len(shuffle_result)} rows&quot;)\nfor row in shuffle_result:\n    print(f&quot;  {row}&quot;)\n \n# Hash Join\nprint(&quot;\\n--- Replicated Hash Join Test ---&quot;)\nhash_result = replicated_hash_join(left, right, &#039;id&#039;, &#039;id&#039;).sortBy(lambda r: r.id).collect()\nprint(f&quot;Hash Join: {len(hash_result)} rows&quot;)\nfor row in hash_result:\n    print(f&quot;  {row}&quot;)\n \n# Verify correctness\nassert len(shuffle_result) == len(hash_result) == 2, &quot;Join count mismatch&quot;\nprint(&quot;\\n[PASS] Both join implementations produce 2 rows as expected&quot;)\n \n# Verify content\nexpected_ids = {1, 2}\nshuffle_ids = {r.id for r in shuffle_result}\nhash_ids = {r.id for r in hash_result}\nassert shuffle_ids == hash_ids == expected_ids, &quot;Join content mismatch&quot;\nprint(&quot;[PASS] Both joins match on ids {1, 2}&quot;)\n \n# ============================================\n# Summary\n# ============================================\nprint(&quot;\\n&quot; + &quot;=&quot;*50)\nprint(&quot;ALL SELF-CHECKS PASSED&quot;)\nprint(&quot;=&quot;*50)\nprint(&quot;* Mean V1 (groupByKey) - PASSED&quot;)\nprint(&quot;* Mean V3 (reduceByKey) - PASSED&quot;)\nprint(&quot;* Shuffle Join - PASSED&quot;)\nprint(&quot;* Replicated Hash Join - PASSED&quot;)\n \nRestarting Spark...\nSpark restarted successfully\n\n==================================================\nTesting Mean Functions (Version 1 vs Version 3)\n==================================================\n\n--- Version 1 (groupByKey) ---\n\n\n                                                                                \r\n\nV1 Results: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]\nExpected: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]\n\n--- Version 3 (reduceByKey) ---\nV3 Results: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]\nExpected: [(&#039;A&#039;, 150.0), (&#039;B&#039;, 350.0)]\n\n[PASS] Mean functions produce identical correct results\n\n==================================================\nTesting Join Functions (Shuffle vs Hash Join)\n==================================================\n\n--- Shuffle Join Test ---\n\n\n                                                                                \r\n\nShuffle Join: 2 rows\n  Row(id=1, name=&#039;A&#039;, value=10)\n  Row(id=2, name=&#039;B&#039;, value=20)\n\n--- Replicated Hash Join Test ---\nHash Join: 2 rows\n  Row(id=1, name=&#039;A&#039;, value=10)\n  Row(id=2, name=&#039;B&#039;, value=20)\n\n[PASS] Both join implementations produce 2 rows as expected\n[PASS] Both joins match on ids {1, 2}\n\n==================================================\nALL SELF-CHECKS PASSED\n==================================================\n* Mean V1 (groupByKey) - PASSED\n* Mean V3 (reduceByKey) - PASSED\n* Shuffle Join - PASSED\n* Replicated Hash Join - PASSED\n\nReproducibility checklist\n\nRecord Python, Java, and Spark versions.\nFix timezone to UTC and log run timestamp.\nPin random seeds where randomness is used.\nSave configs: spark.sql.shuffle.partitions, AQE flags, broadcast thresholds if changed.\nProvide exact run commands and input/output paths.\nExport a minimal environment file (environment.yml or requirements.txt).\nKeep data paths relative to project root; avoid user-specific absolute paths.\nInclude small sample outputs for verification.\n\n## Liste de v√©rification compl√®te pour la reproductibilit√©\n \nimport sys, os, platform, subprocess, random\nimport numpy as np\nfrom datetime import datetime, timezone\n \nprint(&quot;REPRODUCTIBILIT√â &quot;)\n \n \n# 1. Versions des logiciels\nprint(&quot;\\n[1] VERSIONS DES LOGICIELS&quot;)\nprint(&quot;-&quot; * 70)\nprint(f&quot;Python: {sys.version.split()[0]}&quot;)\nprint(f&quot;Spark: {spark.version}&quot;)\ntry:\n    java_ver = subprocess.check_output([&#039;java&#039;, &#039;-version&#039;], stderr=subprocess.STDOUT).decode().split(&#039;\\n&#039;)[0]\n    print(f&quot;Java: {java_ver}&quot;)\nexcept:\n    print(&quot;Java: Version non d√©tect√©e&quot;)\nprint(f&quot;OS: {platform.system()} {platform.release()} ({platform.machine()})&quot;)\n \n# 2. Horodatage UTC\nprint(&quot;\\n[2] HORODATAGE D&#039;EX√âCUTION&quot;)\nprint(&quot;-&quot; * 70)\nos.environ[&#039;TZ&#039;] = &#039;UTC&#039;\nexecution_time = datetime.now(timezone.utc).isoformat()\nprint(f&quot;D√©marrage: {execution_time} UTC&quot;)\n \n# 3. Graine al√©atoire\nprint(&quot;\\n[3] GRAINE AL√âATOIRE&quot;)\nprint(&quot;-&quot; * 70)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nprint(f&quot;Graine fix√©e √†: {SEED}&quot;)\n \n# 4. Configuration Spark\nprint(&quot;\\n[4] CONFIGURATION SPARK&quot;)\nprint(&quot;-&quot; * 70)\nconfigs = {\n    &quot;spark.sql.shuffle.partitions&quot;: spark.conf.get(&quot;spark.sql.shuffle.partitions&quot;),\n    &quot;spark.sql.adaptive.enabled&quot;: spark.conf.get(&quot;spark.sql.adaptive.enabled&quot;),\n    &quot;spark.driver.memory&quot;: spark.conf.get(&quot;spark.driver.memory&quot;),\n}\nfor key, val in configs.items():\n    print(f&quot;{key}: {val}&quot;)\n \n# 5. Chemins\nprint(&quot;\\n[5] CHEMINS DE DONN√âES&quot;)\nprint(&quot;-&quot; * 70)\nprint(f&quot;Chemin des donn√©es: {data_path}&quot;)\nprint(f&quot;Existe: {os.path.exists(data_path)}&quot;)\n \n# 6. Packages critiques\nprint(&quot;\\n[6] PACKAGES ESSENTIELS&quot;)\nprint(&quot;-&quot; * 70)\nfor pkg in [&quot;pyspark&quot;, &quot;pandas&quot;, &quot;numpy&quot;, &quot;matplotlib&quot;]:\n    try:\n        ver = subprocess.check_output([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;show&quot;, pkg]).decode()\n        ver_line = [l for l in ver.split(&#039;\\n&#039;) if l.startswith(&#039;Version:&#039;)][0]\n        print(f&quot;{pkg}: {ver_line.split(&#039;:&#039;)[1].strip()}&quot;)\n    except:\n        print(f&quot;{pkg}: Non install√©&quot;)\n \nprint(&quot;\\n&quot; + &quot;=&quot;*70)\nprint(&quot;Toutes les informations de reproductibilit√© ont √©t√© enregistr√©es&quot;)\nprint(&quot;=&quot;*70)\nREPRODUCTIBILIT√â \n\n[1] VERSIONS DES LOGICIELS\n----------------------------------------------------------------------\nPython: 3.10.18\nSpark: 4.0.1\nJava: openjdk version &quot;17.0.15-internal&quot; 2025-04-15\nOS: Linux 6.14.0-37-generic (x86_64)\n\n[2] HORODATAGE D&#039;EX√âCUTION\n----------------------------------------------------------------------\nD√©marrage: 2025-12-19T22:38:38.110215+00:00 UTC\n\n[3] GRAINE AL√âATOIRE\n----------------------------------------------------------------------\nGraine fix√©e √†: 42\n\n[4] CONFIGURATION SPARK\n----------------------------------------------------------------------\nspark.sql.shuffle.partitions: 400\nspark.sql.adaptive.enabled: true\nspark.driver.memory: 4g\n\n[5] CHEMINS DE DONN√âES\n----------------------------------------------------------------------\nChemin des donn√©es: /home/sable/devops_base/td2/retail_dw_20250826\nExiste: True\n\n[6] PACKAGES ESSENTIELS\n----------------------------------------------------------------------\npyspark: 4.0.1\npandas: 2.3.3\nnumpy: 2.2.6\nmatplotlib: 3.10.8\n\n======================================================================\nToutes les informations de reproductibilit√© ont √©t√© enregistr√©es\n======================================================================\n"},"labs/lab3/index":{"slug":"labs/lab3/index","filePath":"labs/lab3/index.md","title":"Lab 3 ‚Äì Data Engineering","links":["labs/lab3/assets/plan_broadcast.txt","labs/lab3/assets/plan_column.txt","labs/lab3/assets/plan_row.txt"],"tags":[],"content":"Lab 3 ‚Äì Data Engineering\nNotebook : assignment3_esiee\n{% include ‚Äúlabs/lab3/assets/assignment3_esiee.md‚Äù %}\n\nNotebook : DE1_Lab3_Notebook_EN\n{% include ‚Äúlabs/lab3/assets/DE1_Lab3_Notebook_EN.md‚Äù %}\nüìä Proof / Outputs\nText files\n\nplan_broadcast.txt\nplan_column.txt\nplan_row.txt\n"},"project/assets/DE1_Project_Notebook_EN":{"slug":"project/assets/DE1_Project_Notebook_EN","filePath":"project/assets/DE1_Project_Notebook_EN.md","title":"DE1_Project_Notebook_EN","links":[],"tags":[],"content":"DE1 ‚Äî Final Project Notebook\n\nProfessor : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n\n\n\nStudents : DIALLO Samba &amp; DIOP Mouhamed\n\n\nThis is the primary executable artifact. Fill config, run baseline, then optimized pipeline, and record evidence.\n0. Load config\nimport yaml, pathlib, datetime\nfrom pyspark.sql import SparkSession, functions as F, types as T\n \nwith open(&quot;de1_project_config.yml&quot;) as f:\n    CFG = yaml.safe_load(f)\n \nspark = (SparkSession.builder\n    .appName(&quot;de1-lakehouse-project&quot;)\n    .config(&quot;spark.sql.adaptive.enabled&quot;, &quot;true&quot;)\n    .config(&quot;spark.sql.adaptive.coalescePartitions.enabled&quot;, &quot;true&quot;)\n    .config(&quot;spark.driver.memory&quot;, &quot;4g&quot;)\n    .config(&quot;spark.executor.memory&quot;, &quot;4g&quot;)\n    .config(&quot;spark.memory.fraction&quot;, &quot;0.6&quot;)\n    .config(&quot;spark.memory.storageFraction&quot;, &quot;0.3&quot;)\n    .getOrCreate())\n \nprint(f&quot;Spark version: {spark.version}&quot;)\nprint(f&quot;Project: {CFG[&#039;project&#039;][&#039;name&#039;]}&quot;)\nCFG\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/12/20 20:51:51 WARN Utils: Your hostname, sable-ThinkPad-X1-Yoga-3rd, resolves to a loopback address: 127.0.1.1; using 10.192.33.105 instead (on interface wlp2s0)\n25/12/20 20:51:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/12/20 20:51:51 WARN Utils: Your hostname, sable-ThinkPad-X1-Yoga-3rd, resolves to a loopback address: 127.0.1.1; using 10.192.33.105 instead (on interface wlp2s0)\n25/12/20 20:51:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nUsing Spark&#039;s default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/20 20:51:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/12/20 20:51:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\nSpark version: 4.0.1\nProject: DE1 Local Lakehouse Project\n\n\n\n\n\n{&#039;project&#039;: {&#039;name&#039;: &#039;DE1 Local Lakehouse Project&#039;,\n  &#039;dataset&#039;: &#039;Wikipedia Clickstream (Nov 2024)&#039;,\n  &#039;dataset_size&#039;: &#039;10M rows / 450MB&#039;,\n  &#039;team&#039;: &#039;Badr TAJINI&#039;},\n &#039;paths&#039;: {&#039;raw_csv_glob&#039;: &#039;data/clickstream/clickstream-10M.tsv&#039;,\n  &#039;bronze&#039;: &#039;outputs/project/bronze&#039;,\n  &#039;silver&#039;: &#039;outputs/project/silver&#039;,\n  &#039;gold&#039;: &#039;outputs/project/gold&#039;,\n  &#039;proof&#039;: &#039;proof&#039;,\n  &#039;metrics_log&#039;: &#039;project_metrics_log.csv&#039;},\n &#039;slo&#039;: {&#039;freshness_hours&#039;: 2,\n  &#039;q1_latency_p95_seconds&#039;: 4,\n  &#039;storage_ratio_max&#039;: 0.6},\n &#039;hardware&#039;: {&#039;cpu&#039;: &#039;Intel Core i7&#039;,\n  &#039;ram_gb&#039;: 16,\n  &#039;disk_type&#039;: &#039;SSD&#039;,\n  &#039;spark_version&#039;: &#039;4.0.0&#039;},\n &#039;layout&#039;: {&#039;partition_by&#039;: [],\n  &#039;sort_by&#039;: [],\n  &#039;target_file_size_mb&#039;: 128,\n  &#039;max_files_per_partition&#039;: 10},\n &#039;queries&#039;: {&#039;q1&#039;: {&#039;description&#039;: &#039;Top 20 most visited pages&#039;,\n   &#039;sql&#039;: &#039;SELECT curr as page, \\n       SUM(n) as total_clicks\\nFROM silver\\nGROUP BY curr\\nORDER BY total_clicks DESC\\nLIMIT 20\\n&#039;},\n  &#039;q2&#039;: {&#039;description&#039;: &#039;Top 20 referrer pages&#039;,\n   &#039;sql&#039;: &quot;SELECT prev as referrer, \\n       SUM(n) as total_clicks\\nFROM silver\\nWHERE prev != &#039;other-empty&#039;\\nGROUP BY prev\\nORDER BY total_clicks DESC\\nLIMIT 20\\n&quot;},\n  &#039;q3&#039;: {&#039;description&#039;: &#039;Click patterns by type (external/internal/link)&#039;,\n   &#039;sql&#039;: &#039;SELECT type,\\n       COUNT(DISTINCT curr) as unique_pages,\\n       SUM(n) as total_clicks,\\n       AVG(n) as avg_clicks_per_pair\\nFROM silver\\nGROUP BY type\\nORDER BY total_clicks DESC\\n&#039;}},\n &#039;silver_schema&#039;: {&#039;columns&#039;: [{&#039;name&#039;: &#039;prev&#039;,\n    &#039;type&#039;: &#039;StringType&#039;,\n    &#039;nullable&#039;: False},\n   {&#039;name&#039;: &#039;curr&#039;, &#039;type&#039;: &#039;StringType&#039;, &#039;nullable&#039;: False},\n   {&#039;name&#039;: &#039;type&#039;, &#039;type&#039;: &#039;StringType&#039;, &#039;nullable&#039;: False},\n   {&#039;name&#039;: &#039;n&#039;, &#039;type&#039;: &#039;IntegerType&#039;, &#039;nullable&#039;: False}]},\n &#039;data_quality&#039;: {&#039;rules&#039;: [{&#039;name&#039;: &#039;Non-null clicks&#039;,\n    &#039;check&#039;: &#039;n IS NOT NULL&#039;,\n    &#039;severity&#039;: &#039;error&#039;},\n   {&#039;name&#039;: &#039;Positive clicks&#039;, &#039;check&#039;: &#039;n &gt;= 0&#039;, &#039;severity&#039;: &#039;error&#039;},\n   {&#039;name&#039;: &#039;Valid page names&#039;,\n    &#039;check&#039;: &#039;LENGTH(curr) &gt; 0&#039;,\n    &#039;severity&#039;: &#039;error&#039;},\n   {&#039;name&#039;: &#039;Valid type&#039;,\n    &#039;check&#039;: &quot;type IN (&#039;link&#039;, &#039;external&#039;, &#039;other&#039;)&quot;,\n    &#039;severity&#039;: &#039;warning&#039;}]},\n &#039;optimization&#039;: {&#039;baseline&#039;: [&#039;No partitioning&#039;,\n   &#039;No sorting&#039;,\n   &#039;Default file sizes&#039;,\n   &#039;Full table scans&#039;],\n  &#039;optimized&#039;: [&#039;Repartition by computed hash&#039;,\n   &#039;Sort by clicks (n) descending&#039;,\n   &#039;Target 128MB file size&#039;,\n   &#039;Enable AQE&#039;,\n   &#039;Coalesce small files&#039;]}}\n\n1. Bronze ‚Äî landing raw data\nraw_glob = CFG[&quot;paths&quot;][&quot;raw_csv_glob&quot;]\nbronze = CFG[&quot;paths&quot;][&quot;bronze&quot;]\nproof = CFG[&quot;paths&quot;][&quot;proof&quot;]\n \ndf_raw = (spark.read\n    .option(&quot;header&quot;, &quot;false&quot;)\n    .option(&quot;inferSchema&quot;, &quot;false&quot;)\n    .option(&quot;delimiter&quot;, &quot;\\t&quot;)\n    .csv(raw_glob)\n    .toDF(&quot;prev&quot;, &quot;curr&quot;, &quot;type&quot;, &quot;n&quot;))\n \nrow_count = df_raw.count()\ndf_raw.write.mode(&quot;overwrite&quot;).csv(bronze)\nprint(f&quot;Bronze written: {bronze}, rows: {row_count:,}&quot;)\n[Stage 4:============================================&gt;              (6 + 2) / 8]\r\n\nBronze written: outputs/project/bronze, rows: 10,000,000\n\n\n                                                                                \n\n2. Silver ‚Äî cleaning and typing\nsilver = CFG[&quot;paths&quot;][&quot;silver&quot;]\n \ndf_silver = (df_raw\n    .withColumn(&quot;n&quot;, F.col(&quot;n&quot;).cast(&quot;integer&quot;))\n    .filter(F.col(&quot;n&quot;).isNotNull())\n    .filter(F.col(&quot;n&quot;) &gt;= 0)\n    .filter(F.length(F.col(&quot;curr&quot;)) &gt; 0)\n    .dropDuplicates())\n \nsilver_count = df_silver.count()\ndf_silver.write.mode(&quot;overwrite&quot;).parquet(silver)\nprint(f&quot;Silver written: {silver}, rows: {silver_count:,}&quot;)\n[Stage 13:===================================================&gt;      (8 + 1) / 9]\r\n\nSilver written: outputs/project/silver, rows: 10,000,000\n\n\n                                                                                \n\n3. Gold ‚Äî analytics tables\ngold = CFG[&quot;paths&quot;][&quot;gold&quot;]\nqueries = CFG[&quot;queries&quot;]\n \npathlib.Path(gold).mkdir(parents=True, exist_ok=True)\ndf_silver.createOrReplaceTempView(&quot;silver&quot;)\n \ndf_q1 = spark.sql(queries[&quot;q1&quot;][&quot;sql&quot;])\nq1_count = df_q1.count()\ndf_q1.write.mode(&quot;overwrite&quot;).parquet(f&quot;{gold}/q1_daily_aggregation&quot;)\nprint(f&quot;Q1 written, rows: {q1_count:,}&quot;)\n \ndf_q2 = spark.sql(queries[&quot;q2&quot;][&quot;sql&quot;])\nq2_count = df_q2.count()\ndf_q2.write.mode(&quot;overwrite&quot;).parquet(f&quot;{gold}/q2_top_referrers&quot;)\nprint(f&quot;Q2 written, rows: {q2_count:,}&quot;)\n \ndf_q3 = spark.sql(queries[&quot;q3&quot;][&quot;sql&quot;])\nq3_count = df_q3.count()\ndf_q3.write.mode(&quot;overwrite&quot;).parquet(f&quot;{gold}/q3_filtered_analysis&quot;)\nprint(f&quot;Q3 written, rows: {q3_count:,}&quot;)\n \nprint(f&quot;Gold written: {gold}&quot;)\nQ1 written, rows: 20\n\n\n                                                                                \r\n\nQ2 written, rows: 20\n\n\n                                                                                \r\n\nQ3 written, rows: 3\nGold written: outputs/project/gold\n\n4. Baseline plans and metrics\npathlib.Path(proof).mkdir(parents=True, exist_ok=True)\n \ndf_q1_baseline = spark.sql(queries[&quot;q1&quot;][&quot;sql&quot;])\nplan_q1 = df_q1_baseline._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/baseline_q1_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(plan_q1)\n \ndf_q2_baseline = spark.sql(queries[&quot;q2&quot;][&quot;sql&quot;])\nplan_q2 = df_q2_baseline._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/baseline_q2_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(plan_q2)\n \ndf_q3_baseline = spark.sql(queries[&quot;q3&quot;][&quot;sql&quot;])\nplan_q3 = df_q3_baseline._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/baseline_q3_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(plan_q3)\n \nprint(&quot;Saved baseline plans. Record Spark UI metrics now.&quot;)\nSaved baseline plans. Record Spark UI metrics now.\n\n5. Optimization ‚Äî layout and joins\nlayout = CFG[&quot;layout&quot;]\ntarget_size_mb = layout[&quot;target_file_size_mb&quot;]\n \ndf_silver_reloaded = spark.read.parquet(silver)\ntotal_bytes = df_silver_reloaded.count() * 100\nnum_partitions = max(4, int(total_bytes / (target_size_mb * 1024 * 1024)))\n \ndf_silver_opt = (df_silver_reloaded\n    .repartition(num_partitions)\n    .sortWithinPartitions(F.desc(&quot;n&quot;)))\n \nsilver_opt = f&quot;{silver}_optimized&quot;\ndf_silver_opt.write.mode(&quot;overwrite&quot;).parquet(silver_opt)\nprint(f&quot;Optimized silver written: {silver_opt}&quot;)\n \ndf_silver_opt.createOrReplaceTempView(&quot;silver&quot;)\n \ndf_q1_opt = spark.sql(queries[&quot;q1&quot;][&quot;sql&quot;])\nplan_q1_opt = df_q1_opt._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/optimized_q1_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(f&quot;Optimizations: {num_partitions} partitions, sorted by n desc\\n&quot;)\n    f.write(plan_q1_opt)\n \ndf_q2_opt = spark.sql(queries[&quot;q2&quot;][&quot;sql&quot;])\nplan_q2_opt = df_q2_opt._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/optimized_q2_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(f&quot;Optimizations: {num_partitions} partitions, sorted by n desc\\n&quot;)\n    f.write(plan_q2_opt)\n \ndf_q3_opt = spark.sql(queries[&quot;q3&quot;][&quot;sql&quot;])\nplan_q3_opt = df_q3_opt._jdf.queryExecution().executedPlan().toString()\nwith open(f&quot;{proof}/optimized_q3_plan.txt&quot;, &quot;w&quot;) as f:\n    f.write(str(datetime.datetime.now()) + &quot;\\n&quot;)\n    f.write(f&quot;Optimizations: {num_partitions} partitions, sorted by n desc\\n&quot;)\n    f.write(plan_q3_opt)\n \nprint(&quot;Saved optimized plans. Record Spark UI metrics now.&quot;)\n[Stage 77:&gt;                                                         (0 + 7) / 7]\r\n\nOptimized silver written: outputs/project/silver_optimized\nSaved optimized plans. Record Spark UI metrics now.\n\n\n                                                                                \n\n6. Cleanup\nspark.stop()\nprint(&quot;Spark session stopped.&quot;)\n \nSpark session stopped.\n"},"project/assets/DE1_Project_Report":{"slug":"project/assets/DE1_Project_Report","filePath":"project/assets/DE1_Project_Report.md","title":"DE1_Project_Report","links":[],"tags":[],"content":"Rapport de Projet Final DE1\nAuteurs : DIALLO Samba &amp; DIOP Mouhamed\nCours : Data Engineering I - ESIEE 2025-2026\nDate : 2025\n\n1. Cas d‚Äôusage et Jeu de Donn√©es\n1.1 Contexte M√©tier\nCe lakehouse analyse les donn√©es de navigation Wikipedia (clickstream) pour comprendre les parcours utilisateurs, identifier les contenus populaires et optimiser la d√©couverte de contenu.\nJeu de donn√©es : Wikipedia Clickstream (Novembre 2024)\nSource : Wikimedia Foundation (donn√©es publiques)\nTaille : 10 millions de lignes, 450 Mo (TSV brut)\n1.2 Caract√©ristiques du Jeu de Donn√©es\n\nFormat : TSV (valeurs s√©par√©es par tabulation)\nColonnes cl√©s :\n\nprev : Page source ou r√©f√©rent\ncurr : Page destination\ntype : Type de lien (interne, externe, autre)\nn : Nombre de clics (volume de trafic)\n\n\nVolume de donn√©es : 10 000 000 lignes (exigence atteinte)\nP√©riode : Snapshot clickstream de novembre 2024\n\n1.3 Cas d‚ÄôUsage\nCe lakehouse permet trois analyses principales :\n\nIdentifier les pages Wikipedia les plus visit√©es pour prioriser le contenu\nAnalyser les principaux r√©f√©rents pour comprendre les sources de trafic\nComparer les sch√©mas de trafic selon le type de lien (interne vs externe)\n\n\n2. Syst√®me et SLO\n2.1 Sp√©cifications Mat√©rielles\n\nCPU : Intel Core i7 (8 c≈ìurs)\nRAM : 16 Go\nDisque : SSD\nVersion Spark : 4.0.1\nVersion Python : 3.10.18\n\n2.2 Objectifs de Niveau de Service (SLO)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM√©triqueCibleMesureActualit√© des donn√©es‚áê 2 heuresTemps entre l‚Äôarriv√©e des donn√©es brutes et la disponibilit√© en goldLatence requ√™te (p95)‚áê 4 secondesTemps de r√©ponse au 95e percentile pour Q1-Q3Efficacit√© stockage‚áê 60% de la taille CSVRatio de compression Parquet\n2.3 Justification des SLO\n\nActualit√© : Besoin d‚Äôanalytique quasi temps r√©el pour la prise de d√©cision\nLatence : Exigence d‚Äôinteractivit√© (dashboard &lt; 5s)\nStockage : Optimisation des co√ªts pour la r√©tention long terme\n\n\n3. Architecture Lakehouse\n3.1 Architecture Trois Couches\nCSV BRUT\n  |\n  v\n+---------------------+\n| BRONZE (CSV)        |  &lt;- Donn√©es brutes immuables\n| - Pas de sch√©ma     |\n| - Tra√ßabilit√©       |\n+---------------------+\n  |\n  v\n+---------------------+\n| SILVER (Parquet)    |  &lt;- Nettoy√©, typ√©, valid√©\n| - Sch√©ma impos√©     |\n| - Contr√¥les qualit√© |\n| - D√©dupliqu√©        |\n+---------------------+\n  |\n  v\n+---------------------+\n| GOLD (Parquet)      |  &lt;- Tables analytiques\n| - Q1 : Agr√©gat jour |\n| - Q2 : Top r√©f√©rent |\n| - Q3 : Filtr√©       |\n+---------------------+\n\n3.2 Strat√©gie d‚Äô√âvolution du Sch√©ma\n\nBronze : Sch√©ma √† la lecture (tout string)\nSilver : Sch√©ma strict (d√©fini dans la config)\nGold : Sch√©ma sp√©cifique √† la requ√™te\n\n3.3 Contr√¥les Qualit√© des Donn√©es\nQuatre r√®gles de validation appliqu√©es en silver :\n\nClics non nuls (erreur) : Rejeter les enregistrements avec clics nuls\nClics positifs (erreur) : Rejeter les valeurs n√©gatives (n &gt;= 0)\nNom de page valide (erreur) : Rejeter les noms de page vides\nType valide (avertissement) : Journaliser les types de lien inattendus\n\n\n4. Conception Physique et Optimisations\n4.1 Design de Base\n\nSilver : Parquet non partitionn√©\nGold : √âcriture Parquet simple\nRequ√™tes : Scan complet des tables\nNombre de fichiers : Forte fragmentation\n\n4.2 Design Optimis√©\n4.2.1 Strat√©gie de Repartitionnement\nnum_partitions = max(4, int(total_bytes / (128 * 1024 * 1024)))\ndf_silver_opt.repartition(num_partitions)\n\nRaison : √âquilibrer le parall√©lisme et la taille des fichiers\nB√©n√©fice : R√©duit le probl√®me des petits fichiers et am√©liore l‚Äôefficacit√© des scans\nInconv√©nient : Co√ªt de shuffle lors de l‚Äô√©criture\n\n4.2.2 Strat√©gie de Tri\nsortWithinPartitions(F.desc(&quot;n&quot;))\n\nRaison : Trier par nombre de clics d√©croissant pour acc√©l√©rer les requ√™tes TOP-N\nB√©n√©fice : Permet l‚Äôarr√™t anticip√© pour les requ√™tes LIMIT\nImpl√©mentation : Maintient la localit√© des partitions tout en triant\n\n4.2.3 Taille des Fichiers\ntarget_file_size_mb: 128\n\nRaison : √âquilibre entre parall√©lisme et surcharge de m√©tadonn√©es\nCalcul : num_partitions = data_size / 128MB\nR√©sultat : Nombre optimal de fichiers pour 16Go de RAM\n\n4.2.4 Ex√©cution Adaptative des Requ√™tes (AQE)\nspark.sql.adaptive.enabled = true\nspark.sql.adaptive.coalescePartitions.enabled = true\n\nB√©n√©fice : Coalescence dynamique des partitions r√©duit le shuffle\nTuning m√©moire : driver.memory=4g, executor.memory=4g, memory.fraction=0.6\n\n\n5. Preuves et M√©triques\n5.1 Comparaison des Plans Physiques\nQ1 : Top 20 Pages les Plus Visit√©es\nPlan de base :\nFileScan parquet [prev,curr,type,n]\nExchange hashpartitioning(curr)\nHashAggregate [sum(n)]\nSort [total_clicks DESC]\n\n\nTemps √©coul√© : 19 400 ms\nPas d‚Äôoptimisation de tri\n\nPlan optimis√© :\nFileScan parquet [prev,curr,type,n] (partitions tri√©es)\nExchange hashpartitioning(curr)\nHashAggregate [sum(n)]\nSort [total_clicks DESC] (r√©duit gr√¢ce au pr√©-tri)\n\n\nTemps √©coul√© : 517 ms\nGain : 97,3% plus rapide\n\n5.2 M√©triques de Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequ√™tePhaseTemps (ms)GainQ1 : Top PagesBase19 400-Q1 : Top PagesOptimis√©51797,3%Q2 : Top R√©f√©rentsBase17 443-Q2 : Top R√©f√©rentsOptimis√©13 69021,5%Q3 : Analyse TypeBase17 491-Q3 : Analyse TypeOptimis√©1199,9%\nObservations cl√©s :\n\nQ1 : am√©lioration spectaculaire (97%) gr√¢ce au pr√©-tri\nQ2 : am√©lioration mod√©r√©e (21%) car le groupement par r√©f√©rent b√©n√©ficie moins du tri\nQ3 : am√©lioration extr√™me (99,9%) car la cardinalit√© de type est tr√®s faible (3 valeurs)\n\n5.3 Validation des SLO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSLOCibleBaseOptimis√©StatutFra√Æcheur des donn√©es‚áê 2h0,5h0,3hOKLatence Q1 (p95)‚áê 4s19,4s0,5sOKEfficacit√© stockage‚áê 60%42%42%OK\nAnalyse :\n\nTous les SLO atteints apr√®s optimisation\nLatence Q1 pass√©e de 19,4s √† 0,5s (bien en dessous de 4s)\nEfficacit√© de stockage maintenue √† 42% (compression Parquet)\n\n5.4 Analyse du Stockage\nTSV brut :         450 Mo (100%)\nBronze (CSV) :     450 Mo (100%)\nSilver (Parquet) : 189 Mo (42%)\nGold (Parquet) :    12 Mo (2,7%)\n\n\nRatio de compression : 2,4x (TSV vers Parquet)\nMat√©rialisation gold : Surcharge minimale pour les agr√©gats pr√©-calcul√©s\nStockage total : 651 Mo (1,4x la taille brute en incluant toutes les couches)\n\n\n6. R√©sultats et Limites\n6.1 R√©sultats Cl√©s\n\nOptimisation du tri : +97% pour les requ√™tes TOP-N (Q1)\nOptimisation cardinalit√© : +99,9% pour les agr√©gations faible cardinalit√© (Q3)\nEfficacit√© stockage : 42% de la taille TSV d‚Äôorigine avec Parquet\nTous les SLO atteints : Latence r√©duite de 19,4s √† 0,5s (sous 4s)\nStabilit√© m√©moire : La configuration optimis√©e a √©vit√© les erreurs OOM sur 16Go\n\n6.2 Limites\n6.2.1 Contraintes Monoposte\n\nM√©moire : 16Go limite le parall√©lisme des partitions\nI/O disque : SSD limit√© pour les √©critures intensives\nCPU : Pas de b√©n√©fice de traitement distribu√©\n\n6.2.2 Arbitrages d‚ÄôOptimisation\n\nCo√ªt du repartitionnement : Amplification des √©critures lors de l‚Äôoptimisation silver\nSurcharge du tri : sortWithinPartitions ajoute du CPU mais rentable pour les agr√©gations\nTuning m√©moire : Configuration explicite requise pour √©viter les warnings de cache\nPartitionnement limit√© : Pas de partitionnement temporel car snapshot unique (nov 2024)\n\n6.2.3 Limites des Requ√™tes\n\nQ2 gain mod√©r√© : Le groupement par r√©f√©rent b√©n√©ficie peu du tri sur les clics\nP√©nalit√© √† froid : Premi√®re requ√™te plus lente √† cause du chargement des m√©tadonn√©es Parquet\nPas de mise √† jour incr√©mentale : R√©√©criture compl√®te requise (pas de delta merge)\n\n6.3 Am√©liorations Futures\n\nTri multi-colonnes : Ajouter un tri secondaire sur le nom de page pour d√©partager les √©galit√©s\nBucketing : Activer les joins bucketis√©s pour les analyses r√©f√©rents\nTraitement incr√©mental : Impl√©menter l‚Äôingestion streaming pour du temps r√©el\nPruning de colonnes : Utiliser select() explicite pour r√©duire l‚ÄôI/O sur les projections\nStatistiques : Activer l‚Äôoptimisation bas√©e sur les stats de table\n\n\n7. Annexes\n7.1 Inventaire des Fichiers\nprojet-final/\n‚îú‚îÄ‚îÄ DE1_Project_Notebook_EN.ipynb          (Notebook principal)\n‚îú‚îÄ‚îÄ de1_project_config.yml                 (Configuration)\n‚îú‚îÄ‚îÄ DE1_Project_Report.md                  (Ce document)\n‚îú‚îÄ‚îÄ project_genai.md                       (Usage GenAI)\n‚îú‚îÄ‚îÄ outputs/\n‚îÇ   ‚îî‚îÄ‚îÄ project/\n‚îÇ       ‚îú‚îÄ‚îÄ bronze/                        (CSV brut)\n‚îÇ       ‚îú‚îÄ‚îÄ silver/                        (Parquet nettoy√©)\n‚îÇ       ‚îú‚îÄ‚îÄ silver_optimized/              (Parquet partitionn√©)\n‚îÇ       ‚îî‚îÄ‚îÄ gold/                          (Tables analytiques)\n‚îÇ           ‚îú‚îÄ‚îÄ q1_daily_aggregation/\n‚îÇ           ‚îú‚îÄ‚îÄ q2_top_referrers/\n‚îÇ           ‚îî‚îÄ‚îÄ q3_filtered_analysis/\n‚îî‚îÄ‚îÄ proof/\n  ‚îú‚îÄ‚îÄ baseline_q1_plan.txt               (Plan physique Q1)\n  ‚îú‚îÄ‚îÄ baseline_q2_plan.txt               (Plan physique Q2)\n  ‚îú‚îÄ‚îÄ baseline_q3_plan.txt               (Plan physique Q3)\n  ‚îú‚îÄ‚îÄ optimized_q1_plan.txt              (Plan optimis√© Q1)\n  ‚îú‚îÄ‚îÄ optimized_q2_plan.txt              (Plan optimis√© Q2)\n  ‚îú‚îÄ‚îÄ optimized_q3_plan.txt              (Plan optimis√© Q3)\n  ‚îú‚îÄ‚îÄ metrics7.png                       (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics8.png                       (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics9.png                       (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics_10.png                     (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics_11.png                     (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics_13.png                     (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics_15.png                     (M√©triques)\n  ‚îú‚îÄ‚îÄ metrics12.png                      (M√©triques)\n  ‚îú‚îÄ‚îÄ Sparkjob.png                       (Capture Spark)\n\n7.2 Extraits de Configuration\nslo:\n  freshness_hours: 2\n  q1_latency_p95_seconds: 4\n  storage_ratio_max: 0.60\n \nlayout:\n  partition_by: []\n  sort_by: []\n  target_file_size_mb: 128\n \nqueries:\n  q1: &quot;Top 20 pages les plus visit√©es&quot;\n  q2: &quot;Top 20 pages r√©f√©rentes&quot;\n  q3: &quot;Analyse des clics par type&quot;\n7.3 R√©f√©rences\n\nDocumentation Apache Spark : spark.apache.org/docs/latest/\nSp√©cification Parquet : parquet.apache.org/docs/\nSupports de cours DE1  de Bard TAJINI: ESIEE 2025-2026\n\nRemise : Voir project_genai.md pour la d√©claration d‚Äôusage d‚ÄôIA"},"project/assets/project_final_genai":{"slug":"project/assets/project_final_genai","filePath":"project/assets/project_final_genai.md","title":"project_final_genai","links":[],"tags":[],"content":"Documentation d‚ÄôUtilisation de l‚ÄôIA G√©n√©rative\nProjet : Projet Final DE1 - Lakehouse Local\nAuteurs : DIALLO Samba &amp; DIOP Mouhamed\nDate : D√©cembre 2025\n\n1. D√©claration\nCe document d√©crit comment les outils d‚ÄôIA g√©n√©rative ont √©t√© utilis√©s (ou non) pour la r√©alisation de ce projet final de Data Engineering I.\n\n2. Outils d‚ÄôIA Utilis√©s\n2.1 GitHub Copilot (Extension VS Code)\nBut : Assistance au code et g√©n√©ration de structure de projet\nNiveau d‚Äôutilisation : √âtendu\nCas d‚ÄôUsage Sp√©cifiques :\n\n\nMise en place de la structure du projet\n\nG√©n√©ration de l‚Äôarborescence initiale pour les couches lakehouse (bronze/silver/gold)\nCr√©ation du fichier de1_project_config.yml complet avec toutes les sections requises\nG√©n√©ration des cellules de notebook avec titres markdown adapt√©s\n\n\n\nG√©n√©ration de code\n\nTransformations PySpark DataFrame pour la couche silver (application du sch√©ma, conversions de type)\nLogique de validation qualit√© √† partir des r√®gles de config\nCapture des plans physiques et op√©rations de fichiers\nCode de journalisation des m√©triques et validation SLO\n\n\n\nDocumentation\n\nG√©n√©ration du mod√®le de rapport avec structure acad√©mique\nCr√©ation de commentaires de code expliquant les strat√©gies d‚Äôoptimisation\nR√©daction de cette documentation d‚Äôusage GenAI\n\n\n\nCe qui N‚Äôa PAS √©t√© G√©n√©r√© :\n\nD√©cisions m√©tier : Les requ√™tes (Q1-Q3) ont √©t√© con√ßues manuellement √† partir de l‚Äôanalyse du jeu de donn√©es Wikipedia clickstream\nCibles SLO : Les objectifs (latence 4s, stockage 60%) ont √©t√© fix√©s selon les exigences du projet et la contrainte 16Go RAM\nStrat√©gie d‚Äôoptimisation : Les choix de repartitionnement et tri ont √©t√© faits apr√®s analyse manuelle des performances\nInterpr√©tation des m√©triques : L‚Äôanalyse des performances (19,4s vs 0,5s) a √©t√© r√©alis√©e manuellement via Spark UI\n\n\n3. Contributions Humaines\n3.1 D√©cisions de Conception\nToutes les d√©cisions d‚Äôarchitecture ont √©t√© prises manuellement :\n\nChoix du dataset : Wikipedia Clickstream Novembre 2024 pour sa taille (10M lignes, 450Mo) et sa pertinence analytique\nStrat√©gie d‚Äôoptimisation : Repartitionnement + tri par nombre de clics (n DESC) apr√®s analyse des requ√™tes\nOrdre de tri : sortWithinPartitions par n d√©croissant pour optimiser les requ√™tes TOP-N\nTaille des fichiers : Calcul de 128Mo cible selon la RAM et le parall√©lisme\nTuning m√©moire : driver.memory=4g, executor.memory=4g apr√®s avertissements de cache\n\n3.2 Tests et Validation\n\nEx√©cution notebook : Toutes les cellules ont √©t√© ex√©cut√©es manuellement pour valider la justesse\nD√©bogage : Correction manuelle des erreurs de sch√©ma et de qualit√©\nMesure de performance : Capture manuelle des m√©triques Spark UI pour chaque requ√™te\nValidation SLO : Comparaison manuelle des m√©triques base/optimis√© avec les cibles\n\n3.3 Esprit Critique\n\nJustification des optimisations : Analyse des plans physiques pour comprendre les gains\nAnalyse des compromis : √âvaluation du nombre de partitions vs taille de fichier\nIdentification des limites : Reconnaissance des contraintes monoposte et documentation des solutions\n\n\n4. M√©thodologie d‚ÄôInteraction avec l‚ÄôIA\n4.1 Strat√©gie de Prompt\nUtilisation de prompts pr√©cis et contextualis√©s :\nExemple : ¬´ Cr√©e du code PySpark pour appliquer un sch√©ma depuis un YAML,\n           appliquer les r√®gles qualit√©, et journaliser les violations sans emojis ¬ª\n\n4.2 Revue de Code\nChaque code g√©n√©r√© par l‚ÄôIA a √©t√© :\n\nRelu : V√©rifi√© pour la justesse et les bonnes pratiques PySpark\nTest√© : Ex√©cut√© dans le notebook pour valider la fonctionnalit√©\nAdapt√© : Modifi√© pour correspondre au sch√©ma r√©el et aux besoins\nDocument√© : Comment√© pour expliquer la logique\n\n4.3 Limites Rencontr√©es\n\nSch√©mas g√©n√©riques : L‚ÄôIA proposait des sch√©mas √† adapter manuellement\nSp√©cificit√© des requ√™tes : Les requ√™tes exactes ont √©t√© r√©√©crites √† la main\nCollecte des m√©triques : L‚ÄôIA proposait des mod√®les mais ne pouvait acc√©der √† Spark UI ; m√©triques remplies manuellement\n\n\n5. Comp√©tences D√©velopp√©es\n5.1 Comp√©tences gr√¢ce √† l‚ÄôIA\n\nPrototypage rapide : Structure g√©n√©r√©e rapidement puis raffin√©e manuellement\nPatterns de code : Apprentissage d‚Äôidiomes PySpark via suggestions IA (ex : sortWithinPartitions)\nStandards de documentation : Les mod√®les IA ont montr√© une structure professionnelle\n\n5.2 Comp√©tences D√©velopp√©es en Autonomie\n\nAnalyse de performance : Lecture des plans physiques Spark et m√©triques UI\nTechniques d‚Äôoptimisation : Compr√©hension du partition pruning, sizing, AQE par exp√©rimentation\nConception syst√®me : Architecture lakehouse selon les principes du cours DE1\n\n\n6. Consid√©rations √âthiques\n6.1 Int√©grit√© Acad√©mique\n\nPas de plagiat : Tout code IA relu, compris et adapt√©\nAttribution claire : Ce document d√©clare explicitement l‚Äôusage de l‚ÄôIA\nTravail original : Les choix, analyses et conclusions sont originaux\n\n6.2 Respect de la Politique de Collaboration\nCe projet respecte la politique ESIEE sur l‚Äôusage de l‚ÄôIA :\n\nIA utilis√©e comme outil de productivit√©, pas comme substitut √† l‚Äôapprentissage\nTout le travail a √©t√© valid√© et compris avant soumission\nEsprit critique appliqu√© √† chaque suggestion IA\n\n\n7. Conclusion\nR√¥le de l‚ÄôIA : Acc√©l√©rateur et g√©n√©rateur de structure\nR√¥le humain : Concepteur, analyste, validateur, esprit critique\nL‚ÄôIA g√©n√©rative a permis un gain de temps significatif (environ 40% sur le code standard), mais toutes les d√©cisions d‚Äôing√©nierie, analyses de performance et strat√©gies d‚Äôoptimisation sont rest√©es humaines. Ce projet illustre une collaboration IA-humain efficace dans le respect de l‚Äôint√©grit√© acad√©mique et de la compr√©hension technique.\n\n8. Annexe : Pourcentage de Code G√©n√©r√© par l‚ÄôIA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComposantG√©n√©r√© IA√âcrit humainAdapt√© humainYAML config80%20%0%Notebook (bronze)60%20%20%Notebook (silver)50%30%20%Notebook (gold)40%40%20%Code optimisation30%50%20%R√©daction rapport70%20%10%Doc GenAI60%40%0%\nEstimation globale : 55% structure IA, 30% travail original, 15% adaptation humaine\n\nSignature : DIALLO Samba &amp; DIOP Mouhamed\nDate : [Date de soumission]\nAffirmation : Nous affirmons que ce document refl√®te fid√®lement l‚Äôusage de l‚ÄôIA g√©n√©rative dans ce projet."},"project/index":{"slug":"project/index","filePath":"project/index.md","title":"Final Project ‚Äì Data Engineering 1","links":["project/assets/baseline_q1_plan.txt","project/assets/baseline_q2_plan.txt","project/assets/baseline_q3_plan.txt","project/assets/DE1_Project_Report","project/assets/optimized_q1_plan.txt","project/assets/optimized_q2_plan.txt","project/assets/optimized_q3_plan.txt","project/assets/project_final_genai"],"tags":[],"content":"Final Project ‚Äì Data Engineering 1\nNotebook : DE1_Project_Notebook_EN\n{% include ‚Äúproject/assets/DE1_Project_Notebook_EN.md‚Äù %}\nüìä Proof / Outputs\n\n\n\n\n\n\n\n\n\n\n\nText files\n\nbaseline_q1_plan.txt\nbaseline_q2_plan.txt\nbaseline_q3_plan.txt\nDE1_Project_Report.md\noptimized_q1_plan.txt\noptimized_q2_plan.txt\noptimized_q3_plan.txt\nproject_final_genai.md\n"}}